{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiveshj/SeniorThesis/blob/main/entropy_and_trellis_visualization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers torch matplotlib seaborn scipy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjuBo_7K2rw4",
        "outputId": "85d1b0df-9fc5-4e1c-a778-ee1205310976"
      },
      "id": "cjuBo_7K2rw4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "id": "9d80e0be-53d5-4a01-8180-0093ccab1920",
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from collections import defaultdict\n",
        "import scipy.special\n",
        "\n",
        "class EntropyTrellisAnalyzer:\n",
        "    def __init__(self, model, tokenizer):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.paths = {}\n",
        "        self.scores = {}\n",
        "        self.states = {}\n",
        "        self.entropies = {}\n",
        "        self.logits = {}\n",
        "\n",
        "    def record_path(self, name, tokens, states, scores, logits=None):\n",
        "        \"\"\"\n",
        "        Record a decoding path for later analysis.\n",
        "\n",
        "        Args:\n",
        "            name: Name of the decoding algorithm (e.g., 'viterbi', 'beam', 'greedy')\n",
        "            tokens: List of token IDs or strings representing the output\n",
        "            states: List of states visited at each time step\n",
        "            scores: List of scores (log probabilities) at each time step\n",
        "            logits: List of logits at each time step (optional, for entropy calculation)\n",
        "        \"\"\"\n",
        "        self.paths[name] = tokens\n",
        "        self.states[name] = states\n",
        "        self.scores[name] = scores\n",
        "\n",
        "        if logits is not None:\n",
        "            self.logits[name] = logits\n",
        "            # Calculate entropies from logits\n",
        "            self.entropies[name] = [self.calculate_conditional_entropy(logit) for logit in logits]\n",
        "\n",
        "    def calculate_conditional_entropy(self, logits):\n",
        "        \"\"\"\n",
        "        Calculate conditional entropy from token logits.\n",
        "        Higher values indicate more uncertainty in the next token prediction.\n",
        "\n",
        "        Args:\n",
        "            logits: Raw logits from your model for the next token\n",
        "\n",
        "        Returns:\n",
        "            Conditional entropy value\n",
        "        \"\"\"\n",
        "        # Convert logits to probabilities using softmax\n",
        "        probs = scipy.special.softmax(logits, axis=-1)\n",
        "\n",
        "        # Calculate entropy: -sum(p * log(p))\n",
        "        entropy = -np.sum(probs * np.log(probs + 1e-10))\n",
        "        return entropy\n",
        "\n",
        "    def analyze_state_overlap(self):\n",
        "        \"\"\"\n",
        "        Check if states from greedy and beam search are present in Viterbi trellis.\n",
        "        Returns a dict with overlap percentages.\n",
        "        \"\"\"\n",
        "        results = {}\n",
        "        viterbi_states = set(self.states.get('viterbi', []))\n",
        "\n",
        "        for name, states in self.states.items():\n",
        "            if name == 'viterbi':\n",
        "                continue\n",
        "\n",
        "            overlap = [state for state in states if state in viterbi_states]\n",
        "            overlap_percent = len(overlap) / len(states) * 100 if states else 0\n",
        "            results[name] = {\n",
        "                'overlap_count': len(overlap),\n",
        "                'total_states': len(states),\n",
        "                'overlap_percent': overlap_percent\n",
        "            }\n",
        "\n",
        "        return results\n",
        "\n",
        "    def find_divergence_points(self):\n",
        "        \"\"\"\n",
        "        Find points where paths diverge and analyze the scores at these points.\n",
        "        Also compares entropy at divergence points if available.\n",
        "        \"\"\"\n",
        "        divergences = {}\n",
        "        viterbi_tokens = self.paths.get('viterbi', [])\n",
        "\n",
        "        for name, tokens in self.paths.items():\n",
        "            if name == 'viterbi':\n",
        "                continue\n",
        "\n",
        "            # Find position where paths diverge\n",
        "            diverge_pos = None\n",
        "            for i, (vt, t) in enumerate(zip(viterbi_tokens, tokens)):\n",
        "                if vt != t:\n",
        "                    diverge_pos = i\n",
        "                    break\n",
        "\n",
        "            if diverge_pos is not None:\n",
        "                viterbi_score = self.scores['viterbi'][diverge_pos] if diverge_pos < len(self.scores['viterbi']) else None\n",
        "                other_score = self.scores[name][diverge_pos] if diverge_pos < len(self.scores[name]) else None\n",
        "\n",
        "                # Add entropy information if available\n",
        "                viterbi_entropy = self.entropies.get('viterbi', [None])[diverge_pos] if 'viterbi' in self.entropies and diverge_pos < len(self.entropies['viterbi']) else None\n",
        "                other_entropy = self.entropies.get(name, [None])[diverge_pos] if name in self.entropies and diverge_pos < len(self.entropies[name]) else None\n",
        "\n",
        "                divergences[name] = {\n",
        "                    'position': diverge_pos,\n",
        "                    'viterbi_token': viterbi_tokens[diverge_pos] if diverge_pos < len(viterbi_tokens) else None,\n",
        "                    'alternate_token': tokens[diverge_pos] if diverge_pos < len(tokens) else None,\n",
        "                    'viterbi_score': viterbi_score,\n",
        "                    'alternate_score': other_score,\n",
        "                    'score_difference': viterbi_score - other_score if viterbi_score is not None and other_score is not None else None,\n",
        "                    'viterbi_entropy': viterbi_entropy,\n",
        "                    'alternate_entropy': other_entropy\n",
        "                }\n",
        "\n",
        "        return divergences\n",
        "\n",
        "    def validate_paths(self):\n",
        "        \"\"\"\n",
        "        Validate if the paths from beam and greedy search exist in the trellis.\n",
        "        Calculate the full path probabilities.\n",
        "        \"\"\"\n",
        "        # This depends on your trellis implementation\n",
        "        results = {}\n",
        "        viterbi_total = sum(self.scores.get('viterbi', [0]))\n",
        "\n",
        "        for name, scores in self.scores.items():\n",
        "            if name == 'viterbi':\n",
        "                continue\n",
        "\n",
        "            total_score = sum(scores)\n",
        "            results[name] = {\n",
        "                'total_score': total_score,\n",
        "                'viterbi_score': viterbi_total,\n",
        "                'difference': viterbi_total - total_score,\n",
        "                'is_valid': True  # Assuming all paths are valid; adjust based on your implementation\n",
        "            }\n",
        "\n",
        "        return results\n",
        "\n",
        "    def visualize_paths_with_entropy(self, output_file=None):\n",
        "        \"\"\"\n",
        "        Visualize the different paths through the trellis along with entropy.\n",
        "        \"\"\"\n",
        "        has_entropy = any(len(self.entropies.get(name, [])) > 0 for name in self.paths)\n",
        "\n",
        "        if has_entropy:\n",
        "            fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 12), sharex=True)\n",
        "        else:\n",
        "            fig, ax1 = plt.subplots(figsize=(14, 6))\n",
        "\n",
        "        # Get max length of all paths\n",
        "        max_len = max(len(tokens) for tokens in self.paths.values())\n",
        "\n",
        "        # Plot scores for each path\n",
        "        for name, scores in self.scores.items():\n",
        "            # Pad scores to max_len if needed\n",
        "            padded_scores = scores + [None] * (max_len - len(scores))\n",
        "            valid_scores = [s for s in padded_scores if s is not None]\n",
        "            positions = list(range(len(valid_scores)))\n",
        "            ax1.plot(positions, valid_scores, marker='o', label=name)\n",
        "\n",
        "        ax1.set_ylabel('Log probability score')\n",
        "        ax1.set_title('Decoding Paths Comparison')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True)\n",
        "\n",
        "        # Plot entropy if available\n",
        "        if has_entropy:\n",
        "            for name, entropies in self.entropies.items():\n",
        "                padded_entropies = entropies + [None] * (max_len - len(entropies))\n",
        "                valid_entropies = [e for e in padded_entropies if e is not None]\n",
        "                positions = list(range(len(valid_entropies)))\n",
        "                ax2.plot(positions, valid_entropies, marker='s', linestyle='--', label=f\"{name} entropy\")\n",
        "\n",
        "            ax2.set_xlabel('Position in sequence')\n",
        "            ax2.set_ylabel('Conditional entropy')\n",
        "            ax2.set_title('Conditional Entropy at Each Position')\n",
        "            ax2.legend()\n",
        "            ax2.grid(True)\n",
        "        else:\n",
        "            ax1.set_xlabel('Position in sequence')\n",
        "\n",
        "        # Highlight divergence points\n",
        "        divergences = self.find_divergence_points()\n",
        "        for name, info in divergences.items():\n",
        "            pos = info['position']\n",
        "            if pos is not None:\n",
        "                ax1.axvline(x=pos, color='r', linestyle='--', alpha=0.5)\n",
        "                ax1.text(pos, min(s for s in sum([list(self.scores.values())], []) if s is not None) * 0.95,\n",
        "                         f\"Divergence at {pos}\", rotation=90, color='red')\n",
        "\n",
        "                if has_entropy:\n",
        "                    ax2.axvline(x=pos, color='r', linestyle='--', alpha=0.5)\n",
        "\n",
        "        if output_file:\n",
        "            plt.savefig(output_file)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def analyze_entropy_at_divergence(self):\n",
        "        \"\"\"\n",
        "        Analyze the relationship between conditional entropy and path divergence.\n",
        "        \"\"\"\n",
        "        results = {}\n",
        "        divergences = self.find_divergence_points()\n",
        "\n",
        "        for name, info in divergences.items():\n",
        "            pos = info['position']\n",
        "            if pos is None:\n",
        "                continue\n",
        "\n",
        "            # Check if we have entropy data\n",
        "            if info['viterbi_entropy'] is not None:\n",
        "                # Calculate entropy statistics\n",
        "                avg_entropy = np.mean([e for e in self.entropies.get('viterbi', []) if e is not None])\n",
        "                entropy_at_divergence = info['viterbi_entropy']\n",
        "                relative_entropy = entropy_at_divergence / avg_entropy if avg_entropy > 0 else None\n",
        "\n",
        "                results[name] = {\n",
        "                    'position': pos,\n",
        "                    'entropy_at_divergence': entropy_at_divergence,\n",
        "                    'average_entropy': avg_entropy,\n",
        "                    'relative_entropy': relative_entropy,\n",
        "                    'is_high_entropy': entropy_at_divergence > avg_entropy\n",
        "                }\n",
        "\n",
        "        return results\n",
        "\n",
        "    def correlate_entropy_with_divergence(self):\n",
        "        \"\"\"\n",
        "        Calculate correlation between entropy and path divergence.\n",
        "        \"\"\"\n",
        "        # Get all positions where paths differ\n",
        "        viterbi_tokens = self.paths.get('viterbi', [])\n",
        "        divergence_positions = []\n",
        "\n",
        "        for name, tokens in self.paths.items():\n",
        "            if name == 'viterbi':\n",
        "                continue\n",
        "\n",
        "            for i, (vt, t) in enumerate(zip(viterbi_tokens, tokens)):\n",
        "                if vt != t and i not in divergence_positions:\n",
        "                    divergence_positions.append(i)\n",
        "\n",
        "        # If we have entropy data and divergence points\n",
        "        if 'viterbi' in self.entropies and divergence_positions:\n",
        "            entropies = self.entropies['viterbi']\n",
        "            max_pos = min(len(entropies), len(viterbi_tokens))\n",
        "\n",
        "            # Create binary mask: 1 if position has divergence, 0 otherwise\n",
        "            divergence_mask = np.zeros(max_pos)\n",
        "            for pos in divergence_positions:\n",
        "                if pos < max_pos:\n",
        "                    divergence_mask[pos] = 1\n",
        "\n",
        "            # Extract entropies for positions we have\n",
        "            entropy_values = np.array(entropies[:max_pos])\n",
        "\n",
        "            # Calculate point-biserial correlation (between continuous and binary variables)\n",
        "            # This tells us if higher entropy correlates with divergence points\n",
        "            mean_entropy_diverge = np.mean(entropy_values[divergence_mask == 1])\n",
        "            mean_entropy_no_diverge = np.mean(entropy_values[divergence_mask == 0])\n",
        "            std_entropy = np.std(entropy_values)\n",
        "            n_diverge = np.sum(divergence_mask)\n",
        "            n_no_diverge = len(divergence_mask) - n_diverge\n",
        "            n_total = len(divergence_mask)\n",
        "\n",
        "            # Point-biserial formula\n",
        "            if std_entropy > 0 and n_diverge > 0 and n_no_diverge > 0:\n",
        "                correlation = ((mean_entropy_diverge - mean_entropy_no_diverge) / std_entropy) * \\\n",
        "                               np.sqrt((n_diverge * n_no_diverge) / (n_total * n_total))\n",
        "\n",
        "                return {\n",
        "                    'correlation': correlation,\n",
        "                    'mean_entropy_at_divergence': mean_entropy_diverge,\n",
        "                    'mean_entropy_elsewhere': mean_entropy_no_diverge,\n",
        "                    'entropy_ratio': mean_entropy_diverge / mean_entropy_no_diverge if mean_entropy_no_diverge > 0 else None\n",
        "                }\n",
        "\n",
        "        return None\n",
        "\n",
        "    def visualize_trellis(self, timesteps=10, k_best=5, output_file=None):\n",
        "        \"\"\"\n",
        "        Visualize the trellis structure with the different paths.\n",
        "\n",
        "        Args:\n",
        "            timesteps: Number of time steps to visualize\n",
        "            k_best: Number of best states to show at each time step\n",
        "            output_file: If provided, save the figure to this file\n",
        "        \"\"\"\n",
        "        # Create a combined set of all states visited\n",
        "        all_states = defaultdict(lambda: defaultdict(float))\n",
        "\n",
        "        # For simplicity, we assume states dict contains token IDs at each position\n",
        "        for name, states_path in self.states.items():\n",
        "            for t, state in enumerate(states_path):\n",
        "                if t >= timesteps:\n",
        "                    break\n",
        "                all_states[t][state] = max(all_states[t].get(state, 0),\n",
        "                                         self.scores[name][t] if t < len(self.scores[name]) else 0)\n",
        "\n",
        "        # Create a matrix for visualization\n",
        "        matrix = np.zeros((k_best, timesteps))\n",
        "        state_labels = [[] for _ in range(timesteps)]\n",
        "\n",
        "        for t in range(timesteps):\n",
        "            if t not in all_states:\n",
        "                continue\n",
        "\n",
        "            # Get top k states by score\n",
        "            top_states = sorted(all_states[t].items(), key=lambda x: x[1], reverse=True)[:k_best]\n",
        "\n",
        "            for i, (state, score) in enumerate(top_states):\n",
        "                matrix[i, t] = score\n",
        "                # Try to convert state to readable token if it's an integer\n",
        "                if isinstance(state, int):\n",
        "                    try:\n",
        "                        token = self.tokenizer.decode([state])\n",
        "                    except:\n",
        "                        token = str(state)\n",
        "                else:\n",
        "                    token = str(state)\n",
        "                state_labels[t].append(token)\n",
        "\n",
        "        # Create a heatmap\n",
        "        plt.figure(figsize=(16, 10))\n",
        "        ax = sns.heatmap(matrix, cmap=\"YlGnBu\", annot=False)\n",
        "\n",
        "        # Add path markers\n",
        "        for name, states_path in self.states.items():\n",
        "            path_y = []\n",
        "            path_x = []\n",
        "\n",
        "            for t, state in enumerate(states_path):\n",
        "                if t >= timesteps:\n",
        "                    break\n",
        "\n",
        "                # Find this state in our top-k states\n",
        "                try:\n",
        "                    top_states = sorted(all_states[t].items(), key=lambda x: x[1], reverse=True)[:k_best]\n",
        "                    state_idx = [s[0] for s in top_states].index(state)\n",
        "                    path_y.append(state_idx + 0.5)  # +0.5 for center of cell\n",
        "                    path_x.append(t + 0.5)\n",
        "                except ValueError:\n",
        "                    # State not in top-k, skip this point\n",
        "                    continue\n",
        "\n",
        "            # Plot the path\n",
        "            plt.plot(path_x, path_y, marker='o', linewidth=2,\n",
        "                     label=name, alpha=0.7)\n",
        "\n",
        "        # Add labels and other details\n",
        "        plt.title(\"Trellis Paths Visualization\")\n",
        "        plt.ylabel(\"Top-k states at each time step\")\n",
        "        plt.xlabel(\"Time step\")\n",
        "        plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), ncol=3)\n",
        "\n",
        "        # Set custom y-tick labels (state labels)\n",
        "        y_ticks = np.arange(k_best) + 0.5\n",
        "        plt.yticks(y_ticks, [''] * k_best)\n",
        "\n",
        "        # Add state labels as text annotations\n",
        "        for t in range(timesteps):\n",
        "            for i, label in enumerate(state_labels[t]):\n",
        "                if i < k_best:\n",
        "                    plt.text(t + 0.5, i + 0.5, label,\n",
        "                            ha='center', va='center', fontsize=8)\n",
        "\n",
        "        if output_file:\n",
        "            plt.savefig(output_file)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# Example usage function - now with entropy analysis\n",
        "def compare_decoding_paths_with_entropy(input_text, model, tokenizer, viterbi_fn, greedy_fn, beam_fn):\n",
        "    \"\"\"\n",
        "    Compare different decoding paths for the same input, with entropy analysis.\n",
        "\n",
        "    Args:\n",
        "        input_text: The input text to decode\n",
        "        model: Your language model\n",
        "        tokenizer: Your tokenizer\n",
        "        viterbi_fn: Function that returns (tokens, states, scores, logits) for Viterbi decoding\n",
        "        greedy_fn: Function that returns (tokens, states, scores, logits) for greedy decoding\n",
        "        beam_fn: Function that returns (tokens, states, scores, logits) for beam search\n",
        "    \"\"\"\n",
        "    analyzer = EntropyTrellisAnalyzer(model, tokenizer)\n",
        "\n",
        "    # Run each decoding method and record results (now with logits for entropy calculation)\n",
        "    viterbi_tokens, viterbi_states, viterbi_scores, viterbi_logits = viterbi_fn(input_text)\n",
        "    analyzer.record_path('viterbi', viterbi_tokens, viterbi_states, viterbi_scores, viterbi_logits)\n",
        "\n",
        "    greedy_tokens, greedy_states, greedy_scores, greedy_logits = greedy_fn(input_text)\n",
        "    analyzer.record_path('greedy', greedy_tokens, greedy_states, greedy_scores, greedy_logits)\n",
        "\n",
        "    beam_tokens, beam_states, beam_scores, beam_logits = beam_fn(input_text)\n",
        "    analyzer.record_path('beam', beam_tokens, beam_states, beam_scores, beam_logits)\n",
        "\n",
        "    # Run basic analysis\n",
        "    print(\"=== State Overlap Analysis ===\")\n",
        "    overlap = analyzer.analyze_state_overlap()\n",
        "    for name, results in overlap.items():\n",
        "        print(f\"{name.capitalize()} search: {results['overlap_percent']:.2f}% of states appear in Viterbi trellis\")\n",
        "\n",
        "    print(\"\\n=== Divergence Points Analysis ===\")\n",
        "    divergences = analyzer.find_divergence_points()\n",
        "    for name, info in divergences.items():\n",
        "        if info['position'] is not None:\n",
        "            print(f\"{name.capitalize()} diverges from Viterbi at position {info['position']}:\")\n",
        "            print(f\"  Viterbi chose '{info['viterbi_token']}' (score: {info['viterbi_score']:.4f})\")\n",
        "            print(f\"  {name.capitalize()} chose '{info['alternate_token']}' (score: {info['alternate_score']:.4f})\")\n",
        "            print(f\"  Score difference: {info['score_difference']:.4f}\")\n",
        "\n",
        "            # Print entropy information if available\n",
        "            if info['viterbi_entropy'] is not None:\n",
        "                print(f\"  Entropy at divergence point: {info['viterbi_entropy']:.4f}\")\n",
        "\n",
        "    print(\"\\n=== Path Validation ===\")\n",
        "    validations = analyzer.validate_paths()\n",
        "    for name, results in validations.items():\n",
        "        print(f\"{name.capitalize()} path total score: {results['total_score']:.4f}\")\n",
        "        print(f\"Viterbi path total score: {results['viterbi_score']:.4f}\")\n",
        "        print(f\"Difference: {results['difference']:.4f}\")\n",
        "        print(f\"Path exists in trellis: {'Yes' if results['is_valid'] else 'No'}\")\n",
        "\n",
        "    # Run entropy-specific analysis\n",
        "    print(\"\\n=== Entropy Analysis at Divergence Points ===\")\n",
        "    entropy_analysis = analyzer.analyze_entropy_at_divergence()\n",
        "    if entropy_analysis:\n",
        "        for name, results in entropy_analysis.items():\n",
        "            print(f\"{name.capitalize()} divergence entropy analysis:\")\n",
        "            print(f\"  Entropy at divergence point: {results['entropy_at_divergence']:.4f}\")\n",
        "            print(f\"  Average entropy across sequence: {results['average_entropy']:.4f}\")\n",
        "            print(f\"  Relative entropy (divergence/average): {results['relative_entropy']:.4f}\")\n",
        "            print(f\"  Is high entropy point: {'Yes' if results['is_high_entropy'] else 'No'}\")\n",
        "    else:\n",
        "        print(\"No entropy data available for analysis.\")\n",
        "\n",
        "    # Calculate correlation between entropy and divergence\n",
        "    print(\"\\n=== Entropy-Divergence Correlation ===\")\n",
        "    correlation = analyzer.correlate_entropy_with_divergence()\n",
        "    if correlation:\n",
        "        print(f\"Correlation between entropy and path divergence: {correlation['correlation']:.4f}\")\n",
        "        print(f\"Mean entropy at divergence points: {correlation['mean_entropy_at_divergence']:.4f}\")\n",
        "        print(f\"Mean entropy elsewhere: {correlation['mean_entropy_elsewhere']:.4f}\")\n",
        "        print(f\"Ratio of entropy at divergence vs. elsewhere: {correlation['entropy_ratio']:.4f}\")\n",
        "\n",
        "        if correlation['correlation'] > 0.3:\n",
        "            print(\"CONCLUSION: Strong positive correlation suggests higher entropy regions are associated with path divergence.\")\n",
        "        elif correlation['correlation'] < -0.3:\n",
        "            print(\"CONCLUSION: Strong negative correlation suggests lower entropy regions are associated with path divergence.\")\n",
        "        else:\n",
        "            print(\"CONCLUSION: No strong correlation between entropy and path divergence.\")\n",
        "    else:\n",
        "        print(\"Insufficient data to calculate entropy-divergence correlation.\")\n",
        "\n",
        "    # Visualize the results\n",
        "    print(\"\\nGenerating visualizations...\")\n",
        "    analyzer.visualize_paths_with_entropy()\n",
        "    analyzer.visualize_trellis()\n",
        "\n",
        "    return analyzer\n",
        "\n",
        "# Helper function to adapt code to your specific trellis implementation\n",
        "def extract_trellis_data_with_logits(trellis, path_indices, token_logits=None):\n",
        "    \"\"\"\n",
        "    Extract tokens, states, scores, and logits from a trellis for a specific path.\n",
        "\n",
        "    Args:\n",
        "        trellis: Your trellis data structure\n",
        "        path_indices: The indices of the states in the chosen path\n",
        "        token_logits: Optional list of logits for each position (for entropy calculation)\n",
        "\n",
        "    Returns:\n",
        "        (tokens, states, scores, logits) tuples\n",
        "    \"\"\"\n",
        "    tokens = []\n",
        "    states = []\n",
        "    scores = []\n",
        "    logits = []\n",
        "\n",
        "    # Example implementation - adapt to your trellis structure\n",
        "    for t, idx in enumerate(path_indices):\n",
        "        if t < len(trellis) and idx < len(trellis[t]):\n",
        "            token = trellis[t][idx].token  # Assume trellis cells have .token attribute\n",
        "            state = trellis[t][idx].state  # Assume trellis cells have .state attribute\n",
        "            score = trellis[t][idx].score  # Assume trellis cells have .score attribute\n",
        "\n",
        "            tokens.append(token)\n",
        "            states.append(state)\n",
        "            scores.append(score)\n",
        "\n",
        "            # Add logits if available\n",
        "            if token_logits is not None and t < len(token_logits):\n",
        "                logits.append(token_logits[t])\n",
        "\n",
        "    return tokens, states, scores, logits"
      ],
      "metadata": {
        "trusted": true,
        "id": "9d80e0be-53d5-4a01-8180-0093ccab1920"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "beab9ff8-8f68-4b3b-83d7-d7ac37808fff",
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from collections import defaultdict\n",
        "import scipy.special\n",
        "\n",
        "class IterativeTrellisAnalyzer:\n",
        "    def __init__(self, model, tokenizer):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.iteration_paths = {}  # Will store paths for each iteration length\n",
        "        self.entropies = {}\n",
        "        self.trellis_paths = {}\n",
        "        self.scores = {}\n",
        "\n",
        "    def add_iteration_result(self, iteration_length, viterbi_path, beam_path, greedy_path,all_trellis_path_ids):\n",
        "        \"\"\"\n",
        "        Record decoding paths for a specific iteration length.\n",
        "\n",
        "        Args:\n",
        "            iteration_length: Number of tokens generated\n",
        "            viterbi_path: List of state indices for the Viterbi path\n",
        "            beam_path: List of state indices for the beam search path\n",
        "            greedy_path: List of state indices for the greedy search path\n",
        "        \"\"\"\n",
        "        self.iteration_paths[iteration_length] = {\n",
        "            'viterbi': viterbi_path,\n",
        "            'beam': beam_path,\n",
        "            'greedy': greedy_path\n",
        "        }\n",
        "        self.trellis_paths[iteration_length] = all_trellis_path_ids  #added this\n",
        "\n",
        "    def find_path_shift_points(self):\n",
        "        \"\"\"\n",
        "        Find points where paths shift between iterations.\n",
        "        For example, when going from 3 tokens to 4 tokens, does the path for the first 3 tokens change?\n",
        "        \"\"\"\n",
        "        iterations = sorted(self.iteration_paths.keys())\n",
        "        print(\"iterations in find_path_shift points: \", iterations)\n",
        "        shifts = {}\n",
        "\n",
        "        for i in range(len(iterations) - 1):\n",
        "            curr_iter = iterations[i]\n",
        "            next_iter = iterations[i+1]\n",
        "\n",
        "            # For each decoding method\n",
        "            for method in ['viterbi', 'beam', 'greedy']:\n",
        "                curr_path = self.iteration_paths[curr_iter].get(method, [])\n",
        "                next_path = self.iteration_paths[next_iter].get(method, [])\n",
        "\n",
        "                # Compare paths up to the length of the shorter path\n",
        "                min_len = min(len(curr_path), len(next_path))\n",
        "                if min_len == 0:\n",
        "                    continue\n",
        "\n",
        "                # Check if paths diverge\n",
        "                diverges = False\n",
        "                diverge_pos = None\n",
        "\n",
        "                for pos in range(min_len):\n",
        "                    if curr_path[pos] != next_path[pos]:\n",
        "                        diverges = True\n",
        "                        diverge_pos = pos\n",
        "                        break\n",
        "\n",
        "                key = f\"{method}_{curr_iter}_to_{next_iter}\"\n",
        "                shifts[key] = {\n",
        "                    'method': method,\n",
        "                    'from_iteration': curr_iter,\n",
        "                    'to_iteration': next_iter,\n",
        "                    'diverges': diverges,\n",
        "                    'diverge_position': diverge_pos,\n",
        "                    'path_before': curr_path[:min_len],\n",
        "                    'path_after': next_path[:min_len]\n",
        "                }\n",
        "\n",
        "        return shifts\n",
        "\n",
        "    def compare_paths_at_iteration(self, iteration):\n",
        "        \"\"\"\n",
        "        Compare paths between different decoding methods at a specific iteration length.\n",
        "        \"\"\"\n",
        "        if iteration not in self.iteration_paths:\n",
        "            return None\n",
        "\n",
        "        paths = self.iteration_paths[iteration]\n",
        "        comparisons = {}\n",
        "\n",
        "        # Compare Viterbi with other methods\n",
        "        viterbi_path = paths.get('viterbi', [])\n",
        "        for method in ['beam', 'greedy']:\n",
        "            other_path = paths.get(method, [])\n",
        "            min_len = min(len(viterbi_path), len(other_path))\n",
        "\n",
        "            if min_len == 0:\n",
        "                continue\n",
        "\n",
        "            # Check for path divergence\n",
        "            diverges = False\n",
        "            diverge_pos = None\n",
        "\n",
        "            for pos in range(min_len):\n",
        "                if viterbi_path[pos] != other_path[pos]:\n",
        "                    diverges = True\n",
        "                    diverge_pos = pos\n",
        "                    break\n",
        "\n",
        "            comparisons[f\"viterbi_vs_{method}\"] = {\n",
        "                'iteration': iteration,\n",
        "                'diverges': diverges,\n",
        "                'diverge_position': diverge_pos,\n",
        "                'viterbi_path': viterbi_path[:min_len],\n",
        "                'other_path': other_path[:min_len],\n",
        "                'other_method': method\n",
        "            }\n",
        "\n",
        "        return comparisons\n",
        "\n",
        "    def decoder_in_trellis(self,iteration,beam_path,greedy_path,trellis_paths):\n",
        "      # Convert tensor outputs to lists for comparison\n",
        "      if hasattr(beam_path, 'cpu'):  # Check if it's a tensor\n",
        "          beam_list = beam_path[0].cpu().tolist()\n",
        "      else:\n",
        "          beam_list = beam_path[0] if isinstance(beam_path, list) else list(beam_path)\n",
        "\n",
        "      if hasattr(greedy_path, 'cpu'):  # Check if it's a tensor\n",
        "          greedy_list = greedy_path[0].cpu().tolist()\n",
        "      else:\n",
        "          greedy_list = greedy_path[0] if isinstance(greedy_path, list) else list(greedy_path)\n",
        "\n",
        "      # Compare with each path in trellis_paths\n",
        "      for path in trellis_paths:\n",
        "          # Convert path to list if it's not already\n",
        "          path_list = path if isinstance(path, list) else list(path)\n",
        "\n",
        "          if beam_list == path_list or greedy_list == path_list:\n",
        "              return True\n",
        "\n",
        "      return False\n",
        "    def analyze_all_iterations(self):\n",
        "        \"\"\"\n",
        "        Analyze path divergence across all recorded iterations.\n",
        "        \"\"\"\n",
        "        results = {\n",
        "            'path_shifts': self.find_path_shift_points(),\n",
        "            'method_comparisons': {}\n",
        "        }\n",
        "\n",
        "        for iteration in sorted(self.iteration_paths.keys()):\n",
        "            comparison = self.compare_paths_at_iteration(iteration)\n",
        "            if comparison:\n",
        "                results['method_comparisons'][iteration] = comparison\n",
        "\n",
        "        return results\n",
        "\n",
        "    def visualize_path_stability(self, max_states=10, output_file=None):\n",
        "        \"\"\"\n",
        "        Visualize how paths change across iterations.\n",
        "\n",
        "        Args:\n",
        "            max_states: Maximum number of states to show in the visualization\n",
        "            output_file: If provided, save the figure to this file\n",
        "        \"\"\"\n",
        "        iterations = sorted(self.iteration_paths.keys())\n",
        "        if not iterations:\n",
        "            print(\"No iteration data to visualize.\")\n",
        "            return\n",
        "\n",
        "        methods = ['viterbi', 'beam', 'greedy']\n",
        "        colors = {'viterbi': 'blue', 'beam': 'green', 'greedy': 'red'}\n",
        "\n",
        "        # Create a figure with subplots for each method\n",
        "        fig, axes = plt.subplots(len(methods), 1, figsize=(14, 4 * len(methods)), sharex=True)\n",
        "\n",
        "        for i, method in enumerate(methods):\n",
        "            ax = axes[i] if len(methods) > 1 else axes\n",
        "\n",
        "            # Create a matrix of paths for this method across iterations\n",
        "            # Each row is an iteration, each column is a position in the path\n",
        "            max_iteration_len = max(len(self.iteration_paths[it].get(method, []))\n",
        "                                   for it in iterations)\n",
        "\n",
        "            path_matrix = np.ones((len(iterations), max_iteration_len)) * np.nan\n",
        "\n",
        "            # Fill the matrix with path data\n",
        "            for j, iteration in enumerate(iterations):\n",
        "                path = self.iteration_paths[iteration].get(method, [])\n",
        "                for k, state in enumerate(path):\n",
        "                    if state < max_states:  # Only include states below max_states\n",
        "                        path_matrix[j, k] = state\n",
        "\n",
        "            # Create a heatmap\n",
        "            im = ax.imshow(path_matrix, aspect='auto', cmap='viridis',\n",
        "                          interpolation='nearest', vmin=0, vmax=max_states-1)\n",
        "\n",
        "            # Add colorbar\n",
        "            cbar = fig.colorbar(im, ax=ax, orientation='vertical')\n",
        "            cbar.set_label('State index')\n",
        "\n",
        "            # Add labels and title\n",
        "            ax.set_ylabel('Iteration length')\n",
        "            ax.set_title(f'{method.capitalize()} paths across iterations')\n",
        "\n",
        "            # Set y-tick labels as iteration lengths\n",
        "            ax.set_yticks(range(len(iterations)))\n",
        "            ax.set_yticklabels(iterations)\n",
        "\n",
        "            # Highlight path shifts\n",
        "            shifts = self.find_path_shift_points()\n",
        "            for key, info in shifts.items():\n",
        "                if info['method'] == method and info['diverges']:\n",
        "                    from_idx = iterations.index(info['from_iteration'])\n",
        "                    to_idx = iterations.index(info['to_iteration'])\n",
        "                    pos = info['diverge_position']\n",
        "                    if pos is not None and pos < max_iteration_len:\n",
        "                        ax.add_patch(plt.Rectangle((pos-0.5, to_idx-0.5), 1, 1,\n",
        "                                                 fill=False, edgecolor='red', linewidth=2))\n",
        "\n",
        "        # Set common x-label\n",
        "        fig.text(0.5, 0.04, 'Position in sequence', ha='center')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        if output_file:\n",
        "            plt.savefig(output_file)\n",
        "        plt.show()\n",
        "\n",
        "    def visualize_method_comparison(self, iteration, output_file=None):\n",
        "        \"\"\"\n",
        "        Visualize path comparison between methods at a specific iteration length.\n",
        "\n",
        "        Args:\n",
        "            iteration: Iteration length to visualize\n",
        "            output_file: If provided, save the figure to this file\n",
        "        \"\"\"\n",
        "        if iteration not in self.iteration_paths:\n",
        "            print(f\"No data for iteration length {iteration}\")\n",
        "            return\n",
        "\n",
        "        paths = self.iteration_paths[iteration]\n",
        "        methods = [m for m in ['viterbi', 'beam', 'greedy'] if m in paths]\n",
        "\n",
        "        if not methods:\n",
        "            print(f\"No method data for iteration length {iteration}\")\n",
        "            return\n",
        "\n",
        "        # Get the max path length\n",
        "        max_len = max(len(paths[m]) for m in methods)\n",
        "\n",
        "        # Create a figure\n",
        "        plt.figure(figsize=(14, 6))\n",
        "\n",
        "        # Plot each path\n",
        "        for method in methods:\n",
        "            path = paths[method]\n",
        "            plt.plot(range(len(path)), path, 'o-', label=method)\n",
        "\n",
        "        # Add labels and title\n",
        "        plt.xlabel('Position in sequence')\n",
        "        plt.ylabel('State index')\n",
        "        plt.title(f'Path comparison at iteration length {iteration}')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        # Add annotations for divergence points\n",
        "        comparisons = self.compare_paths_at_iteration(iteration)\n",
        "        if comparisons:\n",
        "            for key, info in comparisons.items():\n",
        "                if info['diverges']:\n",
        "                    pos = info['diverge_position']\n",
        "                    if pos is not None:\n",
        "                        plt.axvline(x=pos, color='r', linestyle='--', alpha=0.5)\n",
        "                        plt.text(pos, plt.ylim()[0] * 0.9, f\"Divergence at {pos}\",\n",
        "                               rotation=90, color='red')\n",
        "\n",
        "        if output_file:\n",
        "            plt.savefig(output_file)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def visualize_trellis_with_multiple_paths(self, iteration, top_k=10, output_file=None):\n",
        "        \"\"\"\n",
        "        Visualize the trellis structure with paths from different methods and iterations.\n",
        "\n",
        "        Args:\n",
        "            iteration: Iteration length to visualize\n",
        "            top_k: Number of top states to show at each position\n",
        "            output_file: If provided, save the figure to this file\n",
        "        \"\"\"\n",
        "        if iteration not in self.iteration_paths:\n",
        "            print(f\"No data for iteration length {iteration}\")\n",
        "            return\n",
        "\n",
        "        paths = self.iteration_paths[iteration]\n",
        "        methods = [m for m in ['viterbi', 'beam', 'greedy'] if m in paths]\n",
        "\n",
        "        if not methods:\n",
        "            print(f\"No method data for iteration length {iteration}\")\n",
        "            return\n",
        "\n",
        "        # Get the max path length\n",
        "        max_len = max(len(paths[m]) for m in methods)\n",
        "\n",
        "        # Create a simple synthetic trellis for visualization\n",
        "        # In a real implementation, you would use your actual trellis data\n",
        "        synthetic_trellis = np.zeros((top_k, max_len))\n",
        "\n",
        "        # Fill in the paths we know about\n",
        "        for method in methods:\n",
        "            path = paths[method]\n",
        "            for t, state in enumerate(path):\n",
        "                if state < top_k:\n",
        "                    synthetic_trellis[state, t] = 1  # Mark this state as visited\n",
        "\n",
        "        # Create the visualization\n",
        "        plt.figure(figsize=(16, 10))\n",
        "\n",
        "        # Plot the trellis as a grid\n",
        "        plt.imshow(synthetic_trellis, cmap='Blues', alpha=0.3, aspect='auto')\n",
        "\n",
        "        # Plot each path\n",
        "        colors = {'viterbi': 'blue', 'beam': 'green', 'greedy': 'red'}\n",
        "        for method in methods:\n",
        "            path = paths[method]\n",
        "            y_coords = [state for state in path if state < top_k]\n",
        "            x_coords = list(range(len(y_coords)))\n",
        "            plt.plot(x_coords, y_coords, 'o-', label=method, color=colors.get(method, 'black'))\n",
        "\n",
        "        # Add labels and title\n",
        "        plt.xlabel('Position in sequence')\n",
        "        plt.ylabel('State index')\n",
        "        plt.title(f'Trellis paths at iteration length {iteration}')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        # Add y-axis ticks\n",
        "        plt.yticks(range(top_k))\n",
        "\n",
        "        if output_file:\n",
        "            plt.savefig(output_file)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "def analyze_paths_across_iterations(model, tokenizer,decode_fn, input_text, max_length=10):\n",
        "    \"\"\"\n",
        "    Analyze how paths change across different iteration lengths.\n",
        "\n",
        "    Args:\n",
        "        model: Your language model\n",
        "        tokenizer: Your tokenizer\n",
        "        decode_fn: Function to decode using your model - Returns token ids\n",
        "        input_text: The input text to decode\n",
        "        max_length: Maximum number of tokens to generate\n",
        "    \"\"\"\n",
        "    analyzer = IterativeTrellisAnalyzer(model, tokenizer)\n",
        "\n",
        "    # Run decoding for each length\n",
        "    for length in range(1, max_length + 1):\n",
        "        print(f\"Decoding iteration {length}...\")\n",
        "        viterbi_path, beam_path, greedy_path,trellis_paths = decode_fn(input_text, length)\n",
        "        analyzer.add_iteration_result(length, viterbi_path, beam_path, greedy_path,trellis_paths)\n",
        "    # Run analysis\n",
        "    print(\"\\n=== Path Stability Analysis ===\")\n",
        "    analysis = analyzer.analyze_all_iterations()\n",
        "\n",
        "    # Report path shifts\n",
        "    print(\"\\nPath shifts between iterations:\")\n",
        "    for key, info in analysis['path_shifts'].items():\n",
        "        if info['diverges']:\n",
        "            print(f\"{info['method'].capitalize()} path changes when going from {info['from_iteration']} to {info['to_iteration']} tokens\")\n",
        "            print(f\"  Divergence at position: {info['diverge_position']}\")\n",
        "            print(f\"  Path before: {info['path_before']}\")\n",
        "            print(f\"  Path after:  {info['path_after']}\")\n",
        "\n",
        "    # Report method differences\n",
        "    print(\"\\nMethod comparisons at each iteration:\")\n",
        "    for iteration, comparisons in analysis['method_comparisons'].items():\n",
        "        print(f\"\\nIteration length {iteration}:\")\n",
        "        for key, info in comparisons.items():\n",
        "            if info['diverges']:\n",
        "                print(f\"  {info['other_method'].capitalize()} diverges from Viterbi at position {info['diverge_position']}\")\n",
        "                print(f\"    Viterbi path: {info['viterbi_path']}\")\n",
        "                print(f\"    {info['other_method'].capitalize()} path: {info['other_path']}\")\n",
        "\n",
        "    # Visualize results\n",
        "    print(\"\\nGenerating visualizations...\")\n",
        "    analyzer.visualize_path_stability()\n",
        "\n",
        "    # Visualize a few specific iterations\n",
        "    mid_point = max_length // 2\n",
        "    analyzer.visualize_method_comparison(mid_point)\n",
        "    analyzer.visualize_trellis_with_multiple_paths(mid_point)\n",
        "\n",
        "    if max_length > 1:\n",
        "        analyzer.visualize_method_comparison(max_length)\n",
        "        analyzer.visualize_trellis_with_multiple_paths(max_length)\n",
        "\n",
        "    return analyzer\n",
        "\n",
        "def run_decoding_for_length(input_text, length,model,tokenizer):\n",
        "    \"\"\"\n",
        "    Run all decoding methods for a specific length.\n",
        "    Adapt this to your implementation.\n",
        "\n",
        "    Args:\n",
        "        input_text: Input text to decode\n",
        "        length: Number of tokens to generate\n",
        "\n",
        "    Returns:\n",
        "        (viterbi_path, beam_path, greedy_path) - each is a list of state indices\n",
        "    \"\"\"\n",
        "    tokenized_result = tokenizer(input_text,return_tensors = \"pt\")\n",
        "    input_ids = tokenized_result[\"input_ids\"].to(model.device)\n",
        "    input_length = len(input_ids[0])\n",
        "    attn_mask = tokenized_result['attention_mask'].to(model.device)\n",
        "    greedy_ids = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        max_length=input_length+length,\n",
        "        do_sample=False,  # Greedy\n",
        "        attention_mask = attn_mask\n",
        "    ).to(model.device)\n",
        "    greedy_text = tokenizer.decode(greedy_ids[0], skip_special_tokens=True)\n",
        "    # -- Beam search\n",
        "    beam_ids = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        max_length=input_length+length,\n",
        "        num_beams=3,    # for example\n",
        "        early_stopping=False,\n",
        "        attention_mask = attn_mask\n",
        "    )\n",
        "    beam_text = tokenizer.decode(beam_ids[0], skip_special_tokens=True)\n",
        "    decodedString,best_path_prob,generated_sentence_GI,viterbi_ids,trellis_paths = runViterbiTransformerPipeline(input_text, loop_runner = length)\n",
        "    print(\"decoded_string: \", decodedString)\n",
        "    print(\"greedy_text: \", greedy_text)\n",
        "    print(\"beam_text: \", beam_text)\n",
        "    return list(viterbi_ids[0]), list(beam_ids[0]), list(greedy_ids[0]),trellis_paths"
      ],
      "metadata": {
        "trusted": true,
        "id": "beab9ff8-8f68-4b3b-83d7-d7ac37808fff"
      },
      "outputs": [],
      "execution_count": 72
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "# model2 = LMHeadModel(model_name)\n",
        "model.eval()  # put model in inference mode\n",
        "\n",
        "# If using GPU (e.g., on Colab), you could also do:\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "obj = IterativeTrellisAnalyzer(model,tokenizer)\n",
        "viterbi_ids, beam_ids, greedy_ids,trellis_paths = run_decoding_for_length(\"I enjoy walking in the\", 5,model,tokenizer)\n",
        "print(viterbi_ids)\n",
        "print(beam_ids)\n",
        "print(greedy_ids)\n",
        "print(trellis_paths)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmHyRf4gIAjD",
        "outputId": "46e3c781-1b82-4313-9517-41ba25db9be6"
      },
      "id": "wmHyRf4gIAjD",
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "uniqueTokenLength:  [3, 4, 10, 23, 41]\n",
            "best_path:  [0, 2, 5, 9, 16]\n",
            "decoded_string:  I enjoy walking in the park. It's a\n",
            "greedy_text:  I enjoy walking in the park, but I'm\n",
            "beam_text:  I enjoy walking in the park, but I'm\n",
            "[tensor(40), tensor(2883), tensor(6155), tensor(287), tensor(262), tensor(3952), tensor(13), tensor(632), tensor(338), tensor(257)]\n",
            "[tensor(40), tensor(2883), tensor(6155), tensor(287), tensor(262), tensor(3952), tensor(11), tensor(475), tensor(314), tensor(1101)]\n",
            "[tensor(40), tensor(2883), tensor(6155), tensor(287), tensor(262), tensor(3952), tensor(11), tensor(475), tensor(314), tensor(1101)]\n",
            "[[40, 2883, 6155, 287, 262, 3952, 11, 475, 314, 1101], [40, 2883, 6155, 287, 262, 3952, 11, 475, 314, 836], [40, 2883, 6155, 287, 262, 3952, 11, 475, 314, 635], [40, 2883, 6155, 287, 262, 3952, 11, 475, 340, 338], [40, 2883, 6155, 287, 262, 3952, 11, 475, 340, 318], [40, 2883, 6155, 287, 262, 3952, 11, 475, 340, 460], [40, 2883, 6155, 287, 262, 3952, 11, 475, 618, 314], [40, 2883, 6155, 287, 262, 3952, 11, 475, 618, 345], [40, 2883, 6155, 287, 262, 3952, 11, 475, 618, 340], [40, 2883, 6155, 287, 262, 3952, 11, 290, 262, 661], [40, 2883, 6155, 287, 262, 3952, 11, 290, 262, 7150], [40, 2883, 6155, 287, 262, 3952, 11, 290, 262, 4950], [40, 2883, 6155, 287, 262, 3952, 11, 314, 1842, 262], [40, 2883, 6155, 287, 262, 3952, 11, 314, 1842, 284], [40, 2883, 6155, 287, 262, 3952, 11, 314, 1842, 852], [40, 2883, 6155, 287, 262, 3952, 11, 314, 1101, 407], [40, 2883, 6155, 287, 262, 3952, 11, 314, 1101, 257], [40, 2883, 6155, 287, 262, 3952, 11, 314, 1101, 1464], [40, 2883, 6155, 287, 262, 3952, 290, 4379, 661, 13], [40, 2883, 6155, 287, 262, 3952, 290, 4379, 661, 11], [40, 2883, 6155, 287, 262, 3952, 290, 4379, 661, 290], [40, 2883, 6155, 287, 262, 3952, 290, 4379, 477, 286], [40, 2883, 6155, 287, 262, 3952, 290, 4379, 477, 777], [40, 2883, 6155, 287, 262, 3952, 290, 340, 338, 3621], [40, 2883, 6155, 287, 262, 3952, 290, 340, 318, 523], [40, 2883, 6155, 287, 262, 3952, 290, 340, 318, 845], [40, 2883, 6155, 287, 262, 3952, 290, 340, 1838, 502], [40, 2883, 6155, 287, 262, 3952, 290, 340, 1838, 616], [40, 2883, 6155, 287, 262, 3952, 13, 198, 198, 40], [40, 2883, 6155, 287, 262, 3952, 13, 198, 198, 1], [40, 2883, 6155, 287, 262, 3952, 13, 198, 198, 464], [40, 2883, 6155, 287, 262, 3952, 13, 198, 40, 588], [40, 2883, 6155, 287, 262, 3952, 13, 198, 40, 1842], [40, 2883, 6155, 287, 262, 3952, 13, 198, 464, 3952], [40, 2883, 6155, 287, 262, 3952, 13, 198, 464, 717], [40, 2883, 6155, 287, 262, 3952, 13, 198, 464, 584], [40, 2883, 6155, 287, 262, 6483, 286, 968, 1971, 2254], [40, 2883, 6155, 287, 262, 6483, 286, 3576, 11, 475], [40, 2883, 6155, 287, 262, 6483, 286, 3576, 13, 632], [40, 2883, 6155, 287, 262, 6483, 286, 3576, 13, 198], [40, 2883, 6155, 287, 262, 6483, 286, 3576, 290, 4379]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ---------------------------------------------------------\n",
        "# Setup\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "import math\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from collections import defaultdict\n",
        "import re\n",
        "import itertools\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import datetime\n",
        "import pandas as pd\n",
        "from transformers import logging\n",
        "logging.set_verbosity_error()\n",
        "import math\n",
        "from time import sleep, time\n",
        "import os\n",
        "from scipy.stats import entropy\n",
        "\n",
        "\n",
        "class LMHeadModel:\n",
        "    def __init__(self, model_name, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
        "        # Initialize the model and tokenizer\n",
        "        self.device = device\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float32).to(self.device)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model.eval()\n",
        "\n",
        "        # Ensure the tokenizer has a padding token\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token  # Use EOS token as padding\n",
        "            self.tokenizer.padding_side = \"right\"\n",
        "\n",
        "        self.batch_prediction_count = 0\n",
        "\n",
        "\n",
        "    def batch_encode(self, sentences):\n",
        "        \"\"\"\n",
        "        Encodes a batch of sentences into input tensors.\n",
        "        Args:\n",
        "            sentences (list of str): The input sentences to encode.\n",
        "        Returns:\n",
        "            inputs (dict): A dictionary of tokenized inputs ready for the model.\n",
        "        \"\"\"\n",
        "        return self.tokenizer(\n",
        "            sentences,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,  # Pad to the longest sequence in the batch\n",
        "            truncation=True,  # Truncate sequences longer than the model's max length\n",
        "        ).to(self.device)\n",
        "\n",
        "    def batch_decode(self, token_ids):\n",
        "        \"\"\"\n",
        "        Decodes a batch of token IDs back to sentences.\n",
        "        Args:\n",
        "            token_ids (torch.Tensor): A tensor of token IDs to decode.\n",
        "        Returns:\n",
        "            decoded_sentences (list of str): The decoded sentences.\n",
        "        \"\"\"\n",
        "        return self.tokenizer.batch_decode(token_ids, skip_special_tokens=True)\n",
        "    def batch_decode_top_k(self, token_ids_batch, tokenizer):\n",
        "        \"\"\"\n",
        "        Decodes token IDs to meaningful text while merging subword tokens.\n",
        "        Args:\n",
        "            token_ids_batch (torch.Tensor): A batch of token IDs (e.g., from `topk`).\n",
        "            tokenizer: The tokenizer used for encoding/decoding.\n",
        "        Returns:\n",
        "            list of list of str: Decoded tokens (words/subwords) for each sequence in the batch.\n",
        "        \"\"\"\n",
        "        decoded_tokens = []\n",
        "        for token_ids in token_ids_batch:\n",
        "            # Decode each token ID in the batch, joining subwords correctly\n",
        "            tokens = [tokenizer.decode([token_id]).strip() for token_id in token_ids]\n",
        "            decoded_tokens.append(tokens)\n",
        "        return decoded_tokens\n",
        "\n",
        "    def get_batch_predictions(self, sentences, top_k=100):\n",
        "        \"\"\"\n",
        "        Predicts the next tokens for a batch of input sentences.\n",
        "        Args:\n",
        "            sentences (list of str): The input sentences.\n",
        "            top_k (int): Number of top tokens to return for each sentence.\n",
        "        Returns:\n",
        "            predictions (list of list of tuples): Top-k token predictions for each sentence.\n",
        "        \"\"\"\n",
        "        #Increment to see how many times this function is called after a given layer of trellis.\n",
        "        self.batch_prediction_count += 1\n",
        "\n",
        "\n",
        "        # Tokenize inputs\n",
        "        inputs = self.batch_encode(sentences)\n",
        "\n",
        "        # Pass through the model\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs,use_cache = False)\n",
        "\n",
        "        # Get logits for the last token in each sequence\n",
        "        logits = outputs.logits[:, -1, :]  # Shape: (batch_size, vocab_size)\n",
        "\n",
        "\n",
        "        # Compute probabilities using softmax\n",
        "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "        top_probs, top_token_ids = torch.topk(probs, k=top_k, dim=-1)\n",
        "        top_tokens = self.batch_decode_top_k(top_token_ids, self.tokenizer)\n",
        "\n",
        "\n",
        "        predictions = [\n",
        "            [(token, prob.item()) for token, prob in zip(top_tokens[i], top_probs[i]) if token and token != \"\\n\"]\n",
        "            for i in range(len(sentences))\n",
        "        ]\n",
        "        return predictions\n",
        "\n",
        "    def get_batch_prediction_count(self):\n",
        "        \"\"\"\n",
        "        Returns the number of times batch predictions have been made.\n",
        "        \"\"\"\n",
        "        return self.batch_prediction_count\n",
        "\n",
        "    def reset_batch_prediction_count(self):\n",
        "        \"\"\" Resets the count\n",
        "        \"\"\"\n",
        "\n",
        "        self.batch_prediction_count = 0\n",
        "class SearchTree:\n",
        "    def __init__(self,context,probability,token_id,model,tokenizer,parent = None,child = None,parent_index = None):\n",
        "        self.token_id = token_id\n",
        "        context = context.strip()\n",
        "        self.context = context\n",
        "        self.probability = probability\n",
        "        self.parent = parent\n",
        "        self.child = []\n",
        "        self.parent_index = parent_index  # newly created.\n",
        "        if child is not None:\n",
        "           self.child.append(child)\n",
        "\n",
        "        # Cache cumulative probability at node creation\n",
        "        if parent:\n",
        "            self.cached_prob = parent.calcProbTillNow()+probability #parent.calcProbTillNow() * probability\n",
        "            self.cached_tokenids = parent.token_idsTillNow() + [token_id]\n",
        "\n",
        "        else:\n",
        "            self.cached_prob = probability\n",
        "            self.cached_tokenids = [token_id] if isinstance(token_id, int) else token_id\n",
        "\n",
        "    def build_Context(self):\n",
        "        context_list = []\n",
        "        full_context = []\n",
        "        node = self\n",
        "        while node.parent is not None:\n",
        "            context_list.extend([node.token_id])\n",
        "            node = node.parent\n",
        "        context_list.reverse()\n",
        "        full_context.extend(node.token_id)\n",
        "        full_context.extend(context_list)\n",
        "        full_context = torch.tensor([full_context])\n",
        "        generated_sentence = tokenizer.decode(full_context[0], skip_special_tokens=True)\n",
        "        return generated_sentence\n",
        "\n",
        "\n",
        "    def create_child(self):\n",
        "        if self.parent is not None:\n",
        "           self.parent.child.append(self)\n",
        "\n",
        "    def replace_parent(self, new_parent):\n",
        "        \"\"\"Assign a new parent and update cached probability.\"\"\"\n",
        "        self.parent = new_parent\n",
        "        self.cached_prob = new_parent.calcProbTillNow() + self.probability\n",
        "\n",
        "\n",
        "    def calcProbTillNow(self):\n",
        "        \"\"\"Return cached cumulative probability to avoid redundant calculations.\"\"\"\n",
        "        return self.cached_prob\n",
        "\n",
        "    def token_idsTillNow(self):\n",
        "        \"Return the token ids of all the tokens up till now. This method is to be used in the last step of the trellis to get all the paths in there.\"\n",
        "        return self.cached_tokenids\n",
        "    def change_probability(self,new_probability,new_cached_prob):\n",
        "        self.cached_prob = new_cached_prob\n",
        "        self.probability = new_probability\n",
        "\n",
        "    # def calcProbTillNow(self):\n",
        "    #   prob = self.probability\n",
        "    #   node = self\n",
        "    #   while node.parent is not None:\n",
        "    #     prob = prob*node.parent.probability\n",
        "    #     node = node.parent\n",
        "    #   return prob    #can make this negative log probability.\n",
        "\n",
        "    def assign_parent_index(self,parent_index):\n",
        "      self.parent_index = parent_index\n",
        "\n",
        "\n",
        "def findProbability(InitialToken, FinalTokens, model,tokenizer):\n",
        "    context = InitialToken.build_Context()\n",
        "    with torch.no_grad():\n",
        "       tokens_50K = generate_token_and_probability(model, tokenizer, [context], top_k=500)\n",
        "    token_dict = {}  # Dictionary to store only the first occurrence of each token\n",
        "\n",
        "    for _,token_id,token, prob in tokens_50K[0]:\n",
        "        # token = token.strip()\n",
        "        if token_id.item() not in token_dict or prob>token_dict[token_id.item()]:  # Store only the first occurrence\n",
        "            token_dict[token_id.item()] = prob\n",
        "    return [token_dict.get(FinalToken.token_id, -math.inf) for FinalToken in FinalTokens]  # Return probability if found, else 0\n",
        "\n",
        "\n",
        "\n",
        "def VITERBI_Lists(state_transition_probmat, initial_state_prob, device):\n",
        "    # Convert inputs to PyTorch tensors on GPU\n",
        "    initial_state_tensor = torch.tensor(initial_state_prob,dtype=torch.float32).to(device)\n",
        "\n",
        "    # Initialize with first layer\n",
        "    viterbi_tensor = [initial_state_tensor]\n",
        "    backpointer = []\n",
        "\n",
        "    # Process each time step\n",
        "    for time_step in range(len(state_transition_probmat)):\n",
        "        # Convert transition matrix for this time step to tensor\n",
        "        trans_probs = torch.tensor(state_transition_probmat[time_step],dtype=torch.float32).to(device)\n",
        "\n",
        "        # Create matrices for vectorization\n",
        "        # Shape: [num_prev_states, num_current_states]\n",
        "        prev_probs = viterbi_tensor[-1].unsqueeze(1)\n",
        "\n",
        "        # Calculate all state transitions at once using matrix operations\n",
        "        # This replaces the inner loop over states\n",
        "        iteration_mat = prev_probs + trans_probs.t()\n",
        "\n",
        "        # Find max values and indices in one operation\n",
        "        maxval, maxind = torch.max(iteration_mat, dim=0)\n",
        "\n",
        "        viterbi_tensor.append(maxval)\n",
        "        backpointer.append(maxind.cpu().tolist())  # Move indices back to CPU for path tracking\n",
        "\n",
        "    # Find best path\n",
        "    final_probs = viterbi_tensor[-1]\n",
        "    best_path_prob, max_index = torch.max(final_probs, dim=0)\n",
        "    best_backpointer = max_index.item()\n",
        "\n",
        "    # Backtrack to find path\n",
        "    best_path = [best_backpointer]\n",
        "    j = 0\n",
        "    for i in reversed(range(len(state_transition_probmat))):\n",
        "        best_path.append(backpointer[i][best_path[j]])\n",
        "        j += 1\n",
        "\n",
        "    best_path = best_path[::-1]\n",
        "\n",
        "    # Convert tensors back to lists for return\n",
        "    viterbi_mat = [tensor.cpu().tolist() for tensor in viterbi_tensor]\n",
        "\n",
        "    return best_path, viterbi_mat, best_path_prob.item()\n",
        "\n",
        "def VITERBI_Lists_2(state_transition_probmat, initial_state_prob):\n",
        "\n",
        "    viterbi_mat = []\n",
        "    backpointer = []\n",
        "    viterbi_1stLayer = []\n",
        "    for i in range(len(initial_state_prob)):\n",
        "        viterbi_1stLayer.append(float(initial_state_prob[i]))\n",
        "    viterbi_mat.append(viterbi_1stLayer)\n",
        "\n",
        "    for time_step in range(len(state_transition_probmat)):\n",
        "        viterbi_layer = []\n",
        "        backpointer_layer = []\n",
        "        for state in range(len(state_transition_probmat[time_step])):\n",
        "            iteration_vec = [viterbi_mat[time_step][i]+state_transition_probmat[time_step][state][i] for i in range(len(viterbi_mat[time_step]))]\n",
        "\n",
        "            maxval = max(iteration_vec)\n",
        "\n",
        "#             print(\"time_step: \", time_step)\n",
        "#             print(\"maxval: \",maxval)\n",
        "#             print(iteration_vec)\n",
        "            maxind = iteration_vec.index(maxval)\n",
        "            viterbi_layer.append(maxval)\n",
        "            backpointer_layer.append(maxind)\n",
        "\n",
        "        viterbi_mat.append(viterbi_layer)\n",
        "        backpointer.append(backpointer_layer)\n",
        "\n",
        "    best_path_prob = max(viterbi_mat[-1])\n",
        "    # max_index = max(range(len(viterbi_mat[-1])), key = lambda i: viterbi_mat[-1][i])\n",
        "    max_index = viterbi_mat[-1].index(best_path_prob)\n",
        "    best_backpointer = max_index\n",
        "    best_path = [best_backpointer]\n",
        "    j = 0\n",
        "    for i in reversed(range(len(state_transition_probmat))):\n",
        "        best_path.append(backpointer[i][best_path[j]])\n",
        "        j += 1\n",
        "    best_path = best_path[::-1]\n",
        "    return best_path, viterbi_mat,best_path_prob\n",
        "def decodePath(best_path,unique_tokens_list,root_string,tokenizer):\n",
        "    resultant_token_ids = []\n",
        "    root_ids = tokenizer.encode(root_string)\n",
        "    resultant_token_ids.extend(root_ids)\n",
        "    resultant_token_ids.extend([unique_tokens_list[i][best_path[i]].token_id for i in range(len(best_path))])\n",
        "    # generated_sentence = tokenizer.decode(resultant_token_ids[0], skip_special_tokens=True)\n",
        "    resultant_token_ids = torch.tensor([resultant_token_ids])\n",
        "    generated_sentence = tokenizer.decode(resultant_token_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    return generated_sentence, resultant_token_ids\n",
        "\n",
        "def generate_token_and_probabilityO(model, tokenizer, batch_prompts,max_length=1, top_k=4):\n",
        "    tokenizer.pad_token= tokenizer.eos_token\n",
        "    tokenized_result = tokenizer(batch_prompts, return_tensors=\"pt\",padding = True,truncation = True)\n",
        "    input_ids = tokenized_result[\"input_ids\"].to(model.device)\n",
        "    # for i in range(len(input_ids)):\n",
        "    #     if input_ids[i][-1] == 50256 and probabilityMatrix is not None: #50256 is end of text token or the pad token\n",
        "    #         newToken = SearchTree(model.decode(input_ids[i][-2]),0,input_ids[i][-2].item(),model,tokenizer,parent = uniqueTokensList[i])\n",
        "    #         prob = findProbability(uniqueTokensList[i].parent,newToken,model,tokenizer)\n",
        "    #         newToken.change_probability(prob)\n",
        "    #         probabilityMatrix[i] = prob #probabilityMatrix of one previous iteration should be changed to 0\n",
        "    #         uniqueTokensList[i] = newToken\n",
        "\n",
        "    attn_mask = tokenized_result[\"attention_mask\"].to(model.device)\n",
        "    with torch.no_grad():\n",
        "      outputs = model.generate(\n",
        "         input_ids=input_ids,\n",
        "         attention_mask=attn_mask,\n",
        "         max_length=input_ids.size(-1) + max_length,\n",
        "         do_sample=False,  # Greedy decoding\n",
        "         output_scores=True,\n",
        "         return_dict_in_generate=True\n",
        "        )\n",
        "\n",
        "    sequences, scores = outputs.sequences, outputs.scores  # scores will have only one element per batch\n",
        "    predictions = []\n",
        "    # print(\"generated_token_id\",generated_token_id)\n",
        "    for i in range(len(batch_prompts)):\n",
        "        generated_token_id = sequences[i][input_ids.size(-1):].tolist()[0]  # Extract generated token ID\n",
        "        generated_token = tokenizer.decode(generated_token_id, skip_special_tokens=True)\n",
        "\n",
        "\n",
        "        # Log probabilities of all possible tokens at the generated step\n",
        "        log_probs = torch.nn.functional.log_softmax(scores[0][i], dim=-1)  # scores[0] corresponds to the single generation step\n",
        "        # Keep increasing top_k until we have enough valid tokens\n",
        "        valid_predictions = []\n",
        "        curr_top_k = top_k\n",
        "        while len(valid_predictions) < top_k:\n",
        "            topk_logprobs, topk_ids = log_probs.topk(curr_top_k)  # Get top-k log probabilities\n",
        "            topk_tokens = tokenizer.batch_decode(topk_ids, skip_special_tokens=True)\n",
        "\n",
        "            # Filter out token `198` if the last token in input_ids was `198`\n",
        "            if input_ids[i][-1] == 198:\n",
        "                valid_predictions = [\n",
        "                    (generated_token,tid, tok, lp.item()) for tid, tok, lp in zip(topk_ids, topk_tokens, topk_logprobs) if tid != 198\n",
        "                ]\n",
        "            else:\n",
        "                valid_predictions = [\n",
        "                    (generated_token,tid, tok, lp.item()) for tid, tok, lp in zip(topk_ids, topk_tokens, topk_logprobs)\n",
        "                ]\n",
        "\n",
        "            # If filtering removed tokens, increase `curr_top_k` to get more candidates\n",
        "            if len(valid_predictions) < top_k:\n",
        "                curr_top_k += 1  # Expand search to get more tokens\n",
        "\n",
        "        # Store only the required `top_k` valid predictions\n",
        "        predictions.append(valid_predictions[:top_k])\n",
        "\n",
        "    return predictions\n",
        "\n",
        "def generate_token_and_probability(model, tokenizer, batch_prompts,max_length=1, top_k=4):\n",
        "    tokenizer.pad_token= tokenizer.eos_token\n",
        "    tokenized_result = tokenizer(batch_prompts, return_tensors=\"pt\",padding = True,truncation = True)\n",
        "    input_ids = tokenized_result[\"input_ids\"].to(model.device)\n",
        "    attn_mask = tokenized_result[\"attention_mask\"].to(model.device)\n",
        "    #Added new stuff for handling newlines below\n",
        "    for sentence_id in range(len(input_ids)):\n",
        "        if (input_ids[sentence_id][-1] == 50256 and input_ids[sentence_id][-2] == 628):\n",
        "            input_ids[sentence_id][-2] = 198\n",
        "            input_ids[sentence_id][-1] = 198\n",
        "            attn_mask[sentence_id][-1] = 1\n",
        "            attn_mask[sentence_id][-2] = 1\n",
        "\n",
        "    num_sentences = len(input_ids)\n",
        "    if (num_sentences == 1):\n",
        "        if (input_ids[0][-1] == 628):\n",
        "            input_ids = input_ids[:, :-1]\n",
        "            attn_mask = attn_mask[:, :-1]\n",
        "            input_ids = torch.cat((input_ids,torch.tensor([[198,198]]).to(model.device)),dim = 1)\n",
        "            attn_mask = torch.cat((attn_mask,torch.tensor([[1,1]]).to(model.device)),dim = 1)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      outputs = model.generate(\n",
        "         input_ids=input_ids,\n",
        "         attention_mask=attn_mask,\n",
        "         max_length=input_ids.size(-1) + max_length,\n",
        "         do_sample=False,  # Greedy decoding\n",
        "         output_scores=True,\n",
        "         return_dict_in_generate=True\n",
        "        )\n",
        "\n",
        "    sequences, scores = outputs.sequences, outputs.scores  # scores will have only one element per batch\n",
        "    predictions = []\n",
        "    # print(\"generated_token_id\",generated_token_id)\n",
        "    for i in range(len(batch_prompts)):\n",
        "        generated_token_id = sequences[i][input_ids.size(-1):].tolist()[0]  # Extract generated token ID\n",
        "        generated_token = tokenizer.decode(generated_token_id, skip_special_tokens=True)\n",
        "\n",
        "\n",
        "        # Log probabilities of all possible tokens at the generated step\n",
        "        log_probs = torch.nn.functional.log_softmax(scores[0][i], dim=-1)  # scores[0] corresponds to the single generation step\n",
        "        # Keep increasing top_k until we have enough valid tokens\n",
        "        valid_predictions = []\n",
        "        curr_top_k = top_k\n",
        "        topk_logprobs, topk_ids = log_probs.topk(curr_top_k)  # Get top-k log probabilities\n",
        "        topk_tokens = tokenizer.batch_decode(topk_ids, skip_special_tokens=True)\n",
        "        predictions.append([(generated_token,tid, tok, lp.item()) for tid, tok, lp in zip(topk_ids, topk_tokens, topk_logprobs)])\n",
        "\n",
        "\n",
        "    return predictions\n",
        "\n",
        "\n",
        "def check_bad_predictions(text):\n",
        "    bad_patterns = [r'={2,}', r'!{2,}', r'\\?{2,}', r',{2,}', r';{2,}', r'\\|{2,}', r'~{2,}', r'&{2,}', r'-{2,}']\n",
        "\n",
        "    # Check for unwanted punctuation patterns (two or more consecutive occurrences)\n",
        "    for pattern in bad_patterns:\n",
        "        if re.search(pattern, text):\n",
        "            return True\n",
        "\n",
        "    # Check for non-ASCII characters\n",
        "    if any(ord(char) > 127 for char in text):  # ASCII characters are in the range 0-127\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "def generateIntermediates(root,model,tokenizer,numTokens = 3, loop_runner = 4,**kwargs):\n",
        "  root_token_id = tokenizer.encode(root)\n",
        "  sentence = SearchTree(root,0,token_id =root_token_id,model = model, tokenizer = tokenizer)\n",
        "  context = []\n",
        "  entropy_array = []\n",
        "  num_tokens = numTokens\n",
        "  content = []\n",
        "  probability = []\n",
        "  with torch.no_grad():\n",
        "     tokens_50K = generate_token_and_probability(model, tokenizer, [root], top_k=numTokens)\n",
        "\n",
        "  #unique_elements = []   # to store unique elements at each iteration\n",
        "  unique_tokens = set()\n",
        "  probabilityMatrix = []\n",
        "  uniqueTokensList = []\n",
        "  new_content = []\n",
        "  uniqueTokenLength = []\n",
        "  lastTokens_probability = []\n",
        "  flops_counter = {}\n",
        "  generated_sentence_GI = ''\n",
        "  batch_size = 15\n",
        "  holdout_number = 15\n",
        "  trellis_paths = []\n",
        "  initial_loop_probability = []\n",
        "  for i in range(num_tokens):\n",
        "    _,token_id,context,prob = tokens_50K[0][i]  # Assuming it's structured as a tuple (best_token, token, probability)\n",
        "    # context = context.strip()  #This is not the correct solution. I am doing this rather than only leaving one strip command in search tree because I am appending to unique tokens before I am assigning this to search tree.\n",
        "    initial_loop_probability.append(prob)\n",
        "#     print(\"initial_loop:\")\n",
        "#     print(tokens_50K[0][i])\n",
        "    unique_tokens.add(context)\n",
        "    probability.append(prob)\n",
        "    context = SearchTree(context,prob,token_id = token_id.item(),model = model,tokenizer = tokenizer,parent =sentence,parent_index = 0)\n",
        "    new_content.append(context)\n",
        "    context.create_child()\n",
        "    uniqueTokensList.append(context)\n",
        "\n",
        "  entropy_array.append([entropy(np.array([math.exp(prob) for prob in probability]))])\n",
        "  content.append(new_content)\n",
        "  previousUniqueLength = num_tokens\n",
        "  #unique_elements.append(unique_tokens)\n",
        "  initialStateProbability = probability\n",
        "  uniqueTokenLength.append(num_tokens)\n",
        "  max_index = initial_loop_probability.index(max(initial_loop_probability))\n",
        "  generated_sentence_GI = uniqueTokensList[max_index].build_Context()\n",
        "  for i in range(2,loop_runner):\n",
        "    unique_tokens = set()\n",
        "    probability = []\n",
        "    entropies = []\n",
        "    new_content = []\n",
        "    total_predictions = []\n",
        "    previousSetLength = 0\n",
        "    batch_sentences = [child.build_Context() for child in uniqueTokensList]\n",
        "\n",
        "    if len(batch_sentences)>batch_size:\n",
        "        total_predictions = []\n",
        "        start_index = 0\n",
        "        num_sentences_left = len(batch_sentences)\n",
        "        while (num_sentences_left>batch_size):\n",
        "            batch_sentences2 = batch_sentences[start_index*batch_size:(start_index+1)*batch_size]\n",
        "            with torch.no_grad():\n",
        "              batch_predictions =  generate_token_and_probability(model, tokenizer, batch_sentences2, top_k=numTokens)\n",
        "            total_predictions.extend(batch_predictions)\n",
        "            start_index +=1\n",
        "            num_sentences_left -= batch_size\n",
        "        if num_sentences_left > 0:\n",
        "           batch_sentences2 = batch_sentences[start_index*batch_size :]\n",
        "           with torch.no_grad():\n",
        "             batch_predictions =  generate_token_and_probability(model, tokenizer, batch_sentences2, top_k=numTokens)\n",
        "           total_predictions.extend(batch_predictions)\n",
        "    else:\n",
        "        with torch.no_grad():\n",
        "            total_predictions = generate_token_and_probability(model, tokenizer, batch_sentences,top_k=numTokens)\n",
        "\n",
        "    for j in range(len(uniqueTokensList)):\n",
        "      for s in range(num_tokens):\n",
        "        _,token_id,context,prob = total_predictions[j][s]\n",
        "        context2 = context.strip()\n",
        "        #bad_predictions_checker = check_bad_predictions(context2)\n",
        "        # if context2:\n",
        "        unique_tokens.add(context)   # also this if condition is not the correct solution\n",
        "        context = SearchTree(context,prob,token_id = token_id.item(),model = model,tokenizer = tokenizer,parent = uniqueTokensList[j])   #probably redundant: Because I should only create SearchTree of unique tokens\n",
        "        # context.create_child() Removed this 2/19/2025\n",
        "        if (len(unique_tokens)>previousSetLength):\n",
        "          previousSetLength = len(unique_tokens)\n",
        "          uniqueTokensList.append(context)\n",
        "          new_content.append(context)\n",
        "\n",
        "    #unique_elements.append(unique_tokens) # append the unique tokens list at each iteration to unique_elements list\n",
        "    content.append(new_content) # for storing tokens which will pass to the decode_path function.\n",
        "\n",
        "\n",
        "    comb_prob = []\n",
        "    for prevToken in uniqueTokensList[:previousUniqueLength]:\n",
        "      comb_prob.append(findProbability(prevToken,uniqueTokensList[previousUniqueLength:], model,tokenizer))\n",
        "    comb_prob = list(itertools.chain(*comb_prob)) # flattening the list\n",
        "\n",
        "    for tokenumber,newToken in enumerate(uniqueTokensList[previousUniqueLength:]):\n",
        "      probs = [comb_prob[a*len(uniqueTokensList[previousUniqueLength:]) + tokenumber] for a in range(len(uniqueTokensList[:previousUniqueLength]))]\n",
        "      probs2 = [probs[b] + uniqueTokensList[:previousUniqueLength][b].calcProbTillNow() for b in range(len(probs))]\n",
        "#       print(\"parent_prob Up Till now: \",[uniqueTokensList[:previousUniqueLength][i].calcProbTillNow() for i in range(len(probs))])\n",
        "#       print(\"combined probs: \", probs2)\n",
        "#       print(\"actual_probs: \", [math.exp(probs2[i]) for i in range(len(probs2))])\n",
        "      if not probs2:\n",
        "        continue\n",
        "      else:\n",
        "        max_value = max(probs2)\n",
        "        max_index = probs2.index(max_value)\n",
        "        new_transition_probability = probs[max_index]\n",
        "        newToken.replace_parent(uniqueTokensList[:previousUniqueLength][max_index])\n",
        "        newToken.change_probability(new_transition_probability,max_value) # just added this 4/4/2025\n",
        "        newToken.assign_parent_index(max_index)\n",
        "        if (i == loop_runner-1):\n",
        "             #print(\"parent_assigning_loop\")\n",
        "#             print(tokenumber)\n",
        "             #print(\"uniqueToken.context: \",uniqueTokensList[previousUniqueLength+tokenumber].context)\n",
        "#             print(\"parent_calc_prob_till_now: \",probs3)\n",
        "#             print(\"new_context: \",uniqueTokensList[previousUniqueLength+tokenumber].build_Context())\n",
        "             #print(\"new_transition_probability: \", new_transition_probability)\n",
        "#             print(\"new_total_prob: \", max_value)\n",
        "            lastTokens_probability.append(max_value)\n",
        "            trellis_paths.append(newToken.token_idsTillNow())\n",
        "\n",
        "      probability.append(probs)\n",
        "    probabilityMatrix.append(probability)\n",
        "    #entropy_array.append(entropies)\n",
        "    # flops_counter[i-1] = model.get_batch_prediction_count()\n",
        "    #model.reset_batch_prediction_count()\n",
        "\n",
        "    uniqueTokenLength.append(len(uniqueTokensList[previousUniqueLength:]))\n",
        "\n",
        "    previousUniqueLength = len(uniqueTokensList[previousUniqueLength:])\n",
        "    uniqueTokensList = uniqueTokensList[len(uniqueTokensList)-previousUniqueLength:]\n",
        "\n",
        "    if (i ==loop_runner-1):\n",
        "        max_lastToken = max(lastTokens_probability)\n",
        "        max_lastTokenIndex = lastTokens_probability.index(max_lastToken)\n",
        "        generated_sentence_GI = uniqueTokensList[max_lastTokenIndex].build_Context()\n",
        "\n",
        "  return {\"probabilityMatrix\": probabilityMatrix, \"initialStateProbability\": initialStateProbability,\"content\": content,\"uniqueTokenLength\": uniqueTokenLength,\n",
        "          \"generated_sentence_GI\": generated_sentence_GI,\"trellis_paths\":trellis_paths} #, flops_counter\n",
        "def runViterbiTransformerPipeline(rootSentence, numTokens = 3, loop_runner=3,**kwargs):\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "    result= generateIntermediates(rootSentence,model,tokenizer,numTokens = numTokens,loop_runner =loop_runner+1,**kwargs)\n",
        "    probabilityMatrix = result['probabilityMatrix']\n",
        "    initialStateProbability = result['initialStateProbability']\n",
        "    content = result['content']\n",
        "    uniqueTokenLength = result['uniqueTokenLength']\n",
        "    generated_sentence_GI = result['generated_sentence_GI']\n",
        "    #entropy_array = result['entropies_array']\n",
        "    trellis_paths = result['trellis_paths']\n",
        "    best_path,viterbi_mat,best_path_prob = VITERBI_Lists(probabilityMatrix, initialStateProbability,device)\n",
        "    print(\"uniqueTokenLength: \", uniqueTokenLength)\n",
        "    print(\"best_path: \", best_path)\n",
        "    decodedString,resultant_final_tokenids = decodePath(best_path,content,rootSentence,tokenizer)\n",
        "    return decodedString,best_path_prob,generated_sentence_GI,resultant_final_tokenids,trellis_paths\n",
        "\n",
        "def runTransformerPipeline(rootSentence,loop_runner = 3):\n",
        "  model = LMHeadModel(\"gpt2\")\n",
        "  prob = 1\n",
        "  finalSentence = rootSentence\n",
        "  for i in range(loop_runner):\n",
        "    tokens_50K = model.get_batch_predictions([finalSentence])\n",
        "\n",
        "    context = tokens_50K[0][0][0]\n",
        "    prob =  prob*tokens_50K[0][0][1]\n",
        "    if context in ['.',':',',','?','!',';'] or \"'\" in context:\n",
        "      finalSentence += context\n",
        "\n",
        "    else:\n",
        "      finalSentence = finalSentence + ' ' + context\n",
        "  return finalSentence,prob\n",
        "\n",
        "def gather_log_probabilities(logits: torch.Tensor, labels: torch.LongTensor) -> torch.Tensor:\n",
        "    \"\"\"Gather log probabilities of the given labels from the logits.\"\"\"\n",
        "    log_probs = torch.nn.functional.log_softmax(logits.float(), dim=-1)\n",
        "    log_probs_labels = log_probs.gather(dim=-1, index=labels.unsqueeze(dim=-1))\n",
        "    return log_probs_labels.squeeze(dim=-1)\n",
        "\n"
      ],
      "metadata": {
        "id": "q3RL_Vqg4Ij4"
      },
      "id": "q3RL_Vqg4Ij4",
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "# model2 = LMHeadModel(model_name)\n",
        "model.eval()  # put model in inference mode\n",
        "\n",
        "# If using GPU (e.g., on Colab), you could also do:\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Function to generate text using different decoders\n",
        "# ---------------------------------------------------------\n",
        "def generate_texts(model, tokenizer, prompt, max_length=40):\n",
        "    \"\"\"Generate text from a prompt using different decoding strategies.\"\"\"\n",
        "    tokenized_result = tokenizer(prompt,return_tensors = \"pt\")\n",
        "    input_ids = tokenized_result[\"input_ids\"].to(model.device)\n",
        "    # If on GPU, uncomment next line:\n",
        "    # input_ids = input_ids.to(device)\n",
        "    attn_mask = tokenized_result['attention_mask'].to(model.device)\n",
        "\n",
        "    st1 = time()\n",
        "    # -- Greedy decoding\n",
        "    greedy_ids = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        max_length=max_length,\n",
        "        do_sample=False,  # Greedy\n",
        "        attention_mask = attn_mask\n",
        "    ).to(model.device)\n",
        "    greedy_text = tokenizer.decode(greedy_ids[0], skip_special_tokens=True)\n",
        "    dur = time() -st1\n",
        "\n",
        "    st2 = time()\n",
        "    # -- Beam search\n",
        "    beam_ids = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        max_length=max_length,\n",
        "        num_beams=3,    # for example\n",
        "        early_stopping=True,\n",
        "        attention_mask = attn_mask\n",
        "    )\n",
        "    beam_text = tokenizer.decode(beam_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    dur2 = time() - st2\n",
        "\n",
        "    # -- Top-k sampling\n",
        "    topk_ids = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        max_length=max_length,\n",
        "        do_sample=True,\n",
        "        top_k=50,  # for example\n",
        "        attention_mask = attn_mask\n",
        "    )\n",
        "    topk_text = tokenizer.decode(topk_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    # -- Nucleus (top-p) sampling\n",
        "    topp_ids = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        max_length=max_length,\n",
        "        do_sample=True,\n",
        "        top_p=0.9,  # for example\n",
        "        attention_mask = attn_mask\n",
        "    )\n",
        "    topp_text = tokenizer.decode(topp_ids[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "\n",
        "    Text_dict = {\n",
        "        \"greedy\": greedy_text,\n",
        "        \"beam\": beam_text,\n",
        "        \"topk\": topk_text,\n",
        "        \"topp\": topp_text\n",
        "    }\n",
        "    Time_dict = {\n",
        "     \"Time_greedy\": dur,\n",
        "     \"Time_beam\": dur2,\n",
        "    }\n",
        "    return Text_dict,Time_dict\n",
        "\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Function to compute perplexity of a string\n",
        "# ---------------------------------------------------------\n",
        "def compute_perplexity(model, tokenizer, text):\n",
        "    \"\"\"Compute perplexity of `text` under `model`.\"\"\"\n",
        "    encodings = tokenizer(text, return_tensors=\"pt\")\n",
        "    input_ids = encodings.input_ids.to(device)\n",
        "    num_sentences = len(input_ids)\n",
        "    if (num_sentences == 1):\n",
        "        if (input_ids[0][-1] == 628):\n",
        "            input_ids = input_ids[:, :-1]\n",
        "            input_ids = torch.cat((input_ids,torch.tensor([[198,198]]).to(model.device)),dim = 1)\n",
        "    with torch.no_grad():\n",
        "        # The model returns a tuple of (loss, logits, ...)\n",
        "        outputs = model(input_ids, labels=input_ids)\n",
        "        # outputs.loss is the average cross-entropy across tokens\n",
        "        neg_log_likelihood = outputs.loss.item()\n",
        "\n",
        "    perplexity = math.exp(neg_log_likelihood)\n",
        "    return perplexity\n",
        "\n"
      ],
      "metadata": {
        "id": "2kLUy9XprU05"
      },
      "id": "2kLUy9XprU05",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b5xubofY2ONH"
      },
      "id": "b5xubofY2ONH",
      "execution_count": null,
      "outputs": []
    }
  ]
}