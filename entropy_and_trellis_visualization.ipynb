{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiveshj/SeniorThesis/blob/main/entropy_and_trellis_visualization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "id": "9d80e0be-53d5-4a01-8180-0093ccab1920",
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from collections import defaultdict\n",
        "import scipy.special\n",
        "\n",
        "class EntropyTrellisAnalyzer:\n",
        "    def __init__(self, model, tokenizer):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.paths = {}\n",
        "        self.scores = {}\n",
        "        self.states = {}\n",
        "        self.entropies = {}\n",
        "        self.logits = {}\n",
        "\n",
        "    def record_path(self, name, tokens, states, scores, logits=None):\n",
        "        \"\"\"\n",
        "        Record a decoding path for later analysis.\n",
        "\n",
        "        Args:\n",
        "            name: Name of the decoding algorithm (e.g., 'viterbi', 'beam', 'greedy')\n",
        "            tokens: List of token IDs or strings representing the output\n",
        "            states: List of states visited at each time step\n",
        "            scores: List of scores (log probabilities) at each time step\n",
        "            logits: List of logits at each time step (optional, for entropy calculation)\n",
        "        \"\"\"\n",
        "        self.paths[name] = tokens\n",
        "        self.states[name] = states\n",
        "        self.scores[name] = scores\n",
        "\n",
        "        if logits is not None:\n",
        "            self.logits[name] = logits\n",
        "            # Calculate entropies from logits\n",
        "            self.entropies[name] = [self.calculate_conditional_entropy(logit) for logit in logits]\n",
        "\n",
        "    def calculate_conditional_entropy(self, logits):\n",
        "        \"\"\"\n",
        "        Calculate conditional entropy from token logits.\n",
        "        Higher values indicate more uncertainty in the next token prediction.\n",
        "\n",
        "        Args:\n",
        "            logits: Raw logits from your model for the next token\n",
        "\n",
        "        Returns:\n",
        "            Conditional entropy value\n",
        "        \"\"\"\n",
        "        # Convert logits to probabilities using softmax\n",
        "        probs = scipy.special.softmax(logits, axis=-1)\n",
        "\n",
        "        # Calculate entropy: -sum(p * log(p))\n",
        "        entropy = -np.sum(probs * np.log(probs + 1e-10))\n",
        "        return entropy\n",
        "\n",
        "    def analyze_state_overlap(self):\n",
        "        \"\"\"\n",
        "        Check if states from greedy and beam search are present in Viterbi trellis.\n",
        "        Returns a dict with overlap percentages.\n",
        "        \"\"\"\n",
        "        results = {}\n",
        "        viterbi_states = set(self.states.get('viterbi', []))\n",
        "\n",
        "        for name, states in self.states.items():\n",
        "            if name == 'viterbi':\n",
        "                continue\n",
        "\n",
        "            overlap = [state for state in states if state in viterbi_states]\n",
        "            overlap_percent = len(overlap) / len(states) * 100 if states else 0\n",
        "            results[name] = {\n",
        "                'overlap_count': len(overlap),\n",
        "                'total_states': len(states),\n",
        "                'overlap_percent': overlap_percent\n",
        "            }\n",
        "\n",
        "        return results\n",
        "\n",
        "    def find_divergence_points(self):\n",
        "        \"\"\"\n",
        "        Find points where paths diverge and analyze the scores at these points.\n",
        "        Also compares entropy at divergence points if available.\n",
        "        \"\"\"\n",
        "        divergences = {}\n",
        "        viterbi_tokens = self.paths.get('viterbi', [])\n",
        "\n",
        "        for name, tokens in self.paths.items():\n",
        "            if name == 'viterbi':\n",
        "                continue\n",
        "\n",
        "            # Find position where paths diverge\n",
        "            diverge_pos = None\n",
        "            for i, (vt, t) in enumerate(zip(viterbi_tokens, tokens)):\n",
        "                if vt != t:\n",
        "                    diverge_pos = i\n",
        "                    break\n",
        "\n",
        "            if diverge_pos is not None:\n",
        "                viterbi_score = self.scores['viterbi'][diverge_pos] if diverge_pos < len(self.scores['viterbi']) else None\n",
        "                other_score = self.scores[name][diverge_pos] if diverge_pos < len(self.scores[name]) else None\n",
        "\n",
        "                # Add entropy information if available\n",
        "                viterbi_entropy = self.entropies.get('viterbi', [None])[diverge_pos] if 'viterbi' in self.entropies and diverge_pos < len(self.entropies['viterbi']) else None\n",
        "                other_entropy = self.entropies.get(name, [None])[diverge_pos] if name in self.entropies and diverge_pos < len(self.entropies[name]) else None\n",
        "\n",
        "                divergences[name] = {\n",
        "                    'position': diverge_pos,\n",
        "                    'viterbi_token': viterbi_tokens[diverge_pos] if diverge_pos < len(viterbi_tokens) else None,\n",
        "                    'alternate_token': tokens[diverge_pos] if diverge_pos < len(tokens) else None,\n",
        "                    'viterbi_score': viterbi_score,\n",
        "                    'alternate_score': other_score,\n",
        "                    'score_difference': viterbi_score - other_score if viterbi_score is not None and other_score is not None else None,\n",
        "                    'viterbi_entropy': viterbi_entropy,\n",
        "                    'alternate_entropy': other_entropy\n",
        "                }\n",
        "\n",
        "        return divergences\n",
        "\n",
        "    def validate_paths(self):\n",
        "        \"\"\"\n",
        "        Validate if the paths from beam and greedy search exist in the trellis.\n",
        "        Calculate the full path probabilities.\n",
        "        \"\"\"\n",
        "        # This depends on your trellis implementation\n",
        "        results = {}\n",
        "        viterbi_total = sum(self.scores.get('viterbi', [0]))\n",
        "\n",
        "        for name, scores in self.scores.items():\n",
        "            if name == 'viterbi':\n",
        "                continue\n",
        "\n",
        "            total_score = sum(scores)\n",
        "            results[name] = {\n",
        "                'total_score': total_score,\n",
        "                'viterbi_score': viterbi_total,\n",
        "                'difference': viterbi_total - total_score,\n",
        "                'is_valid': True  # Assuming all paths are valid; adjust based on your implementation\n",
        "            }\n",
        "\n",
        "        return results\n",
        "\n",
        "    def visualize_paths_with_entropy(self, output_file=None):\n",
        "        \"\"\"\n",
        "        Visualize the different paths through the trellis along with entropy.\n",
        "        \"\"\"\n",
        "        has_entropy = any(len(self.entropies.get(name, [])) > 0 for name in self.paths)\n",
        "\n",
        "        if has_entropy:\n",
        "            fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 12), sharex=True)\n",
        "        else:\n",
        "            fig, ax1 = plt.subplots(figsize=(14, 6))\n",
        "\n",
        "        # Get max length of all paths\n",
        "        max_len = max(len(tokens) for tokens in self.paths.values())\n",
        "\n",
        "        # Plot scores for each path\n",
        "        for name, scores in self.scores.items():\n",
        "            # Pad scores to max_len if needed\n",
        "            padded_scores = scores + [None] * (max_len - len(scores))\n",
        "            valid_scores = [s for s in padded_scores if s is not None]\n",
        "            positions = list(range(len(valid_scores)))\n",
        "            ax1.plot(positions, valid_scores, marker='o', label=name)\n",
        "\n",
        "        ax1.set_ylabel('Log probability score')\n",
        "        ax1.set_title('Decoding Paths Comparison')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True)\n",
        "\n",
        "        # Plot entropy if available\n",
        "        if has_entropy:\n",
        "            for name, entropies in self.entropies.items():\n",
        "                padded_entropies = entropies + [None] * (max_len - len(entropies))\n",
        "                valid_entropies = [e for e in padded_entropies if e is not None]\n",
        "                positions = list(range(len(valid_entropies)))\n",
        "                ax2.plot(positions, valid_entropies, marker='s', linestyle='--', label=f\"{name} entropy\")\n",
        "\n",
        "            ax2.set_xlabel('Position in sequence')\n",
        "            ax2.set_ylabel('Conditional entropy')\n",
        "            ax2.set_title('Conditional Entropy at Each Position')\n",
        "            ax2.legend()\n",
        "            ax2.grid(True)\n",
        "        else:\n",
        "            ax1.set_xlabel('Position in sequence')\n",
        "\n",
        "        # Highlight divergence points\n",
        "        divergences = self.find_divergence_points()\n",
        "        for name, info in divergences.items():\n",
        "            pos = info['position']\n",
        "            if pos is not None:\n",
        "                ax1.axvline(x=pos, color='r', linestyle='--', alpha=0.5)\n",
        "                ax1.text(pos, min(s for s in sum([list(self.scores.values())], []) if s is not None) * 0.95,\n",
        "                         f\"Divergence at {pos}\", rotation=90, color='red')\n",
        "\n",
        "                if has_entropy:\n",
        "                    ax2.axvline(x=pos, color='r', linestyle='--', alpha=0.5)\n",
        "\n",
        "        if output_file:\n",
        "            plt.savefig(output_file)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def analyze_entropy_at_divergence(self):\n",
        "        \"\"\"\n",
        "        Analyze the relationship between conditional entropy and path divergence.\n",
        "        \"\"\"\n",
        "        results = {}\n",
        "        divergences = self.find_divergence_points()\n",
        "\n",
        "        for name, info in divergences.items():\n",
        "            pos = info['position']\n",
        "            if pos is None:\n",
        "                continue\n",
        "\n",
        "            # Check if we have entropy data\n",
        "            if info['viterbi_entropy'] is not None:\n",
        "                # Calculate entropy statistics\n",
        "                avg_entropy = np.mean([e for e in self.entropies.get('viterbi', []) if e is not None])\n",
        "                entropy_at_divergence = info['viterbi_entropy']\n",
        "                relative_entropy = entropy_at_divergence / avg_entropy if avg_entropy > 0 else None\n",
        "\n",
        "                results[name] = {\n",
        "                    'position': pos,\n",
        "                    'entropy_at_divergence': entropy_at_divergence,\n",
        "                    'average_entropy': avg_entropy,\n",
        "                    'relative_entropy': relative_entropy,\n",
        "                    'is_high_entropy': entropy_at_divergence > avg_entropy\n",
        "                }\n",
        "\n",
        "        return results\n",
        "\n",
        "    def correlate_entropy_with_divergence(self):\n",
        "        \"\"\"\n",
        "        Calculate correlation between entropy and path divergence.\n",
        "        \"\"\"\n",
        "        # Get all positions where paths differ\n",
        "        viterbi_tokens = self.paths.get('viterbi', [])\n",
        "        divergence_positions = []\n",
        "\n",
        "        for name, tokens in self.paths.items():\n",
        "            if name == 'viterbi':\n",
        "                continue\n",
        "\n",
        "            for i, (vt, t) in enumerate(zip(viterbi_tokens, tokens)):\n",
        "                if vt != t and i not in divergence_positions:\n",
        "                    divergence_positions.append(i)\n",
        "\n",
        "        # If we have entropy data and divergence points\n",
        "        if 'viterbi' in self.entropies and divergence_positions:\n",
        "            entropies = self.entropies['viterbi']\n",
        "            max_pos = min(len(entropies), len(viterbi_tokens))\n",
        "\n",
        "            # Create binary mask: 1 if position has divergence, 0 otherwise\n",
        "            divergence_mask = np.zeros(max_pos)\n",
        "            for pos in divergence_positions:\n",
        "                if pos < max_pos:\n",
        "                    divergence_mask[pos] = 1\n",
        "\n",
        "            # Extract entropies for positions we have\n",
        "            entropy_values = np.array(entropies[:max_pos])\n",
        "\n",
        "            # Calculate point-biserial correlation (between continuous and binary variables)\n",
        "            # This tells us if higher entropy correlates with divergence points\n",
        "            mean_entropy_diverge = np.mean(entropy_values[divergence_mask == 1])\n",
        "            mean_entropy_no_diverge = np.mean(entropy_values[divergence_mask == 0])\n",
        "            std_entropy = np.std(entropy_values)\n",
        "            n_diverge = np.sum(divergence_mask)\n",
        "            n_no_diverge = len(divergence_mask) - n_diverge\n",
        "            n_total = len(divergence_mask)\n",
        "\n",
        "            # Point-biserial formula\n",
        "            if std_entropy > 0 and n_diverge > 0 and n_no_diverge > 0:\n",
        "                correlation = ((mean_entropy_diverge - mean_entropy_no_diverge) / std_entropy) * \\\n",
        "                               np.sqrt((n_diverge * n_no_diverge) / (n_total * n_total))\n",
        "\n",
        "                return {\n",
        "                    'correlation': correlation,\n",
        "                    'mean_entropy_at_divergence': mean_entropy_diverge,\n",
        "                    'mean_entropy_elsewhere': mean_entropy_no_diverge,\n",
        "                    'entropy_ratio': mean_entropy_diverge / mean_entropy_no_diverge if mean_entropy_no_diverge > 0 else None\n",
        "                }\n",
        "\n",
        "        return None\n",
        "\n",
        "    def visualize_trellis(self, timesteps=10, k_best=5, output_file=None):\n",
        "        \"\"\"\n",
        "        Visualize the trellis structure with the different paths.\n",
        "\n",
        "        Args:\n",
        "            timesteps: Number of time steps to visualize\n",
        "            k_best: Number of best states to show at each time step\n",
        "            output_file: If provided, save the figure to this file\n",
        "        \"\"\"\n",
        "        # Create a combined set of all states visited\n",
        "        all_states = defaultdict(lambda: defaultdict(float))\n",
        "\n",
        "        # For simplicity, we assume states dict contains token IDs at each position\n",
        "        for name, states_path in self.states.items():\n",
        "            for t, state in enumerate(states_path):\n",
        "                if t >= timesteps:\n",
        "                    break\n",
        "                all_states[t][state] = max(all_states[t].get(state, 0),\n",
        "                                         self.scores[name][t] if t < len(self.scores[name]) else 0)\n",
        "\n",
        "        # Create a matrix for visualization\n",
        "        matrix = np.zeros((k_best, timesteps))\n",
        "        state_labels = [[] for _ in range(timesteps)]\n",
        "\n",
        "        for t in range(timesteps):\n",
        "            if t not in all_states:\n",
        "                continue\n",
        "\n",
        "            # Get top k states by score\n",
        "            top_states = sorted(all_states[t].items(), key=lambda x: x[1], reverse=True)[:k_best]\n",
        "\n",
        "            for i, (state, score) in enumerate(top_states):\n",
        "                matrix[i, t] = score\n",
        "                # Try to convert state to readable token if it's an integer\n",
        "                if isinstance(state, int):\n",
        "                    try:\n",
        "                        token = self.tokenizer.decode([state])\n",
        "                    except:\n",
        "                        token = str(state)\n",
        "                else:\n",
        "                    token = str(state)\n",
        "                state_labels[t].append(token)\n",
        "\n",
        "        # Create a heatmap\n",
        "        plt.figure(figsize=(16, 10))\n",
        "        ax = sns.heatmap(matrix, cmap=\"YlGnBu\", annot=False)\n",
        "\n",
        "        # Add path markers\n",
        "        for name, states_path in self.states.items():\n",
        "            path_y = []\n",
        "            path_x = []\n",
        "\n",
        "            for t, state in enumerate(states_path):\n",
        "                if t >= timesteps:\n",
        "                    break\n",
        "\n",
        "                # Find this state in our top-k states\n",
        "                try:\n",
        "                    top_states = sorted(all_states[t].items(), key=lambda x: x[1], reverse=True)[:k_best]\n",
        "                    state_idx = [s[0] for s in top_states].index(state)\n",
        "                    path_y.append(state_idx + 0.5)  # +0.5 for center of cell\n",
        "                    path_x.append(t + 0.5)\n",
        "                except ValueError:\n",
        "                    # State not in top-k, skip this point\n",
        "                    continue\n",
        "\n",
        "            # Plot the path\n",
        "            plt.plot(path_x, path_y, marker='o', linewidth=2,\n",
        "                     label=name, alpha=0.7)\n",
        "\n",
        "        # Add labels and other details\n",
        "        plt.title(\"Trellis Paths Visualization\")\n",
        "        plt.ylabel(\"Top-k states at each time step\")\n",
        "        plt.xlabel(\"Time step\")\n",
        "        plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), ncol=3)\n",
        "\n",
        "        # Set custom y-tick labels (state labels)\n",
        "        y_ticks = np.arange(k_best) + 0.5\n",
        "        plt.yticks(y_ticks, [''] * k_best)\n",
        "\n",
        "        # Add state labels as text annotations\n",
        "        for t in range(timesteps):\n",
        "            for i, label in enumerate(state_labels[t]):\n",
        "                if i < k_best:\n",
        "                    plt.text(t + 0.5, i + 0.5, label,\n",
        "                            ha='center', va='center', fontsize=8)\n",
        "\n",
        "        if output_file:\n",
        "            plt.savefig(output_file)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# Example usage function - now with entropy analysis\n",
        "def compare_decoding_paths_with_entropy(input_text, model, tokenizer, viterbi_fn, greedy_fn, beam_fn):\n",
        "    \"\"\"\n",
        "    Compare different decoding paths for the same input, with entropy analysis.\n",
        "\n",
        "    Args:\n",
        "        input_text: The input text to decode\n",
        "        model: Your language model\n",
        "        tokenizer: Your tokenizer\n",
        "        viterbi_fn: Function that returns (tokens, states, scores, logits) for Viterbi decoding\n",
        "        greedy_fn: Function that returns (tokens, states, scores, logits) for greedy decoding\n",
        "        beam_fn: Function that returns (tokens, states, scores, logits) for beam search\n",
        "    \"\"\"\n",
        "    analyzer = EntropyTrellisAnalyzer(model, tokenizer)\n",
        "\n",
        "    # Run each decoding method and record results (now with logits for entropy calculation)\n",
        "    viterbi_tokens, viterbi_states, viterbi_scores, viterbi_logits = viterbi_fn(input_text)\n",
        "    analyzer.record_path('viterbi', viterbi_tokens, viterbi_states, viterbi_scores, viterbi_logits)\n",
        "\n",
        "    greedy_tokens, greedy_states, greedy_scores, greedy_logits = greedy_fn(input_text)\n",
        "    analyzer.record_path('greedy', greedy_tokens, greedy_states, greedy_scores, greedy_logits)\n",
        "\n",
        "    beam_tokens, beam_states, beam_scores, beam_logits = beam_fn(input_text)\n",
        "    analyzer.record_path('beam', beam_tokens, beam_states, beam_scores, beam_logits)\n",
        "\n",
        "    # Run basic analysis\n",
        "    print(\"=== State Overlap Analysis ===\")\n",
        "    overlap = analyzer.analyze_state_overlap()\n",
        "    for name, results in overlap.items():\n",
        "        print(f\"{name.capitalize()} search: {results['overlap_percent']:.2f}% of states appear in Viterbi trellis\")\n",
        "\n",
        "    print(\"\\n=== Divergence Points Analysis ===\")\n",
        "    divergences = analyzer.find_divergence_points()\n",
        "    for name, info in divergences.items():\n",
        "        if info['position'] is not None:\n",
        "            print(f\"{name.capitalize()} diverges from Viterbi at position {info['position']}:\")\n",
        "            print(f\"  Viterbi chose '{info['viterbi_token']}' (score: {info['viterbi_score']:.4f})\")\n",
        "            print(f\"  {name.capitalize()} chose '{info['alternate_token']}' (score: {info['alternate_score']:.4f})\")\n",
        "            print(f\"  Score difference: {info['score_difference']:.4f}\")\n",
        "\n",
        "            # Print entropy information if available\n",
        "            if info['viterbi_entropy'] is not None:\n",
        "                print(f\"  Entropy at divergence point: {info['viterbi_entropy']:.4f}\")\n",
        "\n",
        "    print(\"\\n=== Path Validation ===\")\n",
        "    validations = analyzer.validate_paths()\n",
        "    for name, results in validations.items():\n",
        "        print(f\"{name.capitalize()} path total score: {results['total_score']:.4f}\")\n",
        "        print(f\"Viterbi path total score: {results['viterbi_score']:.4f}\")\n",
        "        print(f\"Difference: {results['difference']:.4f}\")\n",
        "        print(f\"Path exists in trellis: {'Yes' if results['is_valid'] else 'No'}\")\n",
        "\n",
        "    # Run entropy-specific analysis\n",
        "    print(\"\\n=== Entropy Analysis at Divergence Points ===\")\n",
        "    entropy_analysis = analyzer.analyze_entropy_at_divergence()\n",
        "    if entropy_analysis:\n",
        "        for name, results in entropy_analysis.items():\n",
        "            print(f\"{name.capitalize()} divergence entropy analysis:\")\n",
        "            print(f\"  Entropy at divergence point: {results['entropy_at_divergence']:.4f}\")\n",
        "            print(f\"  Average entropy across sequence: {results['average_entropy']:.4f}\")\n",
        "            print(f\"  Relative entropy (divergence/average): {results['relative_entropy']:.4f}\")\n",
        "            print(f\"  Is high entropy point: {'Yes' if results['is_high_entropy'] else 'No'}\")\n",
        "    else:\n",
        "        print(\"No entropy data available for analysis.\")\n",
        "\n",
        "    # Calculate correlation between entropy and divergence\n",
        "    print(\"\\n=== Entropy-Divergence Correlation ===\")\n",
        "    correlation = analyzer.correlate_entropy_with_divergence()\n",
        "    if correlation:\n",
        "        print(f\"Correlation between entropy and path divergence: {correlation['correlation']:.4f}\")\n",
        "        print(f\"Mean entropy at divergence points: {correlation['mean_entropy_at_divergence']:.4f}\")\n",
        "        print(f\"Mean entropy elsewhere: {correlation['mean_entropy_elsewhere']:.4f}\")\n",
        "        print(f\"Ratio of entropy at divergence vs. elsewhere: {correlation['entropy_ratio']:.4f}\")\n",
        "\n",
        "        if correlation['correlation'] > 0.3:\n",
        "            print(\"CONCLUSION: Strong positive correlation suggests higher entropy regions are associated with path divergence.\")\n",
        "        elif correlation['correlation'] < -0.3:\n",
        "            print(\"CONCLUSION: Strong negative correlation suggests lower entropy regions are associated with path divergence.\")\n",
        "        else:\n",
        "            print(\"CONCLUSION: No strong correlation between entropy and path divergence.\")\n",
        "    else:\n",
        "        print(\"Insufficient data to calculate entropy-divergence correlation.\")\n",
        "\n",
        "    # Visualize the results\n",
        "    print(\"\\nGenerating visualizations...\")\n",
        "    analyzer.visualize_paths_with_entropy()\n",
        "    analyzer.visualize_trellis()\n",
        "\n",
        "    return analyzer\n",
        "\n",
        "# Helper function to adapt code to your specific trellis implementation\n",
        "def extract_trellis_data_with_logits(trellis, path_indices, token_logits=None):\n",
        "    \"\"\"\n",
        "    Extract tokens, states, scores, and logits from a trellis for a specific path.\n",
        "\n",
        "    Args:\n",
        "        trellis: Your trellis data structure\n",
        "        path_indices: The indices of the states in the chosen path\n",
        "        token_logits: Optional list of logits for each position (for entropy calculation)\n",
        "\n",
        "    Returns:\n",
        "        (tokens, states, scores, logits) tuples\n",
        "    \"\"\"\n",
        "    tokens = []\n",
        "    states = []\n",
        "    scores = []\n",
        "    logits = []\n",
        "\n",
        "    # Example implementation - adapt to your trellis structure\n",
        "    for t, idx in enumerate(path_indices):\n",
        "        if t < len(trellis) and idx < len(trellis[t]):\n",
        "            token = trellis[t][idx].token  # Assume trellis cells have .token attribute\n",
        "            state = trellis[t][idx].state  # Assume trellis cells have .state attribute\n",
        "            score = trellis[t][idx].score  # Assume trellis cells have .score attribute\n",
        "\n",
        "            tokens.append(token)\n",
        "            states.append(state)\n",
        "            scores.append(score)\n",
        "\n",
        "            # Add logits if available\n",
        "            if token_logits is not None and t < len(token_logits):\n",
        "                logits.append(token_logits[t])\n",
        "\n",
        "    return tokens, states, scores, logits"
      ],
      "metadata": {
        "trusted": true,
        "id": "9d80e0be-53d5-4a01-8180-0093ccab1920"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "beab9ff8-8f68-4b3b-83d7-d7ac37808fff",
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from collections import defaultdict\n",
        "import scipy.special\n",
        "\n",
        "class IterativeTrellisAnalyzer:\n",
        "    def __init__(self, model, tokenizer):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.iteration_paths = {}  # Will store paths for each iteration length\n",
        "        self.entropies = {}\n",
        "        self.trellis_paths = []\n",
        "        self.scores = {}\n",
        "\n",
        "    def add_iteration_result(self, iteration_length, viterbi_path, beam_path, greedy_path,all_trellis_path_ids):\n",
        "        \"\"\"\n",
        "        Record decoding paths for a specific iteration length.\n",
        "\n",
        "        Args:\n",
        "            iteration_length: Number of tokens generated\n",
        "            viterbi_path: List of state indices for the Viterbi path\n",
        "            beam_path: List of state indices for the beam search path\n",
        "            greedy_path: List of state indices for the greedy search path\n",
        "        \"\"\"\n",
        "        self.iteration_paths[iteration_length] = {\n",
        "            'viterbi': viterbi_path,\n",
        "            'beam': beam_path,\n",
        "            'greedy': greedy_path\n",
        "        }\n",
        "        self.trellis_paths = all_trellis_path_ids  #added this\n",
        "\n",
        "    def find_path_shift_points(self):\n",
        "        \"\"\"\n",
        "        Find points where paths shift between iterations.\n",
        "        For example, when going from 3 tokens to 4 tokens, does the path for the first 3 tokens change?\n",
        "        \"\"\"\n",
        "        iterations = sorted(self.iteration_paths.keys())\n",
        "        print(\"iterations in find_path_shift points: \", iterations)\n",
        "        shifts = {}\n",
        "\n",
        "        for i in range(len(iterations) - 1):\n",
        "            curr_iter = iterations[i]\n",
        "            next_iter = iterations[i+1]\n",
        "\n",
        "            # For each decoding method\n",
        "            for method in ['viterbi', 'beam', 'greedy']:\n",
        "                curr_path = self.iteration_paths[curr_iter].get(method, [])\n",
        "                next_path = self.iteration_paths[next_iter].get(method, [])\n",
        "\n",
        "                # Compare paths up to the length of the shorter path\n",
        "                min_len = min(len(curr_path), len(next_path))\n",
        "                if min_len == 0:\n",
        "                    continue\n",
        "\n",
        "                # Check if paths diverge\n",
        "                diverges = False\n",
        "                diverge_pos = None\n",
        "\n",
        "                for pos in range(min_len):\n",
        "                    if curr_path[pos] != next_path[pos]:\n",
        "                        diverges = True\n",
        "                        diverge_pos = pos\n",
        "                        break\n",
        "\n",
        "                key = f\"{method}_{curr_iter}_to_{next_iter}\"\n",
        "                shifts[key] = {\n",
        "                    'method': method,\n",
        "                    'from_iteration': curr_iter,\n",
        "                    'to_iteration': next_iter,\n",
        "                    'diverges': diverges,\n",
        "                    'diverge_position': diverge_pos,\n",
        "                    'path_before': curr_path[:min_len],\n",
        "                    'path_after': next_path[:min_len]\n",
        "                }\n",
        "\n",
        "        return shifts\n",
        "\n",
        "    def compare_paths_at_iteration(self, iteration):\n",
        "        \"\"\"\n",
        "        Compare paths between different decoding methods at a specific iteration length.\n",
        "        \"\"\"\n",
        "        if iteration not in self.iteration_paths:\n",
        "            return None\n",
        "\n",
        "        paths = self.iteration_paths[iteration]\n",
        "        comparisons = {}\n",
        "\n",
        "        # Compare Viterbi with other methods\n",
        "        viterbi_path = paths.get('viterbi', [])\n",
        "        for method in ['beam', 'greedy']:\n",
        "            other_path = paths.get(method, [])\n",
        "            min_len = min(len(viterbi_path), len(other_path))\n",
        "\n",
        "            if min_len == 0:\n",
        "                continue\n",
        "\n",
        "            # Check for path divergence\n",
        "            diverges = False\n",
        "            diverge_pos = None\n",
        "\n",
        "            for pos in range(min_len):\n",
        "                if viterbi_path[pos] != other_path[pos]:\n",
        "                    diverges = True\n",
        "                    diverge_pos = pos\n",
        "                    break\n",
        "\n",
        "            comparisons[f\"viterbi_vs_{method}\"] = {\n",
        "                'iteration': iteration,\n",
        "                'diverges': diverges,\n",
        "                'diverge_position': diverge_pos,\n",
        "                'viterbi_path': viterbi_path[:min_len],\n",
        "                'other_path': other_path[:min_len],\n",
        "                'other_method': method\n",
        "            }\n",
        "\n",
        "        return comparisons\n",
        "\n",
        "    def decoder_in_trellis(self,iteration,beam_path,greedy_path,trellis_paths):\n",
        "        return beam_path in trellis_paths or greedy_path in trellis_paths\n",
        "\n",
        "    def analyze_all_iterations(self):\n",
        "        \"\"\"\n",
        "        Analyze path divergence across all recorded iterations.\n",
        "        \"\"\"\n",
        "        results = {\n",
        "            'path_shifts': self.find_path_shift_points(),\n",
        "            'method_comparisons': {}\n",
        "        }\n",
        "\n",
        "        for iteration in sorted(self.iteration_paths.keys()):\n",
        "            comparison = self.compare_paths_at_iteration(iteration)\n",
        "            if comparison:\n",
        "                results['method_comparisons'][iteration] = comparison\n",
        "\n",
        "        return results\n",
        "\n",
        "    def visualize_path_stability(self, max_states=10, output_file=None):\n",
        "        \"\"\"\n",
        "        Visualize how paths change across iterations.\n",
        "\n",
        "        Args:\n",
        "            max_states: Maximum number of states to show in the visualization\n",
        "            output_file: If provided, save the figure to this file\n",
        "        \"\"\"\n",
        "        iterations = sorted(self.iteration_paths.keys())\n",
        "        if not iterations:\n",
        "            print(\"No iteration data to visualize.\")\n",
        "            return\n",
        "\n",
        "        methods = ['viterbi', 'beam', 'greedy']\n",
        "        colors = {'viterbi': 'blue', 'beam': 'green', 'greedy': 'red'}\n",
        "\n",
        "        # Create a figure with subplots for each method\n",
        "        fig, axes = plt.subplots(len(methods), 1, figsize=(14, 4 * len(methods)), sharex=True)\n",
        "\n",
        "        for i, method in enumerate(methods):\n",
        "            ax = axes[i] if len(methods) > 1 else axes\n",
        "\n",
        "            # Create a matrix of paths for this method across iterations\n",
        "            # Each row is an iteration, each column is a position in the path\n",
        "            max_iteration_len = max(len(self.iteration_paths[it].get(method, []))\n",
        "                                   for it in iterations)\n",
        "\n",
        "            path_matrix = np.ones((len(iterations), max_iteration_len)) * np.nan\n",
        "\n",
        "            # Fill the matrix with path data\n",
        "            for j, iteration in enumerate(iterations):\n",
        "                path = self.iteration_paths[iteration].get(method, [])\n",
        "                for k, state in enumerate(path):\n",
        "                    if state < max_states:  # Only include states below max_states\n",
        "                        path_matrix[j, k] = state\n",
        "\n",
        "            # Create a heatmap\n",
        "            im = ax.imshow(path_matrix, aspect='auto', cmap='viridis',\n",
        "                          interpolation='nearest', vmin=0, vmax=max_states-1)\n",
        "\n",
        "            # Add colorbar\n",
        "            cbar = fig.colorbar(im, ax=ax, orientation='vertical')\n",
        "            cbar.set_label('State index')\n",
        "\n",
        "            # Add labels and title\n",
        "            ax.set_ylabel('Iteration length')\n",
        "            ax.set_title(f'{method.capitalize()} paths across iterations')\n",
        "\n",
        "            # Set y-tick labels as iteration lengths\n",
        "            ax.set_yticks(range(len(iterations)))\n",
        "            ax.set_yticklabels(iterations)\n",
        "\n",
        "            # Highlight path shifts\n",
        "            shifts = self.find_path_shift_points()\n",
        "            for key, info in shifts.items():\n",
        "                if info['method'] == method and info['diverges']:\n",
        "                    from_idx = iterations.index(info['from_iteration'])\n",
        "                    to_idx = iterations.index(info['to_iteration'])\n",
        "                    pos = info['diverge_position']\n",
        "                    if pos is not None and pos < max_iteration_len:\n",
        "                        ax.add_patch(plt.Rectangle((pos-0.5, to_idx-0.5), 1, 1,\n",
        "                                                 fill=False, edgecolor='red', linewidth=2))\n",
        "\n",
        "        # Set common x-label\n",
        "        fig.text(0.5, 0.04, 'Position in sequence', ha='center')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        if output_file:\n",
        "            plt.savefig(output_file)\n",
        "        plt.show()\n",
        "\n",
        "    def visualize_method_comparison(self, iteration, output_file=None):\n",
        "        \"\"\"\n",
        "        Visualize path comparison between methods at a specific iteration length.\n",
        "\n",
        "        Args:\n",
        "            iteration: Iteration length to visualize\n",
        "            output_file: If provided, save the figure to this file\n",
        "        \"\"\"\n",
        "        if iteration not in self.iteration_paths:\n",
        "            print(f\"No data for iteration length {iteration}\")\n",
        "            return\n",
        "\n",
        "        paths = self.iteration_paths[iteration]\n",
        "        methods = [m for m in ['viterbi', 'beam', 'greedy'] if m in paths]\n",
        "\n",
        "        if not methods:\n",
        "            print(f\"No method data for iteration length {iteration}\")\n",
        "            return\n",
        "\n",
        "        # Get the max path length\n",
        "        max_len = max(len(paths[m]) for m in methods)\n",
        "\n",
        "        # Create a figure\n",
        "        plt.figure(figsize=(14, 6))\n",
        "\n",
        "        # Plot each path\n",
        "        for method in methods:\n",
        "            path = paths[method]\n",
        "            plt.plot(range(len(path)), path, 'o-', label=method)\n",
        "\n",
        "        # Add labels and title\n",
        "        plt.xlabel('Position in sequence')\n",
        "        plt.ylabel('State index')\n",
        "        plt.title(f'Path comparison at iteration length {iteration}')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        # Add annotations for divergence points\n",
        "        comparisons = self.compare_paths_at_iteration(iteration)\n",
        "        if comparisons:\n",
        "            for key, info in comparisons.items():\n",
        "                if info['diverges']:\n",
        "                    pos = info['diverge_position']\n",
        "                    if pos is not None:\n",
        "                        plt.axvline(x=pos, color='r', linestyle='--', alpha=0.5)\n",
        "                        plt.text(pos, plt.ylim()[0] * 0.9, f\"Divergence at {pos}\",\n",
        "                               rotation=90, color='red')\n",
        "\n",
        "        if output_file:\n",
        "            plt.savefig(output_file)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def visualize_trellis_with_multiple_paths(self, iteration, top_k=10, output_file=None):\n",
        "        \"\"\"\n",
        "        Visualize the trellis structure with paths from different methods and iterations.\n",
        "\n",
        "        Args:\n",
        "            iteration: Iteration length to visualize\n",
        "            top_k: Number of top states to show at each position\n",
        "            output_file: If provided, save the figure to this file\n",
        "        \"\"\"\n",
        "        if iteration not in self.iteration_paths:\n",
        "            print(f\"No data for iteration length {iteration}\")\n",
        "            return\n",
        "\n",
        "        paths = self.iteration_paths[iteration]\n",
        "        methods = [m for m in ['viterbi', 'beam', 'greedy'] if m in paths]\n",
        "\n",
        "        if not methods:\n",
        "            print(f\"No method data for iteration length {iteration}\")\n",
        "            return\n",
        "\n",
        "        # Get the max path length\n",
        "        max_len = max(len(paths[m]) for m in methods)\n",
        "\n",
        "        # Create a simple synthetic trellis for visualization\n",
        "        # In a real implementation, you would use your actual trellis data\n",
        "        synthetic_trellis = np.zeros((top_k, max_len))\n",
        "\n",
        "        # Fill in the paths we know about\n",
        "        for method in methods:\n",
        "            path = paths[method]\n",
        "            for t, state in enumerate(path):\n",
        "                if state < top_k:\n",
        "                    synthetic_trellis[state, t] = 1  # Mark this state as visited\n",
        "\n",
        "        # Create the visualization\n",
        "        plt.figure(figsize=(16, 10))\n",
        "\n",
        "        # Plot the trellis as a grid\n",
        "        plt.imshow(synthetic_trellis, cmap='Blues', alpha=0.3, aspect='auto')\n",
        "\n",
        "        # Plot each path\n",
        "        colors = {'viterbi': 'blue', 'beam': 'green', 'greedy': 'red'}\n",
        "        for method in methods:\n",
        "            path = paths[method]\n",
        "            y_coords = [state for state in path if state < top_k]\n",
        "            x_coords = list(range(len(y_coords)))\n",
        "            plt.plot(x_coords, y_coords, 'o-', label=method, color=colors.get(method, 'black'))\n",
        "\n",
        "        # Add labels and title\n",
        "        plt.xlabel('Position in sequence')\n",
        "        plt.ylabel('State index')\n",
        "        plt.title(f'Trellis paths at iteration length {iteration}')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        # Add y-axis ticks\n",
        "        plt.yticks(range(top_k))\n",
        "\n",
        "        if output_file:\n",
        "            plt.savefig(output_file)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "def analyze_paths_across_iterations(model, tokenizer, decode_fn, input_text, max_length=10):\n",
        "    \"\"\"\n",
        "    Analyze how paths change across different iteration lengths.\n",
        "\n",
        "    Args:\n",
        "        model: Your language model\n",
        "        tokenizer: Your tokenizer\n",
        "        decode_fn: Function that takes (input_text, length) and returns paths for each method\n",
        "        input_text: The input text to decode\n",
        "        max_length: Maximum number of tokens to generate\n",
        "    \"\"\"\n",
        "    analyzer = IterativeTrellisAnalyzer(model, tokenizer)\n",
        "\n",
        "    # Run decoding for each length\n",
        "    for length in range(1, max_length + 1):\n",
        "        print(f\"Decoding iteration {length}...\")\n",
        "        viterbi_path, beam_path, greedy_path = decode_fn(input_text, length)\n",
        "        analyzer.add_iteration_result(length, viterbi_path, beam_path, greedy_path)\n",
        "\n",
        "    # Run analysis\n",
        "    print(\"\\n=== Path Stability Analysis ===\")\n",
        "    analysis = analyzer.analyze_all_iterations()\n",
        "\n",
        "    # Report path shifts\n",
        "    print(\"\\nPath shifts between iterations:\")\n",
        "    for key, info in analysis['path_shifts'].items():\n",
        "        if info['diverges']:\n",
        "            print(f\"{info['method'].capitalize()} path changes when going from {info['from_iteration']} to {info['to_iteration']} tokens\")\n",
        "            print(f\"  Divergence at position: {info['diverge_position']}\")\n",
        "            print(f\"  Path before: {info['path_before']}\")\n",
        "            print(f\"  Path after:  {info['path_after']}\")\n",
        "\n",
        "    # Report method differences\n",
        "    print(\"\\nMethod comparisons at each iteration:\")\n",
        "    for iteration, comparisons in analysis['method_comparisons'].items():\n",
        "        print(f\"\\nIteration length {iteration}:\")\n",
        "        for key, info in comparisons.items():\n",
        "            if info['diverges']:\n",
        "                print(f\"  {info['other_method'].capitalize()} diverges from Viterbi at position {info['diverge_position']}\")\n",
        "                print(f\"    Viterbi path: {info['viterbi_path']}\")\n",
        "                print(f\"    {info['other_method'].capitalize()} path: {info['other_path']}\")\n",
        "\n",
        "    # Visualize results\n",
        "    print(\"\\nGenerating visualizations...\")\n",
        "    analyzer.visualize_path_stability()\n",
        "\n",
        "    # Visualize a few specific iterations\n",
        "    mid_point = max_length // 2\n",
        "    analyzer.visualize_method_comparison(mid_point)\n",
        "    analyzer.visualize_trellis_with_multiple_paths(mid_point)\n",
        "\n",
        "    if max_length > 1:\n",
        "        analyzer.visualize_method_comparison(max_length)\n",
        "        analyzer.visualize_trellis_with_multiple_paths(max_length)\n",
        "\n",
        "    return analyzer\n",
        "\n",
        "# Example wrapper function to adapt to your implementation\n",
        "def run_decoding_for_length(input_text, length):\n",
        "    \"\"\"\n",
        "    Run all decoding methods for a specific length.\n",
        "    Adapt this to your implementation.\n",
        "\n",
        "    Args:\n",
        "        input_text: Input text to decode\n",
        "        length: Number of tokens to generate\n",
        "\n",
        "    Returns:\n",
        "        (viterbi_path, beam_path, greedy_path) - each is a list of state indices\n",
        "    \"\"\"\n",
        "    # This is where you'd call your actual decoders\n",
        "    # Example placeholder implementation:\n",
        "    viterbi_path = []  # Replace with your actual Viterbi path\n",
        "    beam_path = []     # Replace with your actual beam search path\n",
        "    greedy_path = []   # Replace with your actual greedy search path\n",
        "\n",
        "    return viterbi_path, beam_path, greedy_path"
      ],
      "metadata": {
        "trusted": true,
        "id": "beab9ff8-8f68-4b3b-83d7-d7ac37808fff"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "fbc273fb-5539-44db-a022-aa3763c55134",
      "cell_type": "code",
      "source": [
        "import math\n",
        "class SearchTree:\n",
        "    def __init__(self,context,probability,token_id,model,tokenizer,parent = None,child = None,parent_index = None):\n",
        "        self.token_id = token_id\n",
        "        context = context.strip()\n",
        "        self.context = context\n",
        "        self.probability = probability\n",
        "        self.parent = parent\n",
        "        self.child = []\n",
        "        self.parent_index = parent_index  # newly created.\n",
        "        self.cached_tokenids = [self.token_id]\n",
        "        if child is not None:\n",
        "           self.child.append(child)\n",
        "\n",
        "        # Cache cumulative probability at node creation\n",
        "        if parent:\n",
        "            self.cached_prob = parent.calcProbTillNow()+probability #parent.calcProbTillNow() * probability\n",
        "            self.cached_tokenids.insert(0,parent.token_id)\n",
        "        else:\n",
        "            self.cached_prob = probability\n",
        "\n",
        "    def build_Context(self):\n",
        "        context_list = []\n",
        "        full_context = []\n",
        "        node = self\n",
        "        while node.parent is not None:\n",
        "            context_list.extend([node.token_id])\n",
        "            node = node.parent\n",
        "        context_list.reverse()\n",
        "        full_context.extend(node.token_id)\n",
        "        full_context.extend(context_list)\n",
        "        full_context = torch.tensor([full_context])\n",
        "        generated_sentence = tokenizer.decode(full_context[0], skip_special_tokens=True)\n",
        "        return generated_sentence\n",
        "\n",
        "\n",
        "    def create_child(self):\n",
        "        if self.parent is not None:\n",
        "           self.parent.child.append(self)\n",
        "\n",
        "    def replace_parent(self, new_parent):\n",
        "        \"\"\"Assign a new parent and update cached probability.\"\"\"\n",
        "        self.parent = new_parent\n",
        "        self.cached_prob = new_parent.calcProbTillNow() + self.probability\n",
        "\n",
        "\n",
        "    def calcProbTillNow(self):\n",
        "        \"\"\"Return cached cumulative probability to avoid redundant calculations.\"\"\"\n",
        "        return self.cached_prob\n",
        "\n",
        "    def token_idsTillNow(self):\n",
        "        \"Return the token ids of all the tokens up till now. This method is to be used in the last step of the trellis to get all the paths in there.\"\n",
        "        return self.cached_tokenids\n",
        "    def change_probability(self,new_probability,new_cached_prob):\n",
        "        self.cached_prob = new_cached_prob\n",
        "        self.probability = new_probability\n",
        "\n",
        "    # def calcProbTillNow(self):\n",
        "    #   prob = self.probability\n",
        "    #   node = self\n",
        "    #   while node.parent is not None:\n",
        "    #     prob = prob*node.parent.probability\n",
        "    #     node = node.parent\n",
        "    #   return prob    #can make this negative log probability.\n",
        "\n",
        "    def assign_parent_index(self,parent_index):\n",
        "      self.parent_index = parent_index\n",
        "\n",
        "\n",
        "\n",
        "def generate_token_and_probability(model, tokenizer, batch_prompts,max_length=1, top_k=4):\n",
        "    tokenizer.pad_token= tokenizer.eos_token\n",
        "    tokenized_result = tokenizer(batch_prompts, return_tensors=\"pt\",padding = True,truncation = True)\n",
        "    input_ids = tokenized_result[\"input_ids\"].to(model.device)\n",
        "    attn_mask = tokenized_result[\"attention_mask\"].to(model.device)\n",
        "    #Added new stuff for handling newlines below\n",
        "    for sentence_id in range(len(input_ids)):\n",
        "        if (input_ids[sentence_id][-1] == 50256 and input_ids[sentence_id][-2] == 628):\n",
        "            input_ids[sentence_id][-2] = 198\n",
        "            input_ids[sentence_id][-1] = 198\n",
        "            attn_mask[sentence_id][-1] = 1\n",
        "            attn_mask[sentence_id][-2] = 1\n",
        "\n",
        "    num_sentences = len(input_ids)\n",
        "    if (num_sentences == 1):\n",
        "        if (input_ids[0][-1] == 628):\n",
        "            input_ids = input_ids[:, :-1]\n",
        "            attn_mask = attn_mask[:, :-1]\n",
        "            input_ids = torch.cat((input_ids,torch.tensor([[198,198]]).to(model.device)),dim = 1)\n",
        "            attn_mask = torch.cat((attn_mask,torch.tensor([[1,1]]).to(model.device)),dim = 1)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      outputs = model.generate(\n",
        "         input_ids=input_ids,\n",
        "         attention_mask=attn_mask,\n",
        "         max_length=input_ids.size(-1) + max_length,\n",
        "         do_sample=False,  # Greedy decoding\n",
        "         output_scores=True,\n",
        "         return_dict_in_generate=True\n",
        "        )\n",
        "\n",
        "    sequences, scores = outputs.sequences, outputs.scores  # scores will have only one element per batch\n",
        "    predictions = []\n",
        "    # print(\"generated_token_id\",generated_token_id)\n",
        "    for i in range(len(batch_prompts)):\n",
        "        generated_token_id = sequences[i][input_ids.size(-1):].tolist()[0]  # Extract generated token ID\n",
        "        generated_token = tokenizer.decode(generated_token_id, skip_special_tokens=True)\n",
        "\n",
        "\n",
        "        # Log probabilities of all possible tokens at the generated step\n",
        "        log_probs = torch.nn.functional.log_softmax(scores[0][i], dim=-1)  # scores[0] corresponds to the single generation step\n",
        "        # Keep increasing top_k until we have enough valid tokens\n",
        "        valid_predictions = []\n",
        "        curr_top_k = top_k\n",
        "        topk_logprobs, topk_ids = log_probs.topk(curr_top_k)  # Get top-k log probabilities\n",
        "        topk_tokens = tokenizer.batch_decode(topk_ids, skip_special_tokens=True)\n",
        "        predictions.append([(generated_token,tid, tok, lp.item()) for tid, tok, lp in zip(topk_ids, topk_tokens, topk_logprobs)])\n",
        "\n",
        "\n",
        "    return predictions\n",
        "\n",
        "\n",
        "def check_bad_predictions(text):\n",
        "    bad_patterns = [r'={2,}', r'!{2,}', r'\\?{2,}', r',{2,}', r';{2,}', r'\\|{2,}', r'~{2,}', r'&{2,}', r'-{2,}']\n",
        "\n",
        "    # Check for unwanted punctuation patterns (two or more consecutive occurrences)\n",
        "    for pattern in bad_patterns:\n",
        "        if re.search(pattern, text):\n",
        "            return True\n",
        "\n",
        "    # Check for non-ASCII characters\n",
        "    if any(ord(char) > 127 for char in text):  # ASCII characters are in the range 0-127\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "def generateIntermediates(root,model,tokenizer,numTokens = 3, loop_runner = 4,**kwargs):\n",
        "  root_token_id = tokenizer.encode(root)\n",
        "  sentence = SearchTree(root,0,token_id =root_token_id,model = model, tokenizer = tokenizer)\n",
        "  context = []\n",
        "  entropy_array = []\n",
        "  num_tokens = numTokens\n",
        "  content = []\n",
        "  probability = []\n",
        "  with torch.no_grad():\n",
        "     tokens_50K = generate_token_and_probability(model, tokenizer, [root], top_k=numTokens)\n",
        "\n",
        "  #unique_elements = []   # to store unique elements at each iteration\n",
        "  unique_tokens = set()\n",
        "  probabilityMatrix = []\n",
        "  uniqueTokensList = []\n",
        "  new_content = []\n",
        "  uniqueTokenLength = []\n",
        "  lastTokens_probability = []\n",
        "  flops_counter = {}\n",
        "  generated_sentence_GI = ''\n",
        "  batch_size = 15\n",
        "  holdout_number = 15\n",
        "  trellis_paths = []\n",
        "  for i in range(num_tokens):\n",
        "    _,token_id,context,prob = tokens_50K[0][i]  # Assuming it's structured as a tuple (best_token, token, probability)\n",
        "    # context = context.strip()  #This is not the correct solution. I am doing this rather than only leaving one strip command in search tree because I am appending to unique tokens before I am assigning this to search tree.\n",
        "    # context2 = context.strip()\n",
        "    # bad_prediction_checker = check_bad_predictions(context2)\n",
        "    initial_loop_probability.append(prob)\n",
        "#     print(\"initial_loop:\")\n",
        "#     print(tokens_50K[0][i])\n",
        "    unique_tokens.add(context)\n",
        "    probability.append(prob)\n",
        "    context = SearchTree(context,prob,token_id = token_id.item(),model = model,tokenizer = tokenizer,parent =sentence,parent_index = 0)\n",
        "    new_content.append(context)\n",
        "    context.create_child()\n",
        "    uniqueTokensList.append(context)\n",
        "\n",
        "  entropy_array.append([entropy(np.array([math.exp(prob) for prob in probability]))])\n",
        "  content.append(new_content)\n",
        "  previousUniqueLength = num_tokens\n",
        "  #unique_elements.append(unique_tokens)\n",
        "  initialStateProbability = probability\n",
        "  uniqueTokenLength.append(num_tokens)\n",
        "  max_index = initial_loop_probability.index(max(initial_loop_probability))\n",
        "  generated_sentence_GI = uniqueTokensList[max_index].build_Context()\n",
        "  for i in range(2,loop_runner):\n",
        "    unique_tokens = set()\n",
        "    probability = []\n",
        "    entropies = []\n",
        "    new_content = []\n",
        "    total_predictions = []\n",
        "    previousSetLength = 0\n",
        "    batch_sentences = [child.build_Context() for child in uniqueTokensList]\n",
        "\n",
        "    if len(batch_sentences)>batch_size:\n",
        "        total_predictions = []\n",
        "        start_index = 0\n",
        "        num_sentences_left = len(batch_sentences)\n",
        "        while (num_sentences_left>batch_size):\n",
        "            batch_sentences2 = batch_sentences[start_index*batch_size:(start_index+1)*batch_size]\n",
        "            with torch.no_grad():\n",
        "              batch_predictions =  generate_token_and_probability(model, tokenizer, batch_sentences2, top_k=numTokens)\n",
        "            total_predictions.extend(batch_predictions)\n",
        "            start_index +=1\n",
        "            num_sentences_left -= batch_size\n",
        "        if num_sentences_left > 0:\n",
        "           batch_sentences2 = batch_sentences[start_index*batch_size :]\n",
        "           with torch.no_grad():\n",
        "             batch_predictions =  generate_token_and_probability(model, tokenizer, batch_sentences2, top_k=numTokens)\n",
        "           total_predictions.extend(batch_predictions)\n",
        "    else:\n",
        "        with torch.no_grad():\n",
        "            total_predictions = generate_token_and_probability(model, tokenizer, batch_sentences,top_k=numTokens)\n",
        "\n",
        "    for j in range(len(uniqueTokensList)):\n",
        "      for s in range(num_tokens):\n",
        "        _,token_id,context,prob = total_predictions[j][s]\n",
        "        context2 = context.strip()\n",
        "        #bad_predictions_checker = check_bad_predictions(context2)\n",
        "        # if context2:\n",
        "        unique_tokens.add(context)   # also this if condition is not the correct solution\n",
        "        context = SearchTree(context,prob,token_id = token_id.item(),model = model,tokenizer = tokenizer,parent = uniqueTokensList[j])   #probably redundant: Because I should only create SearchTree of unique tokens\n",
        "        # context.create_child() Removed this 2/19/2025\n",
        "        if (len(unique_tokens)>previousSetLength):\n",
        "          previousSetLength = len(unique_tokens)\n",
        "          uniqueTokensList.append(context)\n",
        "          new_content.append(context)\n",
        "\n",
        "    #unique_elements.append(unique_tokens) # append the unique tokens list at each iteration to unique_elements list\n",
        "    content.append(new_content) # for storing tokens which will pass to the decode_path function.\n",
        "\n",
        "\n",
        "    comb_prob = []\n",
        "    for prevToken in uniqueTokensList[:previousUniqueLength]:\n",
        "      comb_prob.append(findProbability(prevToken,uniqueTokensList[previousUniqueLength:], model,tokenizer))\n",
        "    comb_prob = list(itertools.chain(*comb_prob)) # flattening the list\n",
        "\n",
        "    for tokenumber,newToken in enumerate(uniqueTokensList[previousUniqueLength:]):\n",
        "      probs = [comb_prob[a*len(uniqueTokensList[previousUniqueLength:]) + tokenumber] for a in range(len(uniqueTokensList[:previousUniqueLength]))]\n",
        "      probs2 = [probs[b] + uniqueTokensList[:previousUniqueLength][b].calcProbTillNow() for b in range(len(probs))]\n",
        "      entropies.append(list(entropy(np.array([math.exp(prob) for prob in probs]))))\n",
        "#       print(\"parent_prob Up Till now: \",[uniqueTokensList[:previousUniqueLength][i].calcProbTillNow() for i in range(len(probs))])\n",
        "#       print(\"combined probs: \", probs2)\n",
        "#       print(\"actual_probs: \", [math.exp(probs2[i]) for i in range(len(probs2))])\n",
        "      if not probs2:\n",
        "        continue\n",
        "      else:\n",
        "        max_value = max(probs2)\n",
        "        max_index = probs2.index(max_value)\n",
        "        new_transition_probability = probs[max_index]\n",
        "        newToken.replace_parent(uniqueTokensList[:previousUniqueLength][max_index])\n",
        "        newToken.change_probability(new_transition_probability,max_value) # just added this 4/4/2025\n",
        "        newToken.assign_parent_index(max_index)\n",
        "        if (i == loop_runner-1):\n",
        "             #print(\"parent_assigning_loop\")\n",
        "#             print(tokenumber)\n",
        "             #print(\"uniqueToken.context: \",uniqueTokensList[previousUniqueLength+tokenumber].context)\n",
        "#             print(\"parent_calc_prob_till_now: \",probs3)\n",
        "#             print(\"new_context: \",uniqueTokensList[previousUniqueLength+tokenumber].build_Context())\n",
        "             #print(\"new_transition_probability: \", new_transition_probability)\n",
        "#             print(\"new_total_prob: \", max_value)\n",
        "            lastTokens_probability.append(max_value)\n",
        "            trellis_paths.append(newToken.token_idsTillNow())\n",
        "\n",
        "      probability.append(probs)\n",
        "    probabilityMatrix.append(probability)\n",
        "    entropy_array.append(entropies)\n",
        "    # flops_counter[i-1] = model.get_batch_prediction_count()\n",
        "    #model.reset_batch_prediction_count()\n",
        "\n",
        "    uniqueTokenLength.append(len(uniqueTokensList[previousUniqueLength:]))\n",
        "\n",
        "    previousUniqueLength = len(uniqueTokensList[previousUniqueLength:])\n",
        "    uniqueTokensList = uniqueTokensList[len(uniqueTokensList)-previousUniqueLength:]\n",
        "\n",
        "    if (i ==loop_runner-1):\n",
        "        max_lastToken = max(lastTokens_probability)\n",
        "        max_lastTokenIndex = lastTokens_probability.index(max_lastToken)\n",
        "        generated_sentence_GI = uniqueTokensList[max_lastTokenIndex].build_Context()\n",
        "\n",
        "  return {\"probabilityMatrix\": probabilityMatrix, \"initialStateProbability\": initialStateProbability,\"content\": content,\"uniqueTokenLength\": uniqueTokenLength,\n",
        "          \"generated_sentence_GI\": generated_sentence_GI, \"entropies_array\": entropy_array,\"trellis_paths\":trellis_paths} #, flops_counter\n",
        "def runViterbiTransformerPipeline(rootSentence, numTokens = 3, loop_runner=3,**kwargs):\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "    result= generateIntermediates(rootSentence,model,tokenizer,numTokens = numTokens,loop_runner =loop_runner+1,**kwargs)\n",
        "    probabilityMatrix,initialStateProbability,content,uniqueTokenLength,generated_sentence_GI,entropy_array,trellis_paths  = result[probabilityMatrix],result[initialStateProbability],result[content],result[uniqueTokenLength],result[generated_sentence_GI],result[entropy_array],result[trellis_paths]\n",
        "    best_path,viterbi_mat,best_path_prob = VITERBI_Lists(probabilityMatrix, initialStateProbability,device)\n",
        "    print(\"uniqueTokenLength: \", uniqueTokenLength)\n",
        "    print(\"best_path: \", best_path)\n",
        "    decodedString = decodePath(best_path,content,rootSentence,tokenizer)\n",
        "    return decodedString,best_path_prob,generated_sentence_GI,entropy_array"
      ],
      "metadata": {
        "trusted": true,
        "id": "fbc273fb-5539-44db-a022-aa3763c55134"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}