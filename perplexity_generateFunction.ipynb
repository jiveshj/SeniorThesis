{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2307ad42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------------------------------------\n",
    "# Setup\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "import math\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import itertools\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "import math\n",
    "from time import sleep, time\n",
    "\n",
    "\n",
    "\n",
    "class LMHeadModel:\n",
    "    def __init__(self, model_name, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "        # Initialize the model and tokenizer\n",
    "        self.device = device\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float32).to(self.device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model.eval()\n",
    "\n",
    "        # Ensure the tokenizer has a padding token\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token  # Use EOS token as padding\n",
    "            self.tokenizer.padding_side = \"right\"\n",
    "\n",
    "        self.batch_prediction_count = 0\n",
    "\n",
    "\n",
    "    def batch_encode(self, sentences):\n",
    "        \"\"\"\n",
    "        Encodes a batch of sentences into input tensors.\n",
    "        Args:\n",
    "            sentences (list of str): The input sentences to encode.\n",
    "        Returns:\n",
    "            inputs (dict): A dictionary of tokenized inputs ready for the model.\n",
    "        \"\"\"\n",
    "        return self.tokenizer(\n",
    "            sentences,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,  # Pad to the longest sequence in the batch\n",
    "            truncation=True,  # Truncate sequences longer than the model's max length\n",
    "        ).to(self.device)\n",
    "\n",
    "    def batch_decode(self, token_ids):\n",
    "        \"\"\"\n",
    "        Decodes a batch of token IDs back to sentences.\n",
    "        Args:\n",
    "            token_ids (torch.Tensor): A tensor of token IDs to decode.\n",
    "        Returns:\n",
    "            decoded_sentences (list of str): The decoded sentences.\n",
    "        \"\"\"\n",
    "        return self.tokenizer.batch_decode(token_ids, skip_special_tokens=True)\n",
    "    def batch_decode_top_k(self, token_ids_batch, tokenizer):\n",
    "        \"\"\"\n",
    "        Decodes token IDs to meaningful text while merging subword tokens.\n",
    "        Args:\n",
    "            token_ids_batch (torch.Tensor): A batch of token IDs (e.g., from `topk`).\n",
    "            tokenizer: The tokenizer used for encoding/decoding.\n",
    "        Returns:\n",
    "            list of list of str: Decoded tokens (words/subwords) for each sequence in the batch.\n",
    "        \"\"\"\n",
    "        decoded_tokens = []\n",
    "        for token_ids in token_ids_batch:\n",
    "            # Decode each token ID in the batch, joining subwords correctly\n",
    "            tokens = [tokenizer.decode([token_id]).strip() for token_id in token_ids]\n",
    "            decoded_tokens.append(tokens)\n",
    "        return decoded_tokens\n",
    "\n",
    "    def get_batch_predictions(self, sentences, top_k=100):\n",
    "        \"\"\"\n",
    "        Predicts the next tokens for a batch of input sentences.\n",
    "        Args:\n",
    "            sentences (list of str): The input sentences.\n",
    "            top_k (int): Number of top tokens to return for each sentence.\n",
    "        Returns:\n",
    "            predictions (list of list of tuples): Top-k token predictions for each sentence.\n",
    "        \"\"\"\n",
    "        #Increment to see how many times this function is called after a given layer of trellis.\n",
    "        self.batch_prediction_count += 1\n",
    "\n",
    "\n",
    "        # Tokenize inputs\n",
    "        inputs = self.batch_encode(sentences)\n",
    "\n",
    "        # Pass through the model\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs,use_cache = False)\n",
    "\n",
    "        # Get logits for the last token in each sequence\n",
    "        logits = outputs.logits[:, -1, :]  # Shape: (batch_size, vocab_size)\n",
    "\n",
    "\n",
    "        # Compute probabilities using softmax\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        top_probs, top_token_ids = torch.topk(probs, k=top_k, dim=-1)\n",
    "        top_tokens = self.batch_decode_top_k(top_token_ids, self.tokenizer)\n",
    "\n",
    "\n",
    "        predictions = [\n",
    "            [(token, prob.item()) for token, prob in zip(top_tokens[i], top_probs[i]) if token and token != \"\\n\"]\n",
    "            for i in range(len(sentences))\n",
    "        ]\n",
    "        return predictions\n",
    "\n",
    "    def get_batch_prediction_count(self):\n",
    "        \"\"\"\n",
    "        Returns the number of times batch predictions have been made.\n",
    "        \"\"\"\n",
    "        return self.batch_prediction_count\n",
    "\n",
    "    def reset_batch_prediction_count(self):\n",
    "        \"\"\" Resets the count\n",
    "        \"\"\"\n",
    "\n",
    "        self.batch_prediction_count = 0\n",
    "class SearchTree:\n",
    "    def __init__(self,context,probability,token_id,model,tokenizer,parent = None,child = None,parent_index = None):\n",
    "        self.token_id = token_id\n",
    "        context = context.strip()\n",
    "        self.context = context\n",
    "        self.probability = probability\n",
    "        self.parent = parent\n",
    "        self.child = []\n",
    "        self.parent_index = parent_index  # newly created.\n",
    "        if child is not None:\n",
    "           self.child.append(child)\n",
    "        \n",
    "        # Cache cumulative probability at node creation\n",
    "        if parent:\n",
    "            self.cached_prob = parent.calcProbTillNow()+probability #parent.calcProbTillNow() * probability\n",
    "        else:\n",
    "            self.cached_prob = probability\n",
    "\n",
    "    def build_Context(self):\n",
    "        context_list = []\n",
    "        full_context = []\n",
    "        node = self\n",
    "        while node.parent is not None:\n",
    "            context_list.extend([node.token_id])\n",
    "            node = node.parent\n",
    "        context_list.reverse()\n",
    "        full_context.extend(node.token_id)\n",
    "        full_context.extend(context_list)\n",
    "        full_context = torch.tensor([full_context]) \n",
    "        generated_sentence = tokenizer.decode(full_context[0], skip_special_tokens=True)\n",
    "        return generated_sentence\n",
    "\n",
    "\n",
    "    def create_child(self):\n",
    "        if self.parent is not None:\n",
    "           self.parent.child.append(self)\n",
    "\n",
    "    def replace_parent(self, new_parent):\n",
    "        \"\"\"Assign a new parent and update cached probability.\"\"\"\n",
    "        self.parent = new_parent\n",
    "        self.cached_prob = new_parent.calcProbTillNow() + self.probability\n",
    "    \n",
    "\n",
    "    def calcProbTillNow(self):\n",
    "        \"\"\"Return cached cumulative probability to avoid redundant calculations.\"\"\"\n",
    "        return self.cached_prob\n",
    "    \n",
    "    def change_probability(self,new_probability):\n",
    "        self.cached_prob = self.cached_prob - self.probability\n",
    "        self.probability = new_probability\n",
    "        self.cached_prob += self.probability\n",
    "\n",
    "    # def calcProbTillNow(self):\n",
    "    #   prob = self.probability\n",
    "    #   node = self\n",
    "    #   while node.parent is not None:\n",
    "    #     prob = prob*node.parent.probability\n",
    "    #     node = node.parent\n",
    "    #   return prob    #can make this negative log probability.\n",
    "\n",
    "    def assign_parent_index(self,parent_index):\n",
    "      self.parent_index = parent_index\n",
    "\n",
    "\n",
    "def findProbability(InitialToken, FinalTokens, model,tokenizer):\n",
    "    context = InitialToken.build_Context()\n",
    "    # tokens_50K = model.get_batch_predictions([context], 500)\n",
    "    with torch.no_grad():\n",
    "       tokens_50K = generate_token_and_probability(model, tokenizer, [context], top_k=500)\n",
    "    token_dict = {}  # Dictionary to store only the first occurrence of each token\n",
    "\n",
    "    for _,token_id,token, prob in tokens_50K[0]:\n",
    "        # token = token.strip()\n",
    "        if token_id.item() not in token_dict or prob>token_dict[token_id.item()]:  # Store only the first occurrence\n",
    "            token_dict[token_id.item()] = prob\n",
    "    return [token_dict.get(FinalToken.token_id, -math.inf) for FinalToken in FinalTokens]  # Return probability if found, else 0\n",
    "\n",
    "\n",
    "\n",
    "def VITERBI_Lists(state_transition_probmat, initial_state_prob, device):\n",
    "    # Convert inputs to PyTorch tensors on GPU\n",
    "    initial_state_tensor = torch.tensor(initial_state_prob, dtype=torch.float32).to(device)\n",
    "    \n",
    "    # Initialize with first layer\n",
    "    viterbi_tensor = [initial_state_tensor]\n",
    "    backpointer = []\n",
    "    \n",
    "    # Process each time step\n",
    "    for time_step in range(len(state_transition_probmat)):\n",
    "        # Convert transition matrix for this time step to tensor\n",
    "        trans_probs = torch.tensor(state_transition_probmat[time_step], dtype=torch.float32).to(device)\n",
    "        \n",
    "        # Create matrices for vectorization\n",
    "        # Shape: [num_prev_states, num_current_states]\n",
    "        prev_probs = viterbi_tensor[-1].unsqueeze(1)  \n",
    "        \n",
    "        # Calculate all state transitions at once using matrix operations\n",
    "        # This replaces the inner loop over states\n",
    "        iteration_mat = prev_probs + trans_probs.t() \n",
    "        \n",
    "        # Find max values and indices in one operation\n",
    "        maxval, maxind = torch.max(iteration_mat, dim=0)\n",
    "        \n",
    "        viterbi_tensor.append(maxval)\n",
    "        backpointer.append(maxind.cpu().tolist())  # Move indices back to CPU for path tracking\n",
    "    \n",
    "    # Find best path\n",
    "    final_probs = viterbi_tensor[-1]\n",
    "    best_path_prob, max_index = torch.max(final_probs, dim=0)\n",
    "    best_backpointer = max_index.item()\n",
    "    \n",
    "    # Backtrack to find path\n",
    "    best_path = [best_backpointer]\n",
    "    j = 0\n",
    "    for i in reversed(range(len(state_transition_probmat))):\n",
    "        best_path.append(backpointer[i][best_path[j]])\n",
    "        j += 1\n",
    "    \n",
    "    best_path = best_path[::-1]\n",
    "    \n",
    "    # Convert tensors back to lists for return\n",
    "    viterbi_mat = [tensor.cpu().tolist() for tensor in viterbi_tensor]\n",
    "    \n",
    "    return best_path, viterbi_mat, best_path_prob.item()\n",
    "\n",
    "def VITERBI_Lists_2(state_transition_probmat, initial_state_prob):\n",
    "\n",
    "    viterbi_mat = []\n",
    "    backpointer = []\n",
    "    viterbi_1stLayer = []\n",
    "    for i in range(len(initial_state_prob)):\n",
    "        viterbi_1stLayer.append(float(initial_state_prob[i]))\n",
    "    viterbi_mat.append(viterbi_1stLayer)\n",
    "\n",
    "    for time_step in range(len(state_transition_probmat)):\n",
    "        viterbi_layer = []\n",
    "        backpointer_layer = []\n",
    "        for state in range(len(state_transition_probmat[time_step])):\n",
    "            iteration_vec = [viterbi_mat[time_step][i]+state_transition_probmat[time_step][state][i] for i in range(len(viterbi_mat[time_step]))]\n",
    "\n",
    "            maxval = max(iteration_vec)\n",
    "            maxind = iteration_vec.index(maxval)\n",
    "            viterbi_layer.append(maxval)\n",
    "            backpointer_layer.append(maxind)\n",
    "\n",
    "        viterbi_mat.append(viterbi_layer)\n",
    "        backpointer.append(backpointer_layer)\n",
    "\n",
    "    best_path_prob = max(viterbi_mat[-1])\n",
    "    # max_index = max(range(len(viterbi_mat[-1])), key = lambda i: viterbi_mat[-1][i])\n",
    "    max_index = viterbi_mat[-1].index(best_path_prob)\n",
    "    best_backpointer = max_index\n",
    "    best_path = [best_backpointer]\n",
    "    j = 0\n",
    "    for i in reversed(range(len(state_transition_probmat))):\n",
    "        best_path.append(backpointer[i][best_path[j]])\n",
    "        j += 1\n",
    "    best_path = best_path[::-1]\n",
    "    return best_path, viterbi_mat,best_path_prob\n",
    "def decodePath(best_path,unique_tokens_list,root_string,tokenizer):\n",
    "    resultant_token_ids = []\n",
    "    root_ids = tokenizer.encode(root_string)\n",
    "    resultant_token_ids.extend(root_ids)\n",
    "    resultant_token_ids.extend([unique_tokens_list[i][best_path[i]].token_id for i in range(len(best_path))])\n",
    "\n",
    "    \n",
    "    # generated_sentence = tokenizer.decode(resultant_token_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    resultant_token_ids = torch.tensor([resultant_token_ids])\n",
    "    generated_sentence = tokenizer.decode(resultant_token_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    print(\"resultant_token_ids: \", resultant_token_ids)\n",
    "    return generated_sentence\n",
    "\n",
    "def generate_token_and_probability(model, tokenizer, batch_prompts,max_length=1, top_k=4):\n",
    "    tokenizer.pad_token= tokenizer.eos_token\n",
    "    tokenized_result = tokenizer(batch_prompts, return_tensors=\"pt\",padding = True,truncation = True)\n",
    "    input_ids = tokenized_result[\"input_ids\"].to(model.device)\n",
    "    # print(\"input_ids from the function: \", input_ids)\n",
    "    # for i in range(len(input_ids)):\n",
    "    #     if input_ids[i][-1] == 50256 and probabilityMatrix is not None: #50256 is end of text token or the pad token \n",
    "    #         newToken = SearchTree(model.decode(input_ids[i][-2]),0,input_ids[i][-2].item(),model,tokenizer,parent = uniqueTokensList[i])\n",
    "    #         prob = findProbability(uniqueTokensList[i].parent,newToken,model,tokenizer)\n",
    "    #         newToken.change_probability(prob)\n",
    "    #         probabilityMatrix[i] = prob #probabilityMatrix of one previous iteration should be changed to 0\n",
    "    #         uniqueTokensList[i] = newToken\n",
    "\n",
    "    attn_mask = tokenized_result[\"attention_mask\"].to(model.device)\n",
    "    with torch.no_grad():\n",
    "      outputs = model.generate( \n",
    "         input_ids=input_ids,\n",
    "         attention_mask=attn_mask,\n",
    "         max_length=input_ids.size(-1) + max_length,\n",
    "         do_sample=False,  # Greedy decoding\n",
    "         output_scores=True, \n",
    "         return_dict_in_generate=True\n",
    "        )\n",
    "\n",
    "    sequences, scores = outputs.sequences, outputs.scores  # scores will have only one element per batch\n",
    "    predictions = []\n",
    "    # print(\"generated_token_id\",generated_token_id)\n",
    "    for i in range(len(batch_prompts)):\n",
    "        generated_token_id = sequences[i][input_ids.size(-1):].tolist()[0]  # Extract generated token ID\n",
    "        generated_token = tokenizer.decode(generated_token_id, skip_special_tokens=True)\n",
    "       \n",
    "\n",
    "        # Log probabilities of all possible tokens at the generated step\n",
    "        log_probs = torch.nn.functional.log_softmax(scores[0][i], dim=-1)  # scores[0] corresponds to the single generation step\n",
    "        # Keep increasing top_k until we have enough valid tokens\n",
    "        valid_predictions = []\n",
    "        curr_top_k = top_k\n",
    "        while len(valid_predictions) < top_k:\n",
    "            topk_logprobs, topk_ids = log_probs.topk(curr_top_k)  # Get top-k log probabilities\n",
    "            topk_tokens = tokenizer.batch_decode(topk_ids, skip_special_tokens=True)\n",
    "\n",
    "            # Filter out token `198` if the last token in input_ids was `198`\n",
    "            if input_ids[i][-1] == 198:\n",
    "                valid_predictions = [\n",
    "                    (generated_token,tid, tok, lp.item()) for tid, tok, lp in zip(topk_ids, topk_tokens, topk_logprobs) if tid != 198\n",
    "                ]\n",
    "            else:\n",
    "                valid_predictions = [\n",
    "                    (generated_token,tid, tok, lp.item()) for tid, tok, lp in zip(topk_ids, topk_tokens, topk_logprobs)\n",
    "                ]\n",
    "\n",
    "            # If filtering removed tokens, increase `curr_top_k` to get more candidates\n",
    "            if len(valid_predictions) < top_k:\n",
    "                curr_top_k += 1  # Expand search to get more tokens\n",
    "\n",
    "        # Store only the required `top_k` valid predictions\n",
    "        predictions.append(valid_predictions[:top_k])\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "\n",
    "def check_bad_predictions(text):\n",
    "    bad_patterns = [r'={2,}', r'!{2,}', r'\\?{2,}', r',{2,}', r';{2,}', r'\\|{2,}', r'~{2,}', r'&{2,}', r'-{2,}']\n",
    "    \n",
    "    # Check for unwanted punctuation patterns (two or more consecutive occurrences)\n",
    "    for pattern in bad_patterns:\n",
    "        if re.search(pattern, text):\n",
    "            return True\n",
    "    \n",
    "    # Check for non-ASCII characters\n",
    "    if any(ord(char) > 127 for char in text):  # ASCII characters are in the range 0-127\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def generateIntermediates(root,model,tokenizer,numTokens = 3, loop_runner = 4): \n",
    "  root_token_id = tokenizer.encode(root)\n",
    "  sentence = SearchTree(root,0,token_id =root_token_id,model = model, tokenizer = tokenizer)\n",
    "  context = []\n",
    "  prob_list = []\n",
    "  num_tokens = numTokens\n",
    "  content = []\n",
    "  probability = []\n",
    "  with torch.no_grad():\n",
    "     tokens_50K = generate_token_and_probability(model, tokenizer, [root], top_k=numTokens) #arbitrarly setting +1\n",
    "  children = []\n",
    "  overlap = []\n",
    "  most_common = []\n",
    "  #unique_elements = []   # to store unique elements at each iteration\n",
    "  unique_tokens = set()\n",
    "  probabilityMatrix = []\n",
    "  uniqueTokensList = []\n",
    "  new_content = []\n",
    "  uniqueTokenLength = []\n",
    "  lastTokens_probability = []\n",
    "  generated_sentence_GI = ''\n",
    "  flops_counter = {}\n",
    "  cached_probs = {}\n",
    "  batch_size = 5\n",
    "  holdout_number = 15\n",
    "  for i in range(num_tokens):\n",
    "    _,token_id,context,prob = tokens_50K[0][i]  # Assuming it's structured as a tuple (best_token, token, probability)\n",
    "    # context = context.strip()  #This is not the correct solution. I am doing this rather than only leaving one strip command in search tree because I am appending to unique tokens before I am assigning this to search tree. \n",
    "    # context2 = context.strip()\n",
    "    # bad_prediction_checker = check_bad_predictions(context2)\n",
    "    print(\"initial_loop\")\n",
    "    print(tokens_50K[0][i])\n",
    "    unique_tokens.add(context)\n",
    "    probability.append(prob)  \n",
    "    context = SearchTree(context,prob,token_id = token_id.item(),model = model,tokenizer = tokenizer,parent =sentence,parent_index = 0)\n",
    "    new_content.append(context)\n",
    "    context.create_child()\n",
    "    uniqueTokensList.append(context)\n",
    "    children.append(context)\n",
    "\n",
    "  content.append(new_content)\n",
    "  previousUniqueLength = num_tokens\n",
    "  #unique_elements.append(unique_tokens)\n",
    "  initialStateProbability = probability \n",
    "  uniqueTokenLength.append(num_tokens)\n",
    "  for i in range(2,loop_runner):\n",
    "    unique_tokens = set()\n",
    "    probability = []\n",
    "    new_content = []\n",
    "    total_predictions = []\n",
    "    previousSetLength = 0\n",
    "    batch_sentences = [child.build_Context() for child in uniqueTokensList]\n",
    "    # if ( i == loop_runner-1 or i == loop_runner-2):\n",
    "    #     print(batch_sentences)\n",
    "\n",
    "    if len(batch_sentences)>batch_size:\n",
    "        total_predictions = []\n",
    "        start_index = 0\n",
    "        num_sentences_left = len(batch_sentences)\n",
    "        while (num_sentences_left>batch_size):\n",
    "            batch_sentences2 = batch_sentences[start_index*batch_size:(start_index+1)*batch_size]\n",
    "\n",
    "            with torch.no_grad():\n",
    "              batch_predictions =  generate_token_and_probability(model, tokenizer, batch_sentences2, top_k=numTokens)\n",
    "            total_predictions.extend(batch_predictions)\n",
    "            start_index +=1\n",
    "            num_sentences_left -= batch_size\n",
    "        if num_sentences_left > 0:  \n",
    "           batch_sentences2 = batch_sentences[start_index*batch_size :]\n",
    "           with torch.no_grad():\n",
    "             batch_predictions =  generate_token_and_probability(model, tokenizer, batch_sentences2, top_k=numTokens)\n",
    "           total_predictions.extend(batch_predictions)\n",
    "\n",
    "\n",
    "        # batch_sentences2 = batch_sentences[0:-holdout_number]\n",
    "        # batch_predictions1 = generate_token_and_probability(model, tokenizer, batch_sentences[-holdout_number:],top_k=numTokens)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "          total_predictions = generate_token_and_probability(model, tokenizer, batch_sentences,top_k=numTokens)\n",
    "\n",
    "    for j in range(len(uniqueTokensList)):\n",
    "      for s in range(num_tokens):\n",
    "        # if (i == loop_runner-1):\n",
    "        #     print(\"s: \", s)\n",
    "        #     print(total_predictions[j][s])\n",
    "        _,token_id,context,prob = total_predictions[j][s]\n",
    "        context2 = context.strip()\n",
    "        #bad_predictions_checker = check_bad_predictions(context2)\n",
    "        # if context2:\n",
    "        unique_tokens.add(context)   # also this if condition is not the correct solution\n",
    "        context = SearchTree(context,prob,token_id = token_id.item(),model = model,tokenizer = tokenizer,parent = uniqueTokensList[j])   #probably redundant: Because I should only create SearchTree of unique tokens\n",
    "        # context.create_child() Removed this 2/19/2025\n",
    "        if (len(unique_tokens)>previousSetLength):\n",
    "          previousSetLength = len(unique_tokens)\n",
    "          uniqueTokensList.append(context)\n",
    "          new_content.append(context)\n",
    "    \n",
    "    # if (i == loop_runner-1):\n",
    "    #   print(\"len_unique_tokenslist: \", len(uniqueTokensList))\n",
    "    #   for token in range(len(uniqueTokensList)):\n",
    "    #     print(uniqueTokensList[token].context)\n",
    "    #     if uniqueTokensList[token].context == \"of\":\n",
    "    #         print(token)\n",
    "    #         print(uniqueTokensList[token].build_Context())\n",
    "    #unique_elements.append(unique_tokens) # append the unique tokens list at each iteration to unique_elements list\n",
    "    content.append(new_content) # for storing tokens which will pass to the decode_path function.\n",
    "\n",
    "   \n",
    "    comb_prob = []\n",
    "    for prevToken in uniqueTokensList[:previousUniqueLength]:\n",
    "      comb_prob.append(findProbability(prevToken,uniqueTokensList[previousUniqueLength:], model,tokenizer))\n",
    "    comb_prob = list(itertools.chain(*comb_prob)) # flattening the list\n",
    "\n",
    "    for tokenumber,newToken in enumerate(uniqueTokensList[previousUniqueLength:]):\n",
    "      probs = [comb_prob[a*len(uniqueTokensList[previousUniqueLength:]) + tokenumber] for a in range(len(uniqueTokensList[:previousUniqueLength]))]\n",
    "      probs2 = [probs[i] + uniqueTokensList[:previousUniqueLength][i].calcProbTillNow() for i in range(len(probs))]\n",
    "        #   print(\"parent_prob Up Till now: \",[uniqueTokensList[:previousUniqueLength][i].calcProbTillNow() for i in range(len(probs))])\n",
    "        #   print(\"combined probs: \", probs2)\n",
    "        #   print(\"actual_probs: \", [math.exp(probs2[i]) for i in range(len(probs2))])\n",
    "      if not probs2:\n",
    "        continue\n",
    "      else:\n",
    "        max_value = max(probs2)\n",
    "        max_value2 = max(probs)\n",
    "        max_index = probs2.index(max_value)\n",
    "        newToken.replace_parent(uniqueTokensList[:previousUniqueLength][max_index])\n",
    "        newToken.change_probability(max_value) # just added this 3/27/2025\n",
    "        newToken.assign_parent_index(max_index)\n",
    "        if (i == loop_runner-1):\n",
    "            # print(\"parent_assigning_loop\")\n",
    "            # print(tokenumber)\n",
    "            # print(\"uniqueToken.context: \",uniqueTokensList[previousUniqueLength+tokenumber].context)\n",
    "            # print(\"new_prob: \", max_value)\n",
    "            # print(\"new_probs1: \", max_value2)\n",
    "            # print(\"new_context: \",uniqueTokensList[previousUniqueLength+tokenumber].build_Context())\n",
    "            lastTokens_probability.append(max_value)\n",
    "\n",
    "      probability.append(probs)\n",
    "    probabilityMatrix.append(probability)\n",
    "    # flops_counter[i-1] = model.get_batch_prediction_count()\n",
    "    #model.reset_batch_prediction_count()\n",
    "\n",
    "\n",
    "    uniqueTokenLength.append(len(uniqueTokensList[previousUniqueLength:]))\n",
    "\n",
    "    previousUniqueLength = len(uniqueTokensList[previousUniqueLength:])\n",
    "    uniqueTokensList = uniqueTokensList[len(uniqueTokensList)-previousUniqueLength:]\n",
    "    if (i ==loop_runner-1):\n",
    "        max_lastToken = max(lastTokens_probability)\n",
    "        max_lastTokenIndex = lastTokens_probability.index(max_lastToken)\n",
    "        generated_sentence_GI = uniqueTokensList[max_lastTokenIndex].build_Context()\n",
    "\n",
    "  return probabilityMatrix, initialStateProbability, content,uniqueTokenLength,generated_sentence_GI #, flops_counter\n",
    "def runViterbiTransformerPipeline(rootSentence, numTokens = 3, loop_runner=3):\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    probabilityMatrix,initialStateProbability,content,uniqueTokenLength,generated_sentence_GI = generateIntermediates(rootSentence,model,tokenizer,numTokens = numTokens,loop_runner =loop_runner+1)\n",
    "    best_path,viterbi_mat,best_path_prob = VITERBI_Lists(probabilityMatrix, initialStateProbability,device)\n",
    "    # print(\"uniqueTokenLength: \", uniqueTokenLength)\n",
    "    # print(\"best_path: \", best_path)\n",
    "    decodedString = decodePath(best_path,content,rootSentence,tokenizer)\n",
    "    return decodedString,best_path_prob,generated_sentence_GI\n",
    "def runTransformerPipeline(rootSentence,loop_runner = 3):\n",
    "  model = LMHeadModel(\"gpt2\")\n",
    "  prob = 1\n",
    "  finalSentence = rootSentence\n",
    "  for i in range(loop_runner):\n",
    "    tokens_50K = model.get_batch_predictions([finalSentence])\n",
    "\n",
    "    context = tokens_50K[0][0][0]\n",
    "    prob =  prob*tokens_50K[0][0][1]\n",
    "    if context in ['.',':',',','?','!',';'] or \"'\" in context:\n",
    "      finalSentence += context\n",
    "\n",
    "    else:\n",
    "      finalSentence = finalSentence + ' ' + context\n",
    "  return finalSentence,prob\n",
    "\n",
    "def gather_log_probabilities(logits: torch.Tensor, labels: torch.LongTensor) -> torch.Tensor:\n",
    "    \"\"\"Gather log probabilities of the given labels from the logits.\"\"\"\n",
    "    log_probs = torch.nn.functional.log_softmax(logits.float(), dim=-1)\n",
    "    log_probs_labels = log_probs.gather(dim=-1, index=labels.unsqueeze(dim=-1))\n",
    "    return log_probs_labels.squeeze(dim=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d7ff7ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt:   Troops are divided into five classes : Scouts , <unk> , Engineers , Lancers and Armored Soldier . Troopers can switch classes by changing their\n",
      "initial_loop\n",
      "(' class', tensor(1398), ' class', -2.7211570739746094)\n",
      "initial_loop\n",
      "(' class', tensor(8328), ' armor', -2.831005096435547)\n",
      "initial_loop\n",
      "(' class', tensor(1438), ' name', -3.1902198791503906)\n",
      "resultant_token_ids:  tensor([[ 8498,  2840,   389,  9086,   656,  1936,  6097,  1058, 30456,   837,\n",
      "          1279,  2954,    29,   837, 27170,   837,   406, 20811,   290, 29347,\n",
      "         20104,   764,  8498, 20618,   460,  5078,  6097,   416,  5609,   511,\n",
      "          1438,    11,  4279,   393,   416,  5609,   511]])\n",
      "Mydecoder_text:   Troops are divided into five classes : Scouts, <unk>, Engineers, Lancers and Armored Soldier. Troopers can switch classes by changing their name, rank or by changing their\n",
      "Mydecoder_text_GI:   Troops are divided into five classes : Scouts, <unk>, Engineers, Lancers and Armored Soldier. Troopers can switch classes by changing their class name. The Scout class is\n",
      "input ids:  tensor([[ 8498,  2840,   389,  9086,   656,  1936,  6097,  1058, 30456,   837,\n",
      "          1279,  2954,    29,   837, 27170,   837,   406, 20811,   290, 29347,\n",
      "         20104,   764,  8498, 20618,   460,  5078,  6097,   416,  5609,   511]])\n",
      "Mydecoder_prob:  -12.34911823272705\n",
      "beam_text:   Troops are divided into five classes : Scouts, <unk>, Engineers, Lancers and Armored Soldier. Troopers can switch classes by changing their class name.\n",
      "\n",
      ",,\n",
      "greedy_ids:  tensor([[ 8498,  2840,   389,  9086,   656,  1936,  6097,  1058, 30456,   837,\n",
      "          1279,  2954,    29,   837, 27170,   837,   406, 20811,   290, 29347,\n",
      "         20104,   764,  8498, 20618,   460,  5078,  6097,   416,  5609,   511,\n",
      "          1398,  1438,    13,   198,   198,    11,   837]])\n",
      "greedy_text:   Troops are divided into five classes : Scouts, <unk>, Engineers, Lancers and Armored Soldier. Troopers can switch classes by changing their class name.\n",
      "\n",
      ",,\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "prompt = prompts[7]\n",
    "print(\"prompt: \", prompt)\n",
    "Mydecoder_text,decoder_prob,generated_sentence_GI = runViterbiTransformerPipeline(prompt, loop_runner = 7)\n",
    "print(\"Mydecoder_text: \",Mydecoder_text)\n",
    "print(\"Mydecoder_text_GI: \", generated_sentence_GI)\n",
    "# print(math.exp(decoder_prob))\n",
    "# # -- Greedy decoding\n",
    "# Mydecoder_ids = tokenizer.encode(Mydecoder_text)\n",
    "# print(\"Mydecoder_ids: \", torch.tensor([Mydecoder_ids]))\n",
    "\n",
    "# Mydecoder_ids = torch.tensor([Mydecoder_ids])\n",
    "tokenized_result = tokenizer(prompt,return_tensors = \"pt\")\n",
    "input_ids = tokenized_result[\"input_ids\"]\n",
    "print(\"input ids: \", input_ids)\n",
    "attn_mask = tokenized_result['attention_mask']\n",
    "greedy_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_length= len(input_ids[0])+7,\n",
    "    do_sample=False,  # Greedy\n",
    "    attention_mask = attn_mask\n",
    ")\n",
    "beam_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=len(input_ids[0])+7,\n",
    "        num_beams=3,  \n",
    "        early_stopping=False, #check this\n",
    "        attention_mask = attn_mask\n",
    "    )\n",
    "beam_text = tokenizer.decode(beam_ids[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "print('Mydecoder_prob: ',decoder_prob)\n",
    "print(\"beam_text: \", beam_text)\n",
    "print(\"greedy_ids: \",greedy_ids)\n",
    "greedy_text = tokenizer.decode(greedy_ids[0], skip_special_tokens=True)\n",
    "print(\"greedy_text: \", greedy_text)\n",
    "text1 = tokenizer.decode(greedy_ids[0],skip_special_tokens = True)\n",
    "\n",
    "text2 = tokenizer.decode(greedy_ids[0],skip_special_tokens = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "157ec132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69.33952586368731\n",
      "60.24422949016\n",
      "83.93432413097291\n",
      "83.93432413097291\n"
     ]
    }
   ],
   "source": [
    "perplexity = compute_perplexity(model,tokenizer,Mydecoder_text)\n",
    "print(perplexity)\n",
    "perplexity3 = compute_perplexity(model,tokenizer,generated_sentence_GI)\n",
    "print(perplexity3)\n",
    "perplexity2 = compute_perplexity(model,tokenizer,greedy_text)\n",
    "print(perplexity2)\n",
    "perplexity3 = compute_perplexity(model,tokenizer,beam_text)\n",
    "print(perplexity3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f1dfcb01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids_batch_perplexity:  tensor([[  383,   983,  2540,  2478,   287,  3050,    11,  6872,   625,   257,\n",
      "          1588,  6903,   286,   262,   670,  1760,   319,   569, 18354,  7496,\n",
      "         17740,  2873,    13,  2893,   569, 18354,  7496, 17740,  2873,   373,\n",
      "          6198],\n",
      "        [  383,   983,  2540,  2478,   287,  3050,    11,  6872,   625,   257,\n",
      "          1588,  6903,   286,   262,   670,  1760,   319,   569, 18354,  7496,\n",
      "         17740,  2873,    13,  2893,   262,   983,   373,   991,   287,  2478,\n",
      "            11],\n",
      "        [  383,   983,  2540,  2478,   287,  3050,    11,  6872,   625,   257,\n",
      "          1588,  6903,   286,   262,   670,  1760,   319,   569, 18354,  7496,\n",
      "         17740,  2873,    13,  2893,   262,   983,   373,  7317,  5292,   284,\n",
      "           307],\n",
      "        [  383,   983,  2540,  2478,   287,  3050,    11,  6872,   625,   257,\n",
      "          1588,  6903,   286,   262,   670,  1760,   319,   569, 18354,  7496,\n",
      "         17740,  2873,    13,  2893,   569, 18354,  7496, 17740,  2873,   373,\n",
      "          2716]])\n",
      "model.device:  cpu\n",
      "target_log_probs:  tensor([[-6.8971e+00, -5.9742e+00, -9.3355e+00, -1.4186e+00, -3.6021e+00,\n",
      "         -1.3935e+00, -9.0817e+00, -2.2474e+00, -3.8045e+00, -4.6197e+00,\n",
      "         -1.8031e+00, -4.2098e-02, -7.8856e-01, -4.3350e+00, -1.9319e+00,\n",
      "         -1.7539e+00, -6.7932e+00, -2.0404e+00, -5.6884e-03, -2.7803e-02,\n",
      "         -2.4862e+00, -8.0722e-01, -4.4993e+00, -2.9459e+00, -2.5991e-03,\n",
      "         -9.1215e-03, -1.7106e-02, -2.1948e-01, -1.1450e+00, -2.5481e+00],\n",
      "        [-6.8971e+00, -5.9742e+00, -9.3355e+00, -1.4186e+00, -3.6021e+00,\n",
      "         -1.3935e+00, -9.0817e+00, -2.2474e+00, -3.8045e+00, -4.6197e+00,\n",
      "         -1.8031e+00, -4.2098e-02, -7.8856e-01, -4.3350e+00, -1.9319e+00,\n",
      "         -1.7539e+00, -6.7932e+00, -2.0404e+00, -5.6884e-03, -2.7803e-02,\n",
      "         -2.4862e+00, -8.0722e-01, -4.4993e+00, -1.3654e+00, -1.2799e+00,\n",
      "         -1.4422e+00, -2.4708e+00, -1.0456e+00, -2.4839e-01, -3.9433e-01],\n",
      "        [-6.8971e+00, -5.9742e+00, -9.3355e+00, -1.4186e+00, -3.6021e+00,\n",
      "         -1.3935e+00, -9.0817e+00, -2.2474e+00, -3.8045e+00, -4.6197e+00,\n",
      "         -1.8031e+00, -4.2098e-02, -7.8856e-01, -4.3350e+00, -1.9319e+00,\n",
      "         -1.7539e+00, -6.7932e+00, -2.0404e+00, -5.6884e-03, -2.7803e-02,\n",
      "         -2.4862e+00, -8.0722e-01, -4.4993e+00, -1.3654e+00, -1.2799e+00,\n",
      "         -1.4422e+00, -2.2457e+00, -2.2782e+00, -4.9738e-01, -4.6661e-01],\n",
      "        [-6.8971e+00, -5.9742e+00, -9.3355e+00, -1.4186e+00, -3.6021e+00,\n",
      "         -1.3935e+00, -9.0817e+00, -2.2474e+00, -3.8045e+00, -4.6197e+00,\n",
      "         -1.8031e+00, -4.2098e-02, -7.8856e-01, -4.3350e+00, -1.9319e+00,\n",
      "         -1.7539e+00, -6.7932e+00, -2.0404e+00, -5.6884e-03, -2.7803e-02,\n",
      "         -2.4862e+00, -8.0722e-01, -4.4993e+00, -2.9459e+00, -2.5991e-03,\n",
      "         -9.1215e-03, -1.7106e-02, -2.1948e-01, -1.1450e+00, -2.5895e+00]])\n",
      "{'perplexities': [15.682345390319824, 16.409217834472656, 17.152395248413086, 15.703968048095703]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def calculate_batch_perplexity(model,tokenizer,input_texts):\n",
    "    \"\"\"\n",
    "    Calculate perplexity for a batch of input texts using a pretrained language model.\n",
    "\n",
    "    Args:\n",
    "    - input_texts (List[str]): A list of input texts to evaluate.\n",
    "\n",
    "    Returns:\n",
    "    - List[float]: A list of perplexity scores, one for each input text.\n",
    "    \"\"\"\n",
    "    # Tokenize the batch of texts with padding for uniform length\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        input_texts, return_tensors=\"pt\", padding=True, truncation=True\n",
    "    )\n",
    "\n",
    "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(model.device)\n",
    "    print(\"input_ids_batch_perplexity: \", input_ids)\n",
    "    print(\"model.device: \", model.device)\n",
    "\n",
    "    # Pass the input batch through the model to get logits\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # Shift the logits and input_ids to align targets correctly\n",
    "    # Logits dimensions are: (batch_size, seq_length, vocab_size) \n",
    "    shift_logits = logits[:, :-1, :]  # Ignore the last token's logits\n",
    "    shift_labels = input_ids[:, 1:]   # Skip the first token in the labels\n",
    "\n",
    "    # Compute log probabilities\n",
    "    log_probs = torch.nn.functional.log_softmax(shift_logits, dim=-1)\n",
    "\n",
    "    # Gather the log probabilities for the correct tokens\n",
    "    target_log_probs = log_probs.gather(dim=-1, index=shift_labels.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "    # Mask out positions corresponding to padding tokens\n",
    "    target_log_probs = target_log_probs * attention_mask[:, 1:].to(log_probs.dtype)\n",
    "    print(\"target_log_probs: \", target_log_probs)\n",
    "\n",
    "\n",
    "    # Compute the mean negative log-likelihood for each sequence\n",
    "    negative_log_likelihood = -target_log_probs.sum(dim=-1) / attention_mask[:, 1:].sum(dim=-1)\n",
    "\n",
    "    # Compute perplexity for each sequence\n",
    "    perplexities = torch.exp(negative_log_likelihood)\n",
    "    perplexities = perplexities.tolist()\n",
    "    \n",
    "# Take mean of perplexities of each batch\n",
    "\n",
    "    return {\"perplexities\": perplexities}\n",
    "\n",
    "perplexities = calculate_batch_perplexity(model,tokenizer,[Mydecoder_text,generated_sentence_GI,greedy_text,beam_text])\n",
    "print(perplexities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e2d683",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = LMHeadModel(\"gpt2\")\n",
    "tokens = model2.get_batch_predictions([' Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles, is an RPG game'],top_k=500)\n",
    "print(tokens)\n",
    "for i in range(len(tokens[0])):\n",
    "    if '.' in tokens[0][i][0]:\n",
    "        print(tokens[0][i][1])\n",
    "        print(i)\n",
    "    else:\n",
    "        print(\"not there\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "bb794146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ria', tensor(7496), 'ria', -0.010240748524665833), ('ria', tensor(4484), 'rian', -4.634050369262695), ('ria', tensor(11719), 'riel', -9.42894172668457), ('ria', tensor(380), 'ri', -9.467607498168945), ('ria', tensor(1678), 'ries', -9.57353401184082), ('ria', tensor(19151), 'rians', -10.045183181762695), ('ria', tensor(563), 'ry', -10.460756301879883), ('ria', tensor(430), 'ra', -10.63487434387207), ('ria', tensor(305), 'ro', -10.640275955200195), ('ria', tensor(81), 'r', -11.179460525512695), ('ria', tensor(4454), 'rial', -11.369524002075195), ('ria', tensor(48969), 'rius', -11.664430618286133), ('ria', tensor(27250), 'rio', -11.676271438598633), ('ria', tensor(1313), 'ron', -12.14402961730957), ('ria', tensor(5034), 'rie', -12.242128372192383), ('ria', tensor(19172), 'rium', -12.382280349731445), ('ria', tensor(544), 'ia', -12.528398513793945), ('ria', tensor(1173), 'ric', -12.606416702270508), ('ria', tensor(2442), 'ris', -12.77562141418457), ('ria', tensor(799), 'rit', -12.927873611450195), ('ria', tensor(6582), 'rum', -13.128450393676758), ('ria', tensor(5250), 'rant', -13.309587478637695), ('ria', tensor(5305), 'real', -13.32774543762207), ('ria', tensor(2228), 'ried', -13.613252639770508), ('ria', tensor(5277), 'rier', -13.64067268371582), ('ria', tensor(22379), 'ril', -13.669694900512695), ('ria', tensor(260), 're', -13.766633987426758), ('ria', tensor(3972), 'ya', -13.810365676879883), ('ria', tensor(1373), 'ral', -13.84092903137207), ('ria', tensor(15355), 'rien', -13.852617263793945), ('ria', tensor(19753), 'rys', -13.953752517700195), ('ria', tensor(10466), 'rics', -13.95976448059082), ('ria', tensor(1358), 'ration', -14.08482551574707), ('ria', tensor(7596), 'rous', -14.089372634887695), ('ria', tensor(2596), 'ran', -14.160921096801758), ('ria', tensor(859), 'ram', -14.21733283996582), ('ria', tensor(3201), 'rain', -14.220293045043945), ('ria', tensor(15087), 'rants', -14.41356086730957), ('ria', tensor(14892), 'rod', -14.425783157348633), ('ria', tensor(1681), 'ney', -14.426973342895508), ('ria', tensor(622), 'ru', -14.556291580200195), ('ria', tensor(4087), 'riage', -14.633455276489258), ('ria', tensor(498), 'ial', -14.710634231567383), ('ria', tensor(1806), 'ring', -14.712953567504883), ('ria', tensor(13370), 'tic', -14.723588943481445), ('ria', tensor(3225), 'rol', -14.735170364379883), ('ria', tensor(952), 'io', -14.743715286254883), ('ria', tensor(3036), 'rim', -14.751375198364258), ('ria', tensor(4565), 'ronic', -14.80992317199707), ('ria', tensor(1585), 'rist', -14.85630989074707), ('ria', tensor(6335), 'rad', -14.908601760864258), ('ria', tensor(12769), 'rin', -14.91014289855957), ('ria', tensor(6525), 'rite', -14.997514724731445), ('ria', tensor(398), 'rom', -15.018373489379883), ('ria', tensor(1472), 'ror', -15.027772903442383), ('ria', tensor(39389), 'bara', -15.050981521606445), ('ria', tensor(85), 'v', -15.066972732543945), ('ria', tensor(3882), 'roid', -15.082826614379883), ('ria', tensor(2787), 'rem', -15.104860305786133), ('ria', tensor(5998), 'rome', -15.137361526489258), ('ria', tensor(295), 'ion', -15.140413284301758), ('ria', tensor(4563), 'gar', -15.154985427856445), ('ria', tensor(4111), 'rated', -15.186311721801758), ('ria', tensor(2519), 'rote', -15.271120071411133), ('ria', tensor(49), 'R', -15.272920608520508), ('ria', tensor(7640), 'rine', -15.27739143371582), ('ria', tensor(411), 'res', -15.29258918762207), ('ria', tensor(301), 'st', -15.309511184692383), ('ria', tensor(8704), 'ira', -15.350557327270508), ('ria', tensor(283), 'ar', -15.399919509887695), ('ria', tensor(31142), 'rology', -15.400163650512695), ('ria', tensor(6015), 'rc', -15.406023025512695), ('ria', tensor(198), '\\n', -15.41282844543457), ('ria', tensor(9413), 'yard', -15.422975540161133), ('ria', tensor(9386), 'iate', -15.431413650512695), ('ria', tensor(557), 'ire', -15.439104080200195), ('ria', tensor(26224), 'rm', -15.468263626098633), ('ria', tensor(20282), 'strom', -15.53141975402832), ('ria', tensor(37818), 'rek', -15.541139602661133), ('ria', tensor(1462), 'to', -15.548540115356445), ('ria', tensor(13), '.', -15.55772590637207), ('ria', tensor(15763), 'root', -15.592103958129883), ('ria', tensor(10312), 'aria', -15.606843948364258), ('ria', tensor(12602), 'rik', -15.613588333129883), ('ria', tensor(22609), 'rob', -15.630678176879883), ('ria', tensor(5758), 'rave', -15.639436721801758), ('ria', tensor(13154), 'ride', -15.64256477355957), ('ria', tensor(6440), 'stone', -15.658784866333008), ('ria', tensor(36671), 'riot', -15.65880012512207), ('ria', tensor(13907), 'rox', -15.66020393371582), ('ria', tensor(67), 'd', -15.674455642700195), ('ria', tensor(45895), 'rl', -15.69114875793457), ('ria', tensor(25192), 'rand', -15.694795608520508), ('ria', tensor(14992), 'rying', -15.722017288208008), ('ria', tensor(11751), 'rer', -15.73002815246582), ('ria', tensor(2510), 'rael', -15.739336013793945), ('ria', tensor(5714), 'stein', -15.813051223754883), ('ria', tensor(3438), 'dom', -15.815675735473633), ('ria', tensor(30997), 'rica', -15.819307327270508), ('ria', tensor(7757), 'stal', -15.837343215942383), ('ria', tensor(6098), 'cr', -15.847261428833008), ('ria', tensor(88), 'y', -15.85545539855957), ('ria', tensor(6286), 'born', -15.875932693481445), ('ria', tensor(17179), 'rh', -15.888582229614258), ('ria', tensor(2543), 'lam', -15.912797927856445), ('ria', tensor(24420), 'rera', -15.913057327270508), ('ria', tensor(14932), 'rus', -15.922990798950195), ('ria', tensor(44447), 'riots', -15.92692756652832), ('ria', tensor(7112), 'RI', -15.928895950317383), ('ria', tensor(2411), 'rel', -15.93388557434082), ('ria', tensor(12204), 'roc', -15.941362380981445), ('ria', tensor(18657), 'rov', -15.97490119934082), ('ria', tensor(312), 'id', -15.976564407348633), ('ria', tensor(8289), 'rient', -15.995973587036133), ('ria', tensor(9491), 'jar', -16.006319046020508), ('ria', tensor(6413), 'ua', -16.012392044067383), ('ria', tensor(7701), 'rior', -16.01457405090332), ('ria', tensor(1504), 'ior', -16.03087043762207), ('ria', tensor(445), 'red', -16.036958694458008), ('ria', tensor(25924), 'rene', -16.050386428833008), ('ria', tensor(5143), 'run', -16.053972244262695), ('ria', tensor(4364), 'rey', -16.057024002075195), ('ria', tensor(26145), 'rb', -16.066377639770508), ('ria', tensor(6893), 'bal', -16.090425491333008), ('ria', tensor(9520), 'iah', -16.09691047668457), ('ria', tensor(40329), 'rolog', -16.097352981567383), ('ria', tensor(21510), 'rex', -16.110322952270508), ('ria', tensor(21062), 'rr', -16.112077713012695), ('ria', tensor(527), 'ber', -16.11570930480957), ('ria', tensor(35631), 'rigan', -16.11766242980957), ('ria', tensor(4169), 'ste', -16.123231887817383), ('ria', tensor(35823), 'rano', -16.137712478637695), ('ria', tensor(3808), 'rs', -16.139558792114258), ('ria', tensor(5439), 'lo', -16.162721633911133), ('ria', tensor(7718), 'car', -16.174196243286133), ('ria', tensor(32188), 'raf', -16.17924690246582), ('ria', tensor(14579), 'rane', -16.179811477661133), ('ria', tensor(472), 'rou', -16.180574417114258), ('ria', tensor(822), 'rib', -16.183122634887695), ('ria', tensor(11469), 'rust', -16.184709548950195), ('ria', tensor(82), 's', -16.192659378051758), ('ria', tensor(43270), 'rocal', -16.196977615356445), ('ria', tensor(2417), 'yr', -16.199052810668945), ('ria', tensor(14094), 'tics', -16.20341682434082), ('ria', tensor(710), 'ne', -16.211763381958008), ('ria', tensor(11510), 'rac', -16.215364456176758), ('ria', tensor(8648), 'rogen', -16.233827590942383), ('ria', tensor(29466), 'lene', -16.255891799926758), ('ria', tensor(2536), 'str', -16.257936477661133), ('ria', tensor(27585), 'rade', -16.268922805786133), ('ria', tensor(5557), 'rick', -16.270235061645508), ('ria', tensor(18218), 'rev', -16.293581008911133), ('ria', tensor(75), 'l', -16.299379348754883), ('ria', tensor(3526), 'board', -16.317293167114258), ('ria', tensor(4372), 'rd', -16.318315505981445), ('ria', tensor(25986), 'ropolis', -16.321855545043945), ('ria', tensor(22500), 'blade', -16.338701248168945), ('ria', tensor(12931), 'iated', -16.34379768371582), ('ria', tensor(16740), 'race', -16.353242874145508), ('ria', tensor(9744), 'grad', -16.360979080200195), ('ria', tensor(5965), 'rors', -16.36888313293457), ('ria', tensor(42908), 'rette', -16.368913650512695), ('ria', tensor(3301), 'ara', -16.378129959106445), ('ria', tensor(12818), 'ridge', -16.387056350708008), ('ria', tensor(66), 'c', -16.388277053833008), ('ria', tensor(11392), 'rah', -16.391908645629883), ('ria', tensor(25619), 'rab', -16.413042068481445), ('ria', tensor(16451), 'riages', -16.441560745239258), ('ria', tensor(11), ',', -16.44554328918457), ('ria', tensor(33171), 'rone', -16.456682205200195), ('ria', tensor(83), 't', -16.477617263793945), ('ria', tensor(918), 'ren', -16.47819709777832), ('ria', tensor(12135), 'storm', -16.489030838012695), ('ria', tensor(2991), 'vil', -16.509859085083008), ('ria', tensor(4873), 'rate', -16.519609451293945), ('ria', tensor(34785), 'rero', -16.530168533325195), ('ria', tensor(3828), 'rog', -16.53032112121582), ('ria', tensor(8344), 'rec', -16.530458450317383), ('ria', tensor(29038), 'ryan', -16.53373908996582), ('ria', tensor(12022), 'aro', -16.540971755981445), ('ria', tensor(17946), 'loc', -16.547395706176758), ('ria', tensor(1042), 'ism', -16.554323196411133), ('ria', tensor(13001), 'bery', -16.56572151184082), ('ria', tensor(6780), 'lv', -16.56651496887207), ('ria', tensor(33640), 'roth', -16.592973709106445), ('ria', tensor(21468), 'rea', -16.597047805786133), ('ria', tensor(626), 'vel', -16.599992752075195), ('ria', tensor(318), ' is', -16.60899543762207), ('ria', tensor(5031), 'la', -16.620912551879883), ('ria', tensor(1437), 'ina', -16.631685256958008), ('ria', tensor(20040), 'rar', -16.633211135864258), ('ria', tensor(24660), 'lia', -16.63896369934082), ('ria', tensor(9892), 'dy', -16.657869338989258), ('ria', tensor(6138), 'quest', -16.66069221496582), ('ria', tensor(42407), 'roo', -16.662675857543945), ('ria', tensor(36714), 'croft', -16.665681838989258), ('ria', tensor(5889), 'arts', -16.702699661254883), ('ria', tensor(1416), 'sc', -16.712526321411133), ('ria', tensor(6448), 'sts', -16.73423957824707), ('ria', tensor(12212), 'rons', -16.739473342895508), ('ria', tensor(6058), 'rid', -16.741365432739258), ('ria', tensor(3766), 'raq', -16.753984451293945), ('ria', tensor(23156), 'rites', -16.754655838012695), ('ria', tensor(8231), 'ials', -16.76158332824707), ('ria', tensor(72), 'i', -16.771638870239258), ('ria', tensor(42902), 'roma', -16.77519416809082), ('ria', tensor(9620), 'lan', -16.79167366027832), ('ria', tensor(3247), 'raz', -16.80052375793457), ('ria', tensor(20310), 'rational', -16.801347732543945), ('ria', tensor(25135), 'rule', -16.805253982543945), ('ria', tensor(24833), 'rington', -16.809267044067383), ('ria', tensor(23104), 'strate', -16.81169319152832), ('ria', tensor(10599), 'rot', -16.830293655395508), ('ria', tensor(338), \"'s\", -16.84373664855957), ('ria', tensor(23063), 'ku', -16.846147537231445), ('ria', tensor(30062), 'quist', -16.85325813293457), ('ria', tensor(20470), 'cry', -16.853960037231445), ('ria', tensor(10366), 'rat', -16.854814529418945), ('ria', tensor(23341), 'rost', -16.870759963989258), ('ria', tensor(21244), 'riad', -16.873659133911133), ('ria', tensor(65), 'b', -16.88017463684082), ('ria', tensor(28716), 'rika', -16.88157844543457), ('ria', tensor(7058), 'iro', -16.902238845825195), ('ria', tensor(1495), '25', -16.909944534301758), ('ria', tensor(271), 'is', -16.923372268676758), ('ria', tensor(43591), 'ramids', -16.94359016418457), ('ria', tensor(9108), 'ritten', -16.96055793762207), ('ria', tensor(3823), 'room', -16.974550247192383), ('ria', tensor(2100), 'val', -16.97679328918457), ('ria', tensor(5657), 'bar', -16.980348587036133), ('ria', tensor(20058), 'rama', -16.982210159301758), ('ria', tensor(11252), 'rell', -16.987138748168945), ('ria', tensor(5877), 'nergy', -16.99058723449707), ('ria', tensor(3970), 'ica', -16.992403030395508), ('ria', tensor(15544), 'pit', -16.995729446411133), ('ria', tensor(452), 'iv', -17.00993537902832), ('ria', tensor(8207), 'iel', -17.025087356567383), ('ria', tensor(373), ' was', -17.037477493286133), ('ria', tensor(3743), 'ston', -17.04612922668457), ('ria', tensor(31173), 'jet', -17.046815872192383), ('ria', tensor(28930), 'vik', -17.05308723449707), ('ria', tensor(1186), 'ret', -17.05961799621582), ('ria', tensor(48489), 'yrs', -17.07182502746582), ('ria', tensor(8394), 'rage', -17.072343826293945), ('ria', tensor(38229), 'ringe', -17.07457160949707), ('ria', tensor(23651), ' Realm', -17.077287673950195), ('ria', tensor(14347), 'school', -17.080568313598633), ('ria', tensor(343), 'ir', -17.086259841918945), ('ria', tensor(4491), 'mor', -17.098024368286133), ('ria', tensor(6404), 'log', -17.09971809387207), ('ria', tensor(7527), 'rich', -17.10618782043457), ('ria', tensor(12151), 'iva', -17.112504959106445), ('ria', tensor(16664), 'iov', -17.114900588989258), ('ria', tensor(10215), 'cor', -17.118242263793945), ('ria', tensor(2053), 'rown', -17.120637893676758), ('ria', tensor(2487), 'roll', -17.123567581176758), ('ria', tensor(2118), 'rest', -17.136171340942383), ('ria', tensor(22267), 'rina', -17.14122200012207), ('ria', tensor(4108), 'rav', -17.141618728637695), ('ria', tensor(47119), 'roman', -17.150911331176758), ('ria', tensor(12048), 'script', -17.154970169067383), ('ria', tensor(16057), 'bra', -17.166688919067383), ('ria', tensor(7923), 'bell', -17.185274124145508), ('ria', tensor(48590), 'lio', -17.18681526184082), ('ria', tensor(32820), 'iates', -17.191438674926758), ('ria', tensor(5378), 'tion', -17.210664749145508), ('ria', tensor(1169), 'the', -17.216936111450195), ('ria', tensor(4951), 'ros', -17.219743728637695), ('ria', tensor(8461), 'ni', -17.226961135864258), ('ria', tensor(5685), 'rast', -17.231767654418945), ('ria', tensor(2856), 'cil', -17.23771858215332), ('ria', tensor(1894), 'ball', -17.239046096801758), ('ria', tensor(1359), 'ling', -17.247011184692383), ('ria', tensor(10823), 'rock', -17.247407913208008), ('ria', tensor(4399), 'verse', -17.248613357543945), ('ria', tensor(64), 'a', -17.256242752075195), ('ria', tensor(28750), 'stones', -17.258485794067383), ('ria', tensor(12957), 'last', -17.261491775512695), ('ria', tensor(20844), 'ronics', -17.264772415161133), ('ria', tensor(259), 'in', -17.277177810668945), ('ria', tensor(11930), 'wall', -17.28044319152832), ('ria', tensor(7207), 'fall', -17.286455154418945), ('ria', tensor(1505), 'ium', -17.30906867980957), ('ria', tensor(2433), 'ray', -17.309541702270508), ('ria', tensor(371), ' R', -17.311784744262695), ('ria', tensor(6894), 'world', -17.315767288208008), ('ria', tensor(6057), 'game', -17.319414138793945), ('ria', tensor(1836), 'lish', -17.33751106262207), ('ria', tensor(37518), 'rish', -17.33763313293457), ('ria', tensor(948), 'cy', -17.33824348449707), ('ria', tensor(1773), 'rop', -17.341096878051758), ('ria', tensor(325), 'se', -17.345766067504883), ('ria', tensor(13165), 'tor', -17.350526809692383), ('ria', tensor(5646), 'standing', -17.354997634887695), ('ria', tensor(9607), 'iac', -17.360414505004883), ('ria', tensor(29441), 'ryn', -17.360429763793945), ('ria', tensor(1669), 'iol', -17.362977981567383), ('ria', tensor(290), ' and', -17.377168655395508), ('ria', tensor(362), ' 2', -17.386964797973633), ('ria', tensor(46575), 'topia', -17.398805618286133), ('ria', tensor(9700), 'rates', -17.399995803833008), ('ria', tensor(29734), 'scene', -17.406816482543945), ('ria', tensor(11489), 'rett', -17.422868728637695), ('ria', tensor(687), 'form', -17.427370071411133), ('ria', tensor(433), 'art', -17.427705764770508), ('ria', tensor(2044), 'cher', -17.428010940551758), ('ria', tensor(7266), 'sub', -17.42839241027832), ('ria', tensor(4914), 'ka', -17.452348709106445), ('ria', tensor(3130), 'idge', -17.454011917114258), ('ria', tensor(17868), 'itbart', -17.463747024536133), ('ria', tensor(11669), 'thal', -17.467714309692383), ('ria', tensor(354), 'ch', -17.475160598754883), ('ria', tensor(38211), 'romancer', -17.480745315551758), ('ria', tensor(13698), 'rose', -17.48411750793457), ('ria', tensor(41871), 'rf', -17.486955642700195), ('ria', tensor(6888), 'ca', -17.49009895324707), ('ria', tensor(73), 'j', -17.493074417114258), ('ria', tensor(829), 'les', -17.497591018676758), ('ria', tensor(18798), 'llo', -17.502138137817383), ('ria', tensor(35906), 'rn', -17.502565383911133), ('ria', tensor(38763), 'rification', -17.511720657348633), ('ria', tensor(3900), 'berg', -17.513795852661133), ('ria', tensor(16), '1', -17.518373489379883), ('ria', tensor(3524), 'box', -17.521486282348633), ('ria', tensor(1636), 'ley', -17.52610969543457), ('ria', tensor(291), 'ic', -17.526277542114258), ('ria', tensor(8957), 'pa', -17.542329788208008), ('ria', tensor(17), '2', -17.561357498168945), ('ria', tensor(35836), 'scan', -17.573793411254883), ('ria', tensor(3754), 'ius', -17.57725715637207), ('ria', tensor(5605), 'arc', -17.578981399536133), ('ria', tensor(33100), 'iae', -17.581056594848633), ('ria', tensor(3699), 'arian', -17.58293342590332), ('ria', tensor(1470), 'raph', -17.586824417114258), ('ria', tensor(7661), 'oria', -17.590227127075195), ('ria', tensor(329), ' for', -17.59471321105957), ('ria', tensor(38049), 'aroo', -17.59514045715332), ('ria', tensor(320), 'im', -17.601991653442383), ('ria', tensor(1044), 'land', -17.60276985168457), ('ria', tensor(808), 'row', -17.60515022277832), ('ria', tensor(37326), 'rists', -17.605226516723633), ('ria', tensor(5375), 'iat', -17.624483108520508), ('ria', tensor(19743), ' Saga', -17.63383674621582), ('ria', tensor(1525), 'by', -17.63530158996582), ('ria', tensor(2101), 'iod', -17.640382766723633), ('ria', tensor(35386), 'mpire', -17.643266677856445), ('ria', tensor(11487), 'table', -17.64415168762207), ('ria', tensor(2731), 'uel', -17.647157669067383), ('ria', tensor(23299), 'fell', -17.652956008911133), ('ria', tensor(1679), ' 25', -17.65526008605957), ('ria', tensor(1122), 'ton', -17.667879104614258), ('ria', tensor(33865), 'rique', -17.66948127746582), ('ria', tensor(22853), 'stre', -17.682344436645508), ('ria', tensor(1501), 'arn', -17.689363479614258), ('ria', tensor(23912), 'lab', -17.692445755004883), ('ria', tensor(30909), 'lag', -17.69261360168457), ('ria', tensor(35759), 'yre', -17.693056106567383), ('ria', tensor(391), 'ain', -17.696901321411133), ('ria', tensor(13429), 'sis', -17.70463752746582), ('ria', tensor(9143), 'rations', -17.713685989379883), ('ria', tensor(2387), 'ires', -17.72929573059082), ('ria', tensor(43407), 'riter', -17.730257034301758), ('ria', tensor(29616), 'iris', -17.747865676879883), ('ria', tensor(1084), 'min', -17.74803352355957), ('ria', tensor(32527), 'rious', -17.74864387512207), ('ria', tensor(4005), 'aven', -17.748811721801758), ('ria', tensor(4448), 'ias', -17.75956916809082), ('ria', tensor(314), ' I', -17.760957717895508), ('ria', tensor(32040), 'rax', -17.764253616333008), ('ria', tensor(1954), '23', -17.770341873168945), ('ria', tensor(2242), ' 23', -17.778993606567383), ('ria', tensor(1820), 'my', -17.77946662902832), ('ria', tensor(2473), 'ival', -17.793413162231445), ('ria', tensor(1362), 'ger', -17.793489456176758), ('ria', tensor(5411), 'ania', -17.799715042114258), ('ria', tensor(1371), 'ards', -17.806276321411133), ('ria', tensor(1868), 'oid', -17.808427810668945), ('ria', tensor(33676), 'ROM', -17.818300247192383), ('ria', tensor(45241), 'rums', -17.821931838989258), ('ria', tensor(306), 'ly', -17.828615188598633), ('ria', tensor(43171), 'rival', -17.831789016723633), ('ria', tensor(1008), 'ner', -17.83506965637207), ('ria', tensor(19112), 'arted', -17.835420608520508), ('ria', tensor(40978), 'reth', -17.836549758911133), ('ria', tensor(25), ':', -17.844682693481445), ('ria', tensor(3323), 'craft', -17.845674514770508), ('ria', tensor(1213), 'bers', -17.851747512817383), ('ria', tensor(352), ' 1', -17.854188919067383), ('ria', tensor(1079), 'ved', -17.85429573059082), ('ria', tensor(22902), 'cler', -17.855806350708008), ('ria', tensor(16963), 'prom', -17.871416091918945), ('ria', tensor(17305), 'ido', -17.886857986450195), ('ria', tensor(39159), 'jam', -17.888368606567383), ('ria', tensor(668), 'ark', -17.891313552856445), ('ria', tensor(496), 'age', -17.89305305480957), ('ria', tensor(1548), 'icle', -17.89622688293457), ('ria', tensor(284), ' to', -17.898927688598633), ('ria', tensor(20141), 'neys', -17.903581619262695), ('ria', tensor(28643), 'iku', -17.910646438598633), ('ria', tensor(12371), 'iology', -17.922441482543945), ('ria', tensor(15516), 'lli', -17.92314338684082), ('ria', tensor(565), 'cl', -17.927339553833008), ('ria', tensor(2815), 'lin', -17.927507400512695), ('ria', tensor(262), ' the', -17.928895950317383), ('ria', tensor(561), ' would', -17.933671951293945), ('ria', tensor(21180), 'yrus', -17.938783645629883), ('ria', tensor(12708), 'static', -17.946489334106445), ('ria', tensor(33788), 'rake', -17.94651985168457), ('ria', tensor(25481), 'robe', -17.946931838989258), ('ria', tensor(357), ' (', -17.948869705200195), ('ria', tensor(3810), 'mine', -17.95311164855957), ('ria', tensor(2201), 'minist', -17.970170974731445), ('ria', tensor(12), '-', -17.98021125793457), ('ria', tensor(531), ' said', -17.983278274536133), ('ria', tensor(3245), 'field', -17.984163284301758), ('ria', tensor(30152), 'isol', -17.984865188598633), ('ria', tensor(24), '9', -17.985200881958008), ('ria', tensor(12675), 'リ', -17.98735237121582), ('ria', tensor(5219), 'state', -17.989015579223633), ('ria', tensor(15219), 'girl', -17.991823196411133), ('ria', tensor(7063), 'arl', -17.993486404418945), ('ria', tensor(2736), 'ze', -17.997453689575195), ('ria', tensor(25085), 'ivism', -17.998598098754883), ('ria', tensor(30691), 'rals', -18.000749588012695), ('ria', tensor(24891), 'journal', -18.002565383911133), ('ria', tensor(30354), 'punk', -18.006303787231445), ('ria', tensor(4188), 'que', -18.01225471496582), ('ria', tensor(46395), 'cade', -18.021730422973633), ('ria', tensor(27736), 'rers', -18.021852493286133), ('ria', tensor(32838), 'cru', -18.022798538208008), ('ria', tensor(10728), 'market', -18.023271560668945), ('ria', tensor(7670), 'vy', -18.029573440551758), ('ria', tensor(2733), 'ctions', -18.03166389465332), ('ria', tensor(1754), 'ler', -18.031999588012695), ('ria', tensor(13252), 'RO', -18.032411575317383), ('ria', tensor(40296), 'cube', -18.03349494934082), ('ria', tensor(14954), ' Racing', -18.03575325012207), ('ria', tensor(19106), 'shadow', -18.041017532348633), ('ria', tensor(10178), 'vision', -18.043596267700195), ('ria', tensor(8270), 'ille', -18.050294876098633), ('ria', tensor(17538), 'aiden', -18.054948806762695), ('ria', tensor(5420), 'ref', -18.056779861450195), ('ria', tensor(9860), 'yer', -18.06181526184082), ('ria', tensor(5235), 'gen', -18.069490432739258), ('ria', tensor(3755), 'ida', -18.069734573364258), ('ria', tensor(16474), 'ranean', -18.076875686645508), ('ria', tensor(1214), 'ross', -18.080080032348633), ('ria', tensor(31079), 'thia', -18.080354690551758), ('ria', tensor(519), 'og', -18.082063674926758), ('ria', tensor(481), ' will', -18.083498001098633), ('ria', tensor(7423), 'burg', -18.08421516418457), ('ria', tensor(40914), 'rx', -18.097429275512695), ('ria', tensor(8083), 'ima', -18.104692459106445), ('ria', tensor(49309), 'raised', -18.109926223754883), ('ria', tensor(8044), 'sex', -18.11406135559082), ('ria', tensor(10051), 'roph', -18.122316360473633), ('ria', tensor(36072), 'xia', -18.126054763793945), ('ria', tensor(1156), 'rent', -18.12852668762207), ('ria', tensor(22834), 'Rom', -18.131776809692383), ('ria', tensor(1640), 'for', -18.141633987426758), ('ria', tensor(19205), 'city', -18.15300178527832), ('ria', tensor(11718), 'hero', -18.153383255004883), ('ria', tensor(20481), 'rella', -18.162675857543945), ('ria', tensor(27612), 'wyn', -18.165895462036133), ('ria', tensor(4102), 'mark', -18.166093826293945), ('ria', tensor(43933), 'vre', -18.166414260864258), ('ria', tensor(6726), 'friend', -18.168581008911133), ('ria', tensor(1477), 'sh', -18.177186965942383), ('ria', tensor(5767), 'war', -18.177431106567383), ('ria', tensor(70), 'g', -18.180208206176758), ('ria', tensor(1980), 'irc', -18.18840217590332), ('ria', tensor(5528), 'rip', -18.191789627075195), ('ria', tensor(32762), 'gren', -18.19511604309082), ('ria', tensor(2743), 'ari', -18.195314407348633), ('ria', tensor(16514), 'tim', -18.198701858520508), ('ria', tensor(1676), 'pro', -18.19975471496582), ('ria', tensor(8143), 'rical', -18.200410842895508), ('ria', tensor(8821), 'rating', -18.201570510864258), ('ria', tensor(15266), 'written', -18.202455520629883), ('ria', tensor(7935), 'ione', -18.203310012817383), ('ria', tensor(24427), 'bral', -18.206926345825195), ('ria', tensor(30292), 'iola', -18.210466384887695), ('ria', tensor(20760), 'yt', -18.211381912231445), ('ria', tensor(805), 'man', -18.212556838989258), ('ria', tensor(3919), 'no', -18.218355178833008), ('ria', tensor(20970), 'rice', -18.220125198364258), ('ria', tensor(8846), 'iti', -18.227876663208008), ('ria', tensor(11377), 'public', -18.227998733520508), ('ria', tensor(12639), 'hill', -18.228899002075195), ('ria', tensor(30224), 'rail', -18.237642288208008), ('ria', tensor(8910), 'riers', -18.242128372192383), ('ria', tensor(8310), 'fr', -18.25023078918457), ('ria', tensor(15539), 'cycl', -18.25584602355957), ('ria', tensor(422), ' from', -18.261232376098633), ('ria', tensor(33750), 'yards', -18.262422561645508), ('ria', tensor(37827), 'thereal', -18.265871047973633), ('ria', tensor(15805), 'cost', -18.27305793762207), ('ria', tensor(26084), 'rans', -18.276460647583008), ('ria', tensor(521), 'ind', -18.277116775512695), ('ria', tensor(27398), 'ritis', -18.278825759887695)]\n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# batch_sentences = ['I enjoy walking in the park, but','I enjoy walking in the park, and', 'I enjoy walking in the park. I', 'I enjoy walking in the park and seeing', 'I enjoy walking in the park and it', 'I enjoy walking in the park. It', 'I enjoy walking in the park.\\n', 'I enjoy walking in the streets of New', 'I enjoy walking in the streets of the', 'I enjoy walking in the streets of London']\n",
    "batch_sentences = ['   The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While Valky']\n",
    "batch_predictions =  generate_token_and_probability(model, tokenizer, batch_sentences, top_k=500)\n",
    "\n",
    "for k in range(len(batch_sentences)):\n",
    "    print(batch_predictions[k])\n",
    "\n",
    "batch_sentences2 = [\"Hi How are you\", \"I am good\"]\n",
    "tokenizer.pad_token= tokenizer.eos_token\n",
    "\n",
    "tokenized_result = tokenizer(batch_sentences2, return_tensors=\"pt\",padding = True,truncation = True)\n",
    "s = tokenized_result[\"input_ids\"][1][-1] == tokenizer.decode(1)\n",
    "if s:\n",
    "    print(\"YES\")\n",
    "\n",
    "print(tokenizer.decode(50256))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "894a36e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n"
     ]
    }
   ],
   "source": [
    "wikitext = load_dataset(\"wikitext\", \"wikitext-103-v1\")\n",
    "prompts = []\n",
    "\n",
    "for text in wikitext[\"train\"][\"text\"][:100]:\n",
    "    if len(text) > 0:\n",
    "        # Use regex to find word boundaries\n",
    "        matches = list(re.finditer(r'\\b\\w+\\b', text))\n",
    "        if len(matches) >= 20:\n",
    "            # Get the end position of the 20th word\n",
    "            end_pos = matches[19].end()\n",
    "            prompt = text[:end_pos]\n",
    "            prompts.append(prompt)\n",
    "        else:\n",
    "            prompts.append(text)\n",
    "# print(prompts)\n",
    "print(len(prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "810ca8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(' park', ' park', -1.8385964632034302), (' park', ' woods', -2.2997779846191406), (' park', ' streets', -3.1739540100097656), (' park', ' dark', -3.468158721923828), (' park', ' door', -3.519336700439453)]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(model.device)\n",
    "\n",
    "prompts = [\"I enjoy walking in the\"]\n",
    "results = generate_token_and_probability(model, tokenizer, prompts, max_length=3, top_k=5)\n",
    "\n",
    "for res in results:\n",
    "    print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce3e18b0-d3ed-4c7d-9599-c002b1159881",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jivesh\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "# model2 = LMHeadModel(model_name)\n",
    "model.eval()  # put model in inference mode\n",
    "\n",
    "# If using GPU (e.g., on Colab), you could also do:\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Function to generate text using different decoders\n",
    "# ---------------------------------------------------------\n",
    "def generate_texts(model, tokenizer, prompt, max_length=40):\n",
    "    \"\"\"Generate text from a prompt using different decoding strategies.\"\"\"\n",
    "    tokenized_result = tokenizer(prompt,return_tensors = \"pt\")\n",
    "    input_ids = tokenized_result[\"input_ids\"].to(model.device)\n",
    "    # If on GPU, uncomment next line:\n",
    "    # input_ids = input_ids.to(device)\n",
    "    attn_mask = tokenized_result['attention_mask'].to(model.device)\n",
    "    \n",
    "    st1 = time()\n",
    "    # -- Greedy decoding\n",
    "    greedy_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        do_sample=False,  # Greedy\n",
    "        attention_mask = attn_mask\n",
    "    )\n",
    "    greedy_text = tokenizer.decode(greedy_ids[0], skip_special_tokens=True)\n",
    "    dur = time() -st1\n",
    "    \n",
    "    st2 = time()\n",
    "    # -- Beam search\n",
    "    beam_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        num_beams=3,    # for example\n",
    "        early_stopping=True,\n",
    "        attention_mask = attn_mask\n",
    "    )\n",
    "    beam_text = tokenizer.decode(beam_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    dur2 = time() - st2\n",
    "\n",
    "    # -- Top-k sampling\n",
    "    topk_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        do_sample=True,\n",
    "        top_k=50,  # for example\n",
    "        attention_mask = attn_mask\n",
    "    )\n",
    "    topk_text = tokenizer.decode(topk_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # -- Nucleus (top-p) sampling\n",
    "    topp_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,  # for example\n",
    "        attention_mask = attn_mask\n",
    "    )\n",
    "    topp_text = tokenizer.decode(topp_ids[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "    st3 = time()\n",
    "    Mydecoder_text,decoder_prob = runViterbiTransformerPipeline(prompt, loop_runner = 7)\n",
    "    dur3 = time() - st3\n",
    "    Mygreedy_text,Mygreedy_prob  =runTransformerPipeline(prompt,loop_runner = 7)\n",
    "\n",
    "    Text_dict = {\n",
    "        \"greedy\": greedy_text,\n",
    "        \"beam\": beam_text,\n",
    "        \"topk\": topk_text,\n",
    "        \"topp\": topp_text,\n",
    "        \"ourDecoder\": Mydecoder_text,\n",
    "        \"ourGreedy\": Mygreedy_text,\n",
    "    }\n",
    "    Time_dict = {\n",
    "     \"Time_greedy\": dur,\n",
    "     \"Time_beam\": dur2,\n",
    "     \"Time_ourDecoder\":dur3\n",
    "    }\n",
    "    return Text_dict,Time_dict\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Function to compute perplexity of a string\n",
    "# ---------------------------------------------------------\n",
    "def compute_perplexity(model, tokenizer, text):\n",
    "    \"\"\"Compute perplexity of `text` under `model`.\"\"\"\n",
    "    encodings = tokenizer(text, return_tensors=\"pt\")\n",
    "    input_ids = encodings.input_ids.to(device)\n",
    "    # If on GPU, uncomment next line:\n",
    "    # input_ids = input_ids.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # The model returns a tuple of (loss, logits, ...)\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        # outputs.loss is the average cross-entropy across tokens\n",
    "        neg_log_likelihood = outputs.loss.item()\n",
    "\n",
    "    perplexity = math.exp(neg_log_likelihood)\n",
    "    return perplexity\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Function to save results to CSV\n",
    "# ---------------------------------------------------------\n",
    "def save_batch_to_csv(results, base_filename, batch_num):\n",
    "    \"\"\"Save a batch of results to CSV.\"\"\"\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(base_filename), exist_ok=True)\n",
    "    \n",
    "    # Create filename with batch number\n",
    "    filename = f\"{base_filename}_batch{batch_num}.csv\"\n",
    "    \n",
    "    # Save to CSV\n",
    "    results_df.to_csv(filename, index=False)\n",
    "    print(f\"Batch {batch_num} saved to {filename}\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Main loop: generate text, then compute perplexities\n",
    "# ---------------------------------------------------------\n",
    "def process_prompts(prompts, batch_size=100):\n",
    "    \"\"\"Process prompts and save results in batches.\"\"\"\n",
    "    all_results = []\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    base_output_file = f\"C:/Users/jivesh/Desktop/SeniorThesis/decoder_comparison_{timestamp}\"\n",
    "    \n",
    "    # Also save a combined file at the end\n",
    "    final_output_file = f\"{base_output_file}_complete.csv\"\n",
    "    \n",
    "    batch_count = 0\n",
    "    \n",
    "    for i, prompt in enumerate(tqdm(prompts)):\n",
    "        # Process a single prompt\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "        length = len(input_ids[0])\n",
    "\n",
    "        # Generate text via different decoding methods\n",
    "        gen_texts, gen_times = generate_texts(model, tokenizer, prompt, max_length=length+7)\n",
    "\n",
    "        # Compute perplexities of the generated texts\n",
    "        results_for_prompt = {\"prompt\": prompt}\n",
    "        for method, text in gen_texts.items():\n",
    "            ppl = compute_perplexity(model, tokenizer, text)\n",
    "            results_for_prompt[f'{method}_text'] = text\n",
    "            results_for_prompt[f'{method}_ppl'] = ppl\n",
    "        for method, time in gen_times.items():\n",
    "            results_for_prompt[f'{method}'] = time\n",
    "\n",
    "        # Store results\n",
    "        all_results.append(results_for_prompt)\n",
    "        \n",
    "        # Save batch if we've reached batch_size or on the last prompt\n",
    "        if (i + 1) % batch_size == 0 or i == len(prompts) - 1:\n",
    "            batch_count += 1\n",
    "            current_batch = all_results[-batch_size:] if len(all_results) >= batch_size else all_results\n",
    "            save_batch_to_csv(current_batch, base_output_file, batch_count)\n",
    "            \n",
    "    # Save all results at the end\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    results_df.to_csv(final_output_file, index=False)\n",
    "    print(f\"All results saved to {final_output_file}\")\n",
    "    \n",
    "    return all_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be752232",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▊      | 29/75 [51:16<1:17:18, 100.84s/it]"
     ]
    }
   ],
   "source": [
    "all_results = process_prompts(prompts,batch_size = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d97ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_results = []\n",
    "\n",
    "# for prompt in tqdm(prompts):\n",
    "\n",
    "#     input_ids = tokenizer(prompt, return_tensors = \"pt\")[\"input_ids\"].to(device)\n",
    "#     length = len(input_ids[0])\n",
    "\n",
    "#     # Generate text via different decoding methods\n",
    "#     gen_texts,gen_times = generate_texts(model, tokenizer, prompt,max_length = length+7)\n",
    "\n",
    "#     # Compute perplexities of the generated texts\n",
    "#     results_for_prompt = {\"prompt\": prompt}\n",
    "#     for method, text in gen_texts.items():\n",
    "#         ppl = compute_perplexity(model, tokenizer, text)\n",
    "#         results_for_prompt[f'{method}_text'] = text\n",
    "#         results_for_prompt[f'{method}_ppl'] = ppl\n",
    "#     for method,time in gen_times.items():\n",
    "#         results_for_prompt[f'{method}'] = time\n",
    "\n",
    "#     # Store results\n",
    "#     all_results.append(results_for_prompt)\n",
    "\n",
    "# # ---------------------------------------------------------\n",
    "# # Printing the Results\n",
    "# # for res in all_results:\n",
    "# #     print(f\"Prompt: {res['prompt']}\")\n",
    "# #     print(f\"  Greedy PPL: {res['greedy_ppl']:.2f}\")\n",
    "# #     print(f\"  Beam   PPL: {res['beam_ppl']:.2f}\")\n",
    "# #     print(f\"  Top-k  PPL: {res['topk_ppl']:.2f}\")\n",
    "# #     print(f\"  Top-p  PPL: {res['topp_ppl']:.2f}\")\n",
    "# #     print(f\" Viterbi PPL: {res['ourDecoder_ppl']:.2f}\")\n",
    "# #     print(f\"OurGreedy  PPL: {res[\"ourGreedy_ppl\"]:.2f}\")\n",
    "# #     print(f\"Greedy answer: {res[\"greedy_text\"]}\")\n",
    "\n",
    "# #     print(f\"ourGreedy answer: {res[\"ourGreedy_text\"]}\")\n",
    "# #     print(f\"ourDecoder answer: {res[\"ourDecoder_text\"]}\")\n",
    "# #     print(f\"Beam answer: {res['beam_text']}\")\n",
    "\n",
    "\n",
    "# # ---------------------------------------------------------\n",
    "# # Save results to CSV\n",
    "# # ---------------------------------------------------------\n",
    "# results_df = pd.DataFrame(all_results)\n",
    "# timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "# output_file = f\"C:/Users/jivesh/Desktop/SeniorThesis/decoder_comparison_{timestamp}.csv\"\n",
    "# results_df.to_csv(output_file, index=False)\n",
    "# print(f\"Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "36bc6714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of times ourDecoder_ppl is greater than or equal to beam_ppl  22\n",
      "Number of times ourDecoder_ppl is greater than or equal to greedy_ppl  23\n",
      "Number of times ourDecoder_ppl is greater than to beam_ppl  17\n",
      "Number of times ourDecoder_ppl is greater than to greedy_ppl  20\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(output_file)  \n",
    "\n",
    "# Count occurrences where ourDecoder_ppl > beam_ppl and ourDecoder_ppl > greedy_ppl\n",
    "count_beam = ((df[\"ourDecoder_ppl\"] <= df[\"beam_ppl\"])).sum()  \n",
    "count_greedy = (df[\"ourDecoder_ppl\"] <= df[\"greedy_ppl\"]).sum()\n",
    "\n",
    "print(\"Number of times ourDecoder_ppl is greater than or equal to beam_ppl \", count_beam)\n",
    "print(\"Number of times ourDecoder_ppl is greater than or equal to greedy_ppl \", count_greedy)\n",
    "\n",
    "count_beam = ((df[\"ourDecoder_ppl\"] < df[\"beam_ppl\"])).sum()  \n",
    "count_greedy = (df[\"ourDecoder_ppl\"] < df[\"greedy_ppl\"]).sum()\n",
    "\n",
    "print(\"Number of times ourDecoder_ppl is greater than to beam_ppl \", count_beam)\n",
    "print(\"Number of times ourDecoder_ppl is greater than to greedy_ppl \", count_greedy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d7601372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.413704459110896\n"
     ]
    }
   ],
   "source": [
    "perplexity = compute_perplexity(model,tokenizer,beam_text)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "07c0f804-76b8-4d46-8468-b63c16518a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Valkyria', 'Chronicles', 'III']\n",
      "9\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "prompt = prompts[0]\n",
    "result = re.findall(r'\\w+|[.,!?;'']', prompt)\n",
    "print(result)\n",
    "input_ids = tokenizer(prompt, return_tensors = \"pt\")[\"input_ids\"]\n",
    "print(len(input_ids[0]))\n",
    "print(len(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8b7ff021-5f40-40c1-8ad3-a40bd7313f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import itertools\n",
    "\n",
    "class SearchTree:\n",
    "    def __init__(self, context, probability, parent=None, parent_index=None):\n",
    "        self.context = context\n",
    "        self.probability = probability\n",
    "        self.parent = parent\n",
    "        self.parent_index = parent_index\n",
    "        self.children = []\n",
    "    \n",
    "    def create_child(self):\n",
    "        return self\n",
    "    \n",
    "    def build_Context(self):\n",
    "        if self.parent is None:\n",
    "            return self.context\n",
    "        else:\n",
    "            return self.parent.build_Context() + self.context\n",
    "    \n",
    "    def calcProbTillNow(self):\n",
    "        if self.parent is None:\n",
    "            return self.probability\n",
    "        else:\n",
    "            return self.probability * self.parent.calcProbTillNow()\n",
    "    \n",
    "    def replace_parent(self, new_parent):\n",
    "        self.parent = new_parent\n",
    "        if new_parent is not None:\n",
    "            new_parent.children.append(self)\n",
    "    \n",
    "    def assign_parent_index(self, index):\n",
    "        self.parent_index = index\n",
    "\n",
    "def findProbability(prevToken, newTokens, model_tokenizer_tuple):\n",
    "    model, tokenizer = model_tokenizer_tuple\n",
    "    \n",
    "    probs = []\n",
    "    batch_sentences = [prevToken.build_Context() + newToken.context for newToken in newTokens]\n",
    "    \n",
    "    # Tokenize inputs\n",
    "    inputs = tokenizer(batch_sentences, return_tensors=\"pt\", padding=True)\n",
    "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(model.device)\n",
    "    \n",
    "    # Pass through the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    # Get logits for the last token in each sequence\n",
    "    last_token_indices = attention_mask.sum(dim=1) - 1\n",
    "    batch_size = input_ids.shape[0]\n",
    "    logits = outputs.logits[torch.arange(batch_size), last_token_indices]\n",
    "    \n",
    "    # Compute probabilities using softmax\n",
    "    probs_tensor = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    \n",
    "    # Get the probability of the specific token that was generated\n",
    "    for i, newToken in enumerate(newTokens):\n",
    "        token_id = tokenizer.encode(newToken.context, add_special_tokens=False)[-1]\n",
    "        if token_id < probs_tensor.shape[1]:\n",
    "            probs.append(probs_tensor[i, token_id].item())\n",
    "        else:\n",
    "            probs.append(0.0)  # Fallback if token ID is out of range\n",
    "    \n",
    "    return probs\n",
    "\n",
    "def generateIntermediates(root, numTokens=3, loop_runner=4):\n",
    "    # Initialize transformers model and tokenizer directly\n",
    "    model_name = \"gpt2\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    model.eval()\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Ensure padding token is set\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.padding_side = \"right\"\n",
    "    \n",
    "    # Model and tokenizer tuple for probability calculations\n",
    "    model_tokenizer = (model, tokenizer)\n",
    "    \n",
    "    sentence = SearchTree(root, 1)\n",
    "    context = []\n",
    "    prob_list = []\n",
    "    num_tokens = numTokens\n",
    "    content = []\n",
    "    probability = []\n",
    "    children = []\n",
    "    overlap = []\n",
    "    most_common = []\n",
    "    unique_tokens = set()\n",
    "    probabilityMatrix = []\n",
    "    uniqueTokensList = []\n",
    "    new_content = []\n",
    "    uniqueTokenLength = []\n",
    "    \n",
    "    flops_counter = {}\n",
    "    batch_size = 75\n",
    "    holdout_number = 15\n",
    "    \n",
    "    # Get initial predictions\n",
    "    input_ids = tokenizer(sentence.context, return_tensors=\"pt\").input_ids.to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "    \n",
    "    # Get logits for the last token\n",
    "    logits = outputs.logits[:, -1, :]\n",
    "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    top_probs, top_indices = torch.topk(probs, k=numTokens+3)\n",
    "    \n",
    "    # Process initial tokens\n",
    "    for i in range(num_tokens):\n",
    "        token_id = top_indices[0, i].item()\n",
    "        token_text = tokenizer.decode([token_id]).strip()\n",
    "        token_prob = top_probs[0, i].item()\n",
    "        \n",
    "        unique_tokens.add(token_text)\n",
    "        new_content.append(token_text)\n",
    "        probability.append(token_prob)\n",
    "        \n",
    "        context = SearchTree(token_text, token_prob, sentence, parent_index=0)\n",
    "        context.create_child()\n",
    "        uniqueTokensList.append(context)\n",
    "        children.append(context)\n",
    "    \n",
    "    content.append(new_content)\n",
    "    previousUniqueLength = num_tokens\n",
    "    initialStateProbability = probability\n",
    "    uniqueTokenLength.append(num_tokens)\n",
    "    \n",
    "    # Main loop for building the trellis\n",
    "    for i in range(2, loop_runner):\n",
    "        unique_tokens = set()\n",
    "        probability = []\n",
    "        new_content = []\n",
    "        previousSetLength = 0\n",
    "        \n",
    "        # Prepare batch sentences\n",
    "        batch_sentences = [child.build_Context() for child in uniqueTokensList]\n",
    "        total_predictions = []\n",
    "        \n",
    "        # Process in batches\n",
    "        if len(batch_sentences) > holdout_number:\n",
    "            # First batch\n",
    "            first_batch = batch_sentences[0:-holdout_number]\n",
    "            first_batch_predictions = get_top_k_predictions(first_batch, model, tokenizer, numTokens+2)\n",
    "            total_predictions.extend(first_batch_predictions)\n",
    "            \n",
    "            # Second batch\n",
    "            second_batch = batch_sentences[-holdout_number:]\n",
    "            second_batch_predictions = get_top_k_predictions(second_batch, model, tokenizer, numTokens+2)\n",
    "            total_predictions.extend(second_batch_predictions)\n",
    "        else:\n",
    "            total_predictions = get_top_k_predictions(batch_sentences, model, tokenizer, numTokens+2)\n",
    "        \n",
    "        # Process predictions\n",
    "        for j in range(len(uniqueTokensList)):\n",
    "            for s in range(num_tokens):\n",
    "                if s < len(total_predictions[j]):\n",
    "                    context_text = total_predictions[j][s][0]\n",
    "                    prob = total_predictions[j][s][1]\n",
    "                    \n",
    "                    unique_tokens.add(context_text)\n",
    "                    context = SearchTree(context_text, prob, uniqueTokensList[j])\n",
    "                    \n",
    "                    if len(unique_tokens) > previousSetLength:\n",
    "                        previousSetLength = len(unique_tokens)\n",
    "                        uniqueTokensList.append(context)\n",
    "                        new_content.append(context.context)\n",
    "        \n",
    "        # Store content\n",
    "        content.append(new_content)\n",
    "        \n",
    "        # Calculate combined probabilities\n",
    "        comb_prob = []\n",
    "        for prevToken in uniqueTokensList[:previousUniqueLength]:\n",
    "            comb_prob.append(findProbability(prevToken, uniqueTokensList[previousUniqueLength:], model_tokenizer))\n",
    "        comb_prob = list(itertools.chain(*comb_prob))  # flattening the list\n",
    "        \n",
    "        # Update parent relationships\n",
    "        for tokenumber, newToken in enumerate(uniqueTokensList[previousUniqueLength:]):\n",
    "            probs = [comb_prob[a*len(uniqueTokensList[previousUniqueLength:]) + tokenumber] for a in range(len(uniqueTokensList[:previousUniqueLength]))]\n",
    "            probs2 = [probs[i]*uniqueTokensList[:previousUniqueLength][i].calcProbTillNow() for i in range(len(probs))]\n",
    "            \n",
    "            if not probs2:\n",
    "                continue\n",
    "            else:\n",
    "                max_value = max(probs2)\n",
    "                max_index = probs2.index(max_value)\n",
    "                newToken.replace_parent(uniqueTokensList[:previousUniqueLength][max_index])\n",
    "                newToken.assign_parent_index(max_index)\n",
    "            \n",
    "            probability.append(probs)\n",
    "        \n",
    "        probabilityMatrix.append(probability)\n",
    "        flops_counter[i-1] = i  # Just a placeholder since we're not tracking actual FLOPS\n",
    "        \n",
    "        uniqueTokenLength.append(len(uniqueTokensList[previousUniqueLength:]))\n",
    "        previousUniqueLength = len(uniqueTokensList[previousUniqueLength:])\n",
    "        uniqueTokensList = uniqueTokensList[len(uniqueTokensList)-previousUniqueLength:]\n",
    "    \n",
    "    return probabilityMatrix, initialStateProbability, content, uniqueTokenLength, flops_counter\n",
    "\n",
    "def get_top_k_predictions(sentences, model, tokenizer, top_k=100):\n",
    "    \"\"\"\n",
    "    Get top-k token predictions for each sentence.\n",
    "    Returns a list of lists of (token, probability) tuples.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Tokenize\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "        input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "        \n",
    "        # Get model output\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "        \n",
    "        # Get predictions for the last token\n",
    "        logits = outputs.logits[:, -1, :]\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        top_probs, top_indices = torch.topk(probs, k=top_k)\n",
    "        \n",
    "        # Decode tokens and pair with probabilities\n",
    "        sentence_predictions = []\n",
    "        for i in range(min(top_k, top_indices.shape[1])):\n",
    "            token_id = top_indices[0, i].item()\n",
    "            token = tokenizer.decode([token_id]).strip()\n",
    "            if token and token != \"\\n\":\n",
    "                sentence_predictions.append((token, top_probs[0, i].item()))\n",
    "        \n",
    "        predictions.append(sentence_predictions)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63a5e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def generate_with_probabilities(text, max_new_tokens=50, top_k=10):\n",
    "    # Load model and tokenizer\n",
    "    model_name = \"gpt2\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    model.eval()\n",
    "    \n",
    "    # Make sure padding token is set\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Tokenize input text\n",
    "    input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "    \n",
    "    # Track generated tokens and their probabilities\n",
    "    generated_tokens = []\n",
    "    token_probabilities = []\n",
    "    \n",
    "    # Generate tokens one by one\n",
    "    for _ in range(max_new_tokens):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "            \n",
    "        # Get logits for the last token\n",
    "        next_token_logits = outputs.logits[:, -1, :]\n",
    "        \n",
    "        # Apply softmax to get probabilities\n",
    "        next_token_probs = torch.nn.functional.softmax(next_token_logits, dim=-1)\n",
    "        \n",
    "        # Get top k tokens and their probabilities\n",
    "        topk_probs, topk_indices = torch.topk(next_token_probs, top_k)\n",
    "        \n",
    "        # Select the token with highest probability\n",
    "        next_token = topk_indices[0, 0].unsqueeze(0).unsqueeze(0)\n",
    "        next_token_prob = topk_probs[0, 0].item()\n",
    "        \n",
    "        # Append to results\n",
    "        generated_tokens.append(tokenizer.decode(next_token[0]))\n",
    "        token_probabilities.append(next_token_prob)\n",
    "        \n",
    "        # Update input_ids for next iteration\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "    \n",
    "    return generated_tokens, token_probabilities\n",
    "\n",
    "# Example usage\n",
    "text = \"I enjoy walking in the park\"\n",
    "tokens, probs = generate_with_probabilities(text)\n",
    "\n",
    "# Print results\n",
    "for token, prob in zip(tokens, probs):\n",
    "    print(f\"Token: {token}, Probability: {prob:.4f}\")\n",
    "\n",
    "# Get the full generated text\n",
    "full_text = text + \"\".join(tokens)\n",
    "print(f\"\\nFull text: {full_text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
