{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "2307ad42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------------------------------------\n",
    "# Setup\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "import math\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import itertools\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "import math\n",
    "\n",
    "\n",
    "class LMHeadModel:\n",
    "    def __init__(self, model_name, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "        # Initialize the model and tokenizer\n",
    "        self.device = device\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float32).to(self.device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model.eval()\n",
    "\n",
    "        # Ensure the tokenizer has a padding token\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token  # Use EOS token as padding\n",
    "            self.tokenizer.padding_side = \"right\"\n",
    "\n",
    "        self.batch_prediction_count = 0\n",
    "\n",
    "\n",
    "    def batch_encode(self, sentences):\n",
    "        \"\"\"\n",
    "        Encodes a batch of sentences into input tensors.\n",
    "        Args:\n",
    "            sentences (list of str): The input sentences to encode.\n",
    "        Returns:\n",
    "            inputs (dict): A dictionary of tokenized inputs ready for the model.\n",
    "        \"\"\"\n",
    "        return self.tokenizer(\n",
    "            sentences,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,  # Pad to the longest sequence in the batch\n",
    "            truncation=True,  # Truncate sequences longer than the model's max length\n",
    "        ).to(self.device)\n",
    "\n",
    "    def batch_decode(self, token_ids):\n",
    "        \"\"\"\n",
    "        Decodes a batch of token IDs back to sentences.\n",
    "        Args:\n",
    "            token_ids (torch.Tensor): A tensor of token IDs to decode.\n",
    "        Returns:\n",
    "            decoded_sentences (list of str): The decoded sentences.\n",
    "        \"\"\"\n",
    "        return self.tokenizer.batch_decode(token_ids, skip_special_tokens=True)\n",
    "    def batch_decode_top_k(self, token_ids_batch, tokenizer):\n",
    "        \"\"\"\n",
    "        Decodes token IDs to meaningful text while merging subword tokens.\n",
    "        Args:\n",
    "            token_ids_batch (torch.Tensor): A batch of token IDs (e.g., from `topk`).\n",
    "            tokenizer: The tokenizer used for encoding/decoding.\n",
    "        Returns:\n",
    "            list of list of str: Decoded tokens (words/subwords) for each sequence in the batch.\n",
    "        \"\"\"\n",
    "        decoded_tokens = []\n",
    "        for token_ids in token_ids_batch:\n",
    "            # Decode each token ID in the batch, joining subwords correctly\n",
    "            tokens = [tokenizer.decode([token_id]).strip() for token_id in token_ids]\n",
    "            decoded_tokens.append(tokens)\n",
    "        return decoded_tokens\n",
    "\n",
    "    def get_batch_predictions(self, sentences, top_k=100):\n",
    "        \"\"\"\n",
    "        Predicts the next tokens for a batch of input sentences.\n",
    "        Args:\n",
    "            sentences (list of str): The input sentences.\n",
    "            top_k (int): Number of top tokens to return for each sentence.\n",
    "        Returns:\n",
    "            predictions (list of list of tuples): Top-k token predictions for each sentence.\n",
    "        \"\"\"\n",
    "        #Increment to see how many times this function is called after a given layer of trellis.\n",
    "        self.batch_prediction_count += 1\n",
    "\n",
    "\n",
    "        # Tokenize inputs\n",
    "        inputs = self.batch_encode(sentences)\n",
    "\n",
    "        # Pass through the model\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs,use_cache = False)\n",
    "\n",
    "        # Get logits for the last token in each sequence\n",
    "        logits = outputs.logits[:, -1, :]  # Shape: (batch_size, vocab_size)\n",
    "\n",
    "\n",
    "        # Compute probabilities using softmax\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        top_probs, top_token_ids = torch.topk(probs, k=top_k, dim=-1)\n",
    "        top_tokens = self.batch_decode_top_k(top_token_ids, self.tokenizer)\n",
    "\n",
    "\n",
    "        predictions = [\n",
    "            [(token, prob.item()) for token, prob in zip(top_tokens[i], top_probs[i]) if token and token != \"\\n\"]\n",
    "            for i in range(len(sentences))\n",
    "        ]\n",
    "        return predictions\n",
    "\n",
    "    def get_batch_prediction_count(self):\n",
    "        \"\"\"\n",
    "        Returns the number of times batch predictions have been made.\n",
    "        \"\"\"\n",
    "        return self.batch_prediction_count\n",
    "\n",
    "    def reset_batch_prediction_count(self):\n",
    "        \"\"\" Resets the count\n",
    "        \"\"\"\n",
    "\n",
    "        self.batch_prediction_count = 0\n",
    "class SearchTree:\n",
    "    def __init__(self,context,probability,token_id,model,tokenizer,parent = None,child = None,parent_index = None):\n",
    "        self.token_id = token_id\n",
    "        context = context.strip()\n",
    "        self.context = context\n",
    "        self.probability = probability\n",
    "        self.parent = parent\n",
    "        self.child = []\n",
    "        self.parent_index = parent_index  # newly created.\n",
    "        if child is not None:\n",
    "           self.child.append(child)\n",
    "        \n",
    "        # Cache cumulative probability at node creation\n",
    "        if parent:\n",
    "            self.cached_prob = parent.calcProbTillNow()+probability #parent.calcProbTillNow() * probability\n",
    "        else:\n",
    "            self.cached_prob = probability\n",
    "\n",
    "    def build_Context(self):\n",
    "        context_list = []\n",
    "        full_context = []\n",
    "        node = self\n",
    "        while node.parent is not None:\n",
    "            context_list.extend([node.token_id])\n",
    "            node = node.parent\n",
    "        context_list.reverse()\n",
    "        full_context.extend(node.token_id)\n",
    "        full_context.extend(context_list)\n",
    "        \n",
    "        # formatted_contextList = []\n",
    "        # for i in range(len(context_list)):\n",
    "        #     if context_list[i] in ['.',':',',','?','!',';'] or (\"'\" in context_list[i]):\n",
    "        #         if (i-1>= 0):\n",
    "        #             if context_list[i-1] not in  ['.',':',',','?','!',';'] and (\"'\" not in context_list[i-1]):#if two consecutive contexts are , ' etc.\n",
    "        #                 word = context_list[i-1]+context_list[i]\n",
    "\n",
    "        #                 formatted_contextList.remove(context_list[i-1])\n",
    "        #                 formatted_contextList.append(word)\n",
    "        #             else:\n",
    "        #                 formatted_contextList.append(context_list[i])\n",
    "        #     else:\n",
    "\n",
    "        #         formatted_contextList.append(context_list[i])\n",
    "        # return ' '.join(formatted_contextList)\n",
    "        generated_sentence = tokenizer.decode(full_context, skip_special_tokens=True)\n",
    "        return generated_sentence\n",
    "\n",
    "\n",
    "    def create_child(self):\n",
    "        if self.parent is not None:\n",
    "           self.parent.child.append(self)\n",
    "\n",
    "    def replace_parent(self, new_parent):\n",
    "        \"\"\"Assign a new parent and update cached probability.\"\"\"\n",
    "        self.parent = new_parent\n",
    "        self.cached_prob = new_parent.calcProbTillNow() + self.probability\n",
    "    \n",
    "\n",
    "    def calcProbTillNow(self):\n",
    "        \"\"\"Return cached cumulative probability to avoid redundant calculations.\"\"\"\n",
    "        return self.cached_prob\n",
    "    \n",
    "    def change_probability(self,new_probability):\n",
    "        self.cached_prob = self.cached_prob - self.probability\n",
    "        self.probability = new_probability\n",
    "        self.cached_prob += self.probability\n",
    "\n",
    "    # def calcProbTillNow(self):\n",
    "    #   prob = self.probability\n",
    "    #   node = self\n",
    "    #   while node.parent is not None:\n",
    "    #     prob = prob*node.parent.probability\n",
    "    #     node = node.parent\n",
    "    #   return prob    #can make this negative log probability.\n",
    "\n",
    "    def assign_parent_index(self,parent_index):\n",
    "      self.parent_index = parent_index\n",
    "\n",
    "\n",
    "def findProbability(InitialToken, FinalTokens, model,tokenizer):\n",
    "    context = InitialToken.build_Context()\n",
    "    # tokens_50K = model.get_batch_predictions([context], 500)\n",
    "    tokens_50K = generate_token_and_probability(model, tokenizer, [context], top_k=500)\n",
    "    token_dict = {}  # Dictionary to store only the first occurrence of each token\n",
    "\n",
    "    for _,token_id,token, prob in tokens_50K[0]:\n",
    "        # token = token.strip()\n",
    "        if token_id.item() not in token_dict or prob>token_dict[token_id.item()]:  # Store only the first occurrence\n",
    "            token_dict[token_id.item()] = prob\n",
    "    return [token_dict.get(FinalToken.token_id, -math.inf) for FinalToken in FinalTokens]  # Return probability if found, else 0\n",
    "\n",
    "\n",
    "def VITERBI_Lists(state_transition_probmat, initial_state_prob):\n",
    "\n",
    "    viterbi_mat = []\n",
    "    backpointer = []\n",
    "    viterbi_1stLayer = []\n",
    "    for i in range(len(initial_state_prob)):\n",
    "        viterbi_1stLayer.append(float(initial_state_prob[i]))\n",
    "    viterbi_mat.append(viterbi_1stLayer)\n",
    "\n",
    "    for time_step in range(len(state_transition_probmat)):\n",
    "        viterbi_layer = []\n",
    "        backpointer_layer = []\n",
    "        for state in range(len(state_transition_probmat[time_step])):\n",
    "            iteration_vec = [viterbi_mat[time_step][i]+state_transition_probmat[time_step][state][i] for i in range(len(viterbi_mat[time_step]))]\n",
    "\n",
    "            maxval = max(iteration_vec)\n",
    "            maxind = iteration_vec.index(maxval)\n",
    "            viterbi_layer.append(maxval)\n",
    "            backpointer_layer.append(maxind)\n",
    "\n",
    "        viterbi_mat.append(viterbi_layer)\n",
    "        backpointer.append(backpointer_layer)\n",
    "\n",
    "    best_path_prob = max(viterbi_mat[-1])\n",
    "    # max_index = max(range(len(viterbi_mat[-1])), key = lambda i: viterbi_mat[-1][i])\n",
    "    max_index = viterbi_mat[-1].index(best_path_prob)\n",
    "    best_backpointer = max_index\n",
    "    best_path = [best_backpointer]\n",
    "    j = 0\n",
    "    for i in reversed(range(len(state_transition_probmat))):\n",
    "        best_path.append(backpointer[i][best_path[j]])\n",
    "        j += 1\n",
    "    best_path = best_path[::-1]\n",
    "    return best_path, viterbi_mat,best_path_prob\n",
    "def decodePath(best_path,unique_tokens_list,root_string,tokenizer):\n",
    "    resultant_token_ids = []\n",
    "    root_ids = tokenizer.encode(root_string)\n",
    "    resultant_token_ids.extend(root_ids)\n",
    "    for i in range(len(best_path)):\n",
    "      token_id_to_add=(unique_tokens_list[i][best_path[i]]).token_id\n",
    "      resultant_token_ids.append(token_id_to_add)\n",
    "    \n",
    "    # generated_sentence = tokenizer.decode(resultant_token_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    resultant_token_ids = torch.tensor([resultant_token_ids])\n",
    "    generated_sentence = tokenizer.decode(resultant_token_ids[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "    return generated_sentence\n",
    "\n",
    "def generate_token_and_probability(model, tokenizer, batch_prompts,max_length=1, top_k=4):\n",
    "    tokenizer.pad_token= tokenizer.eos_token\n",
    "    tokenized_result = tokenizer(batch_prompts, return_tensors=\"pt\",padding = True,truncation = True)\n",
    "    input_ids = tokenized_result[\"input_ids\"].to(model.device)\n",
    "    # print(\"input_ids from the function: \", input_ids)\n",
    "    # for i in range(len(input_ids)):\n",
    "    #     if input_ids[i][-1] == 50256 and probabilityMatrix is not None: #50256 is end of text token or the pad token \n",
    "    #         newToken = SearchTree(model.decode(input_ids[i][-2]),0,input_ids[i][-2].item(),model,tokenizer,parent = uniqueTokensList[i])\n",
    "    #         prob = findProbability(uniqueTokensList[i].parent,newToken,model,tokenizer)\n",
    "    #         newToken.change_probability(prob)\n",
    "    #         probabilityMatrix[i] = prob #probabilityMatrix of one previous iteration should be changed to 0\n",
    "    #         uniqueTokensList[i] = newToken\n",
    "\n",
    "    attn_mask = tokenized_result[\"attention_mask\"].to(model.device)\n",
    "    outputs = model.generate( \n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attn_mask,\n",
    "        max_length=input_ids.size(-1) + max_length,\n",
    "        do_sample=False,  # Greedy decoding\n",
    "        output_scores=True, \n",
    "        return_dict_in_generate=True\n",
    "    )\n",
    "\n",
    "    sequences, scores = outputs.sequences, outputs.scores  # scores will have only one element per batch\n",
    "    predictions = []\n",
    "    # print(\"generated_token_id\",generated_token_id)\n",
    "    for i in range(len(batch_prompts)):\n",
    "        generated_token_id = sequences[i][input_ids.size(-1):].tolist()[0]  # Extract generated token ID\n",
    "        generated_token = tokenizer.decode(generated_token_id, skip_special_tokens=True)\n",
    "       \n",
    "\n",
    "        # Log probabilities of all possible tokens at the generated step\n",
    "        log_probs = torch.nn.functional.log_softmax(scores[0][i], dim=-1)  # scores[0] corresponds to the single generation step\n",
    "        # Keep increasing top_k until we have enough valid tokens\n",
    "        valid_predictions = []\n",
    "        curr_top_k = top_k\n",
    "        while len(valid_predictions) < top_k:\n",
    "            topk_logprobs, topk_ids = log_probs.topk(curr_top_k)  # Get top-k log probabilities\n",
    "            topk_tokens = tokenizer.batch_decode(topk_ids, skip_special_tokens=True)\n",
    "\n",
    "            # Filter out token `198` if the last token in input_ids was `198`\n",
    "            if input_ids[i][-1] == 198:\n",
    "                valid_predictions = [\n",
    "                    (generated_token,tid, tok, lp.item()) for tid, tok, lp in zip(topk_ids, topk_tokens, topk_logprobs) if tid != 198\n",
    "                ]\n",
    "            else:\n",
    "                valid_predictions = [\n",
    "                    (generated_token,tid, tok, lp.item()) for tid, tok, lp in zip(topk_ids, topk_tokens, topk_logprobs)\n",
    "                ]\n",
    "\n",
    "            # If filtering removed tokens, increase `curr_top_k` to get more candidates\n",
    "            if len(valid_predictions) < top_k:\n",
    "                curr_top_k += 1  # Expand search to get more tokens\n",
    "\n",
    "        # Store only the required `top_k` valid predictions\n",
    "        predictions.append(valid_predictions[:top_k])\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "\n",
    "def check_bad_predictions(text):\n",
    "    bad_patterns = [r'={2,}', r'!{2,}', r'\\?{2,}', r',{2,}', r';{2,}', r'\\|{2,}', r'~{2,}', r'&{2,}', r'-{2,}']\n",
    "    \n",
    "    # Check for unwanted punctuation patterns (two or more consecutive occurrences)\n",
    "    for pattern in bad_patterns:\n",
    "        if re.search(pattern, text):\n",
    "            return True\n",
    "    \n",
    "    # Check for non-ASCII characters\n",
    "    if any(ord(char) > 127 for char in text):  # ASCII characters are in the range 0-127\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def generateIntermediates(root,model,tokenizer,numTokens = 3, loop_runner = 4): \n",
    "  root_token_id = tokenizer.encode(root)\n",
    "  sentence = SearchTree(root,0,token_id =root_token_id,model = model, tokenizer = tokenizer)\n",
    "  context = []\n",
    "  prob_list = []\n",
    "  num_tokens = numTokens\n",
    "  content = []\n",
    "  probability = []\n",
    "  \n",
    "  tokens_50K = generate_token_and_probability(model, tokenizer, [root], top_k=numTokens) #arbitrarly setting +1\n",
    "  children = []\n",
    "  overlap = []\n",
    "  most_common = []\n",
    "  #unique_elements = []   # to store unique elements at each iteration\n",
    "  unique_tokens = set()\n",
    "  probabilityMatrix = []\n",
    "  uniqueTokensList = []\n",
    "  new_content = []\n",
    "  uniqueTokenLength = []\n",
    "\n",
    "  flops_counter = {}\n",
    "  cached_probs = {}\n",
    "  batch_size = 5\n",
    "  holdout_number = 15\n",
    "  for i in range(num_tokens):\n",
    "    _,token_id,context,prob = tokens_50K[0][i]  # Assuming it's structured as a tuple (best_token, token, probability)\n",
    "    # context = context.strip()  #This is not the correct solution. I am doing this rather than only leaving one strip command in search tree because I am appending to unique tokens before I am assigning this to search tree. \n",
    "    # context2 = context.strip()\n",
    "    # bad_prediction_checker = check_bad_predictions(context2)\n",
    "    print(\"initial loop\")\n",
    "    print(tokens_50K[0][i])\n",
    "    unique_tokens.add(context)\n",
    "    probability.append(prob)  \n",
    "    context = SearchTree(context,prob,token_id = token_id.item(),model = model,tokenizer = tokenizer,parent =sentence,parent_index = 0)\n",
    "    new_content.append(context)\n",
    "    context.create_child()\n",
    "    uniqueTokensList.append(context)\n",
    "    children.append(context)\n",
    "\n",
    "  content.append(new_content)\n",
    "  previousUniqueLength = num_tokens\n",
    "  #unique_elements.append(unique_tokens)\n",
    "  initialStateProbability = probability \n",
    "  uniqueTokenLength.append(num_tokens)\n",
    "  for i in range(2,loop_runner):\n",
    "    unique_tokens = set()\n",
    "    probability = []\n",
    "    new_content = []\n",
    "    total_predictions = []\n",
    "    previousSetLength = 0\n",
    "    batch_sentences = [child.build_Context() for child in uniqueTokensList]\n",
    "    # if ( i == loop_runner-1 or i == loop_runner-2):\n",
    "    #     print(batch_sentences)\n",
    "\n",
    "    if len(batch_sentences)>batch_size:\n",
    "        total_predictions = []\n",
    "        start_index = 0\n",
    "        num_sentences_left = len(batch_sentences)\n",
    "        while (num_sentences_left>batch_size):\n",
    "            batch_sentences2 = batch_sentences[start_index*batch_size:(start_index+1)*batch_size]\n",
    "            batch_predictions =  generate_token_and_probability(model, tokenizer, batch_sentences2, top_k=numTokens)\n",
    "            total_predictions.extend(batch_predictions)\n",
    "            start_index +=1\n",
    "            num_sentences_left -= batch_size\n",
    "        if num_sentences_left > 0:  \n",
    "           batch_sentences2 = batch_sentences[start_index*batch_size :]\n",
    "           batch_predictions =  generate_token_and_probability(model, tokenizer, batch_sentences2, top_k=numTokens)\n",
    "           total_predictions.extend(batch_predictions)\n",
    "\n",
    "\n",
    "        # batch_sentences2 = batch_sentences[0:-holdout_number]\n",
    "        # batch_predictions1 = generate_token_and_probability(model, tokenizer, batch_sentences[-holdout_number:],top_k=numTokens)\n",
    "    else:\n",
    "        total_predictions = generate_token_and_probability(model, tokenizer, batch_sentences,top_k=numTokens)\n",
    "\n",
    "    for j in range(len(uniqueTokensList)):\n",
    "      for s in range(num_tokens):\n",
    "        # if (i == loop_runner-1 and j<10):\n",
    "        #     print(\"s: \", s)\n",
    "        #     print(total_predictions[j][s])\n",
    "        _,token_id,context,prob = total_predictions[j][s]\n",
    "        context2 = context.strip()\n",
    "        #bad_predictions_checker = check_bad_predictions(context2)\n",
    "        # if context2:\n",
    "        unique_tokens.add(context)   # also this if condition is not the correct solution\n",
    "        context = SearchTree(context,prob,token_id = token_id.item(),model = model,tokenizer = tokenizer,parent = uniqueTokensList[j])   #probably redundant: Because I should only create SearchTree of unique tokens\n",
    "        # context.create_child() Removed this 2/19/2025\n",
    "        if (len(unique_tokens)>previousSetLength):\n",
    "          previousSetLength = len(unique_tokens)\n",
    "          uniqueTokensList.append(context)\n",
    "          new_content.append(context)\n",
    "    \n",
    "    # if (i == loop_runner-1):\n",
    "    #     for token in range(len(uniqueTokensList)):\n",
    "    #        print(uniqueTokensList[token].context)\n",
    "    #unique_elements.append(unique_tokens) # append the unique tokens list at each iteration to unique_elements list\n",
    "    content.append(new_content) # for storing tokens which will pass to the decode_path function.\n",
    "\n",
    "   \n",
    "    comb_prob = []\n",
    "    for prevToken in uniqueTokensList[:previousUniqueLength]:\n",
    "      comb_prob.append(findProbability(prevToken,uniqueTokensList[previousUniqueLength:], model,tokenizer))\n",
    "    comb_prob = list(itertools.chain(*comb_prob)) # flattening the list\n",
    "\n",
    "    for tokenumber,newToken in enumerate(uniqueTokensList[previousUniqueLength:]):\n",
    "      probs = [comb_prob[a*len(uniqueTokensList[previousUniqueLength:]) + tokenumber] for a in range(len(uniqueTokensList[:previousUniqueLength]))]\n",
    "      probs2 = [probs[i] + uniqueTokensList[:previousUniqueLength][i].calcProbTillNow() for i in range(len(probs))]\n",
    "      #   print(\"parent_prob Up Till now: \",[uniqueTokensList[:previousUniqueLength][i].calcProbTillNow() for i in range(len(probs))])\n",
    "      #   print(\"combined probs: \", probs2)\n",
    "      #   print(\"actual_probs: \", [math.exp(probs2[i]) for i in range(len(probs2))])\n",
    "      if not probs2:\n",
    "        continue\n",
    "      else:\n",
    "        max_value = max(probs2)\n",
    "        max_index = probs2.index(max_value)\n",
    "        newToken.replace_parent(uniqueTokensList[:previousUniqueLength][max_index])\n",
    "        newToken.change_probability(max_value) # just added this 3/27/2025\n",
    "        newToken.assign_parent_index(max_index)\n",
    "      probability.append(probs)\n",
    "    probabilityMatrix.append(probability)\n",
    "    # flops_counter[i-1] = model.get_batch_prediction_count()\n",
    "    #model.reset_batch_prediction_count()\n",
    "\n",
    "\n",
    "    uniqueTokenLength.append(len(uniqueTokensList[previousUniqueLength:]))\n",
    "\n",
    "    previousUniqueLength = len(uniqueTokensList[previousUniqueLength:])\n",
    "    uniqueTokensList = uniqueTokensList[len(uniqueTokensList)-previousUniqueLength:]\n",
    "\n",
    "  return probabilityMatrix, initialStateProbability, content,uniqueTokenLength #, flops_counter\n",
    "def runViterbiTransformerPipeline(rootSentence, numTokens = 3, loop_runner=3):\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "    probabilityMatrix,initialStateProbability,content,uniqueTokenLength = generateIntermediates(rootSentence,model,tokenizer,numTokens = numTokens,loop_runner =loop_runner+1)\n",
    "    best_path,viterbi_mat,best_path_prob = VITERBI_Lists(probabilityMatrix, initialStateProbability)\n",
    "    print(\"uniqueTokenLength: \", uniqueTokenLength)\n",
    "    print(\"best_path: \", best_path)\n",
    "    decodedString = decodePath(best_path,content,rootSentence,tokenizer)\n",
    "    return decodedString,best_path_prob\n",
    "def runTransformerPipeline(rootSentence,loop_runner = 3):\n",
    "  model = LMHeadModel(\"gpt2\")\n",
    "  prob = 1\n",
    "  finalSentence = rootSentence\n",
    "  for i in range(loop_runner):\n",
    "    tokens_50K = model.get_batch_predictions([finalSentence])\n",
    "\n",
    "    context = tokens_50K[0][0][0]\n",
    "    prob =  prob*tokens_50K[0][0][1]\n",
    "    if context in ['.',':',',','?','!',';'] or \"'\" in context:\n",
    "      finalSentence += context\n",
    "\n",
    "    else:\n",
    "      finalSentence = finalSentence + ' ' + context\n",
    "  return finalSentence,prob\n",
    "\n",
    "def gather_log_probabilities(logits: torch.Tensor, labels: torch.LongTensor) -> torch.Tensor:\n",
    "    \"\"\"Gather log probabilities of the given labels from the logits.\"\"\"\n",
    "    log_probs = torch.nn.functional.log_softmax(logits.float(), dim=-1)\n",
    "    log_probs_labels = log_probs.gather(dim=-1, index=labels.unsqueeze(dim=-1))\n",
    "    return log_probs_labels.squeeze(dim=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "d7ff7ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loop\n",
      "(' of', tensor(286), ' of', -0.5144959688186646)\n",
      "initial loop\n",
      "(' of', tensor(290), ' and', -2.7704010009765625)\n",
      "initial loop\n",
      "(' of', tensor(326), ' that', -2.822998046875)\n",
      "uniqueTokenLength:  [3, 9, 20]\n",
      "best_path:  [0, 1, 3]\n",
      "Mydecoder_text:   The anime's title was inspired by the principle purpose of the Nameless : to suffer in battle for the goals of humanity.\n",
      "input ids:  tensor([[  383, 11984,   705,    82,  3670,   373,  7867,   416,   262,  7989,\n",
      "          4007,   286,   262, 17871,  5321,  1058,   284,  8659,   287,  3344,\n",
      "           329,   262,  4661]])\n",
      "Mydecoder_prob:  -3.385124385356903\n",
      "beam_text:   The anime's title was inspired by the principle purpose of the Nameless : to suffer in battle for the goals of humanity.\n",
      "greedy_ids:  tensor([[  383, 11984,   705,    82,  3670,   373,  7867,   416,   262,  7989,\n",
      "          4007,   286,   262, 17871,  5321,  1058,   284,  8659,   287,  3344,\n",
      "           329,   262,  4661,   286,   262, 17871]])\n",
      "greedy_text:   The anime's title was inspired by the principle purpose of the Nameless : to suffer in battle for the goals of the Nam\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "prompt =\" The anime 's title was inspired by the principle purpose of the Nameless : to suffer in battle for the goals\"\n",
    "Mydecoder_text,decoder_prob = runViterbiTransformerPipeline(prompt, loop_runner = 3)\n",
    "print(\"Mydecoder_text: \",Mydecoder_text)\n",
    "# print(math.exp(decoder_prob))\n",
    "# # -- Greedy decoding\n",
    "# Mydecoder_ids = tokenizer.encode(Mydecoder_text)\n",
    "# print(\"Mydecoder_ids: \", torch.tensor([Mydecoder_ids]))\n",
    "\n",
    "# Mydecoder_ids = torch.tensor([Mydecoder_ids])\n",
    "tokenized_result = tokenizer(prompt,return_tensors = \"pt\")\n",
    "input_ids = tokenized_result[\"input_ids\"]\n",
    "print(\"input ids: \", input_ids)\n",
    "attn_mask = tokenized_result['attention_mask']\n",
    "greedy_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_length= len(input_ids[0])+3,\n",
    "    do_sample=False,  # Greedy\n",
    "    attention_mask = attn_mask\n",
    ")\n",
    "beam_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=len(input_ids[0])+3,\n",
    "        num_beams=3,  \n",
    "        early_stopping=False, #check this\n",
    "        attention_mask = attn_mask\n",
    "    )\n",
    "beam_text = tokenizer.decode(beam_ids[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "print('Mydecoder_prob: ',decoder_prob)\n",
    "print(\"beam_text: \", beam_text)\n",
    "print(\"greedy_ids: \",greedy_ids)\n",
    "greedy_text = tokenizer.decode(greedy_ids[0], skip_special_tokens=True)\n",
    "print(\"greedy_text: \", greedy_text)\n",
    "text1 = tokenizer.decode(greedy_ids[0],skip_special_tokens = True)\n",
    "\n",
    "text2 = tokenizer.decode(greedy_ids[0],skip_special_tokens = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157ec132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65.06618778890784\n",
      "68.60735560665874\n",
      "65.06618778890784\n"
     ]
    }
   ],
   "source": [
    "perplexity = compute_perplexity(model,tokenizer,Mydecoder_text)\n",
    "print(perplexity)\n",
    "perplexity2 = compute_perplexity(model,tokenizer,greedy_text)\n",
    "print(perplexity2)\n",
    "perplexity3 = compute_perplexity(model,tokenizer,beam_text)\n",
    "print(perplexity3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e2d683",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = LMHeadModel(\"gpt2\")\n",
    "tokens = model2.get_batch_predictions([' Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles, is an RPG game'],top_k=500)\n",
    "print(tokens)\n",
    "for i in range(len(tokens[0])):\n",
    "    if '.' in tokens[0][i][0]:\n",
    "        print(tokens[0][i][1])\n",
    "        print(i)\n",
    "    else:\n",
    "        print(\"not there\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "bb794146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(' ', tensor(220), ' ', -2.4594573974609375), (' ', tensor(4707), '・', -3.4752273559570312), (' ', tensor(11839), 'ア', -3.8759841918945312)]\n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# batch_sentences = ['I enjoy walking in the park, but','I enjoy walking in the park, and', 'I enjoy walking in the park. I', 'I enjoy walking in the park and seeing', 'I enjoy walking in the park and it', 'I enjoy walking in the park. It', 'I enjoy walking in the park.\\n', 'I enjoy walking in the streets of New', 'I enjoy walking in the streets of the', 'I enjoy walking in the streets of London']\n",
    "batch_sentences = [' Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3, lit. Valkyria of the Battlefield 3 ), commonly referred to as Valkyria Chronicles ( Japanese : ��']\n",
    "batch_predictions =  generate_token_and_probability(model, tokenizer, batch_sentences, top_k=3)\n",
    "\n",
    "for k in range(len(batch_sentences)):\n",
    "    print(batch_predictions[k])\n",
    "\n",
    "batch_sentences2 = [\"Hi How are you\", \"I am good\"]\n",
    "tokenizer.pad_token= tokenizer.eos_token\n",
    "\n",
    "tokenized_result = tokenizer(batch_sentences2, return_tensors=\"pt\",padding = True,truncation = True)\n",
    "s = tokenized_result[\"input_ids\"][1][-1] == tokenizer.decode(1)\n",
    "if s:\n",
    "    print(\"YES\")\n",
    "\n",
    "print(tokenizer.decode(50256))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "894a36e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' = Valkyria Chronicles III = \\n', ' Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles', ' The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While', ' It met with positive sales in Japan , and was praised by both Japanese and western critics . After release , it received', ' = = Gameplay = = \\n', ' As with previous <unk> Chronicles games , Valkyria Chronicles III is a tactical role @-@ playing game where players take control of', \" The game 's battle system , the <unk> system , is carried over directly from <unk> Chronicles . During missions , players select each\", ' Troops are divided into five classes : Scouts , <unk> , Engineers , Lancers and Armored Soldier . Troopers can switch classes by changing their', ' = = Plot = = \\n', ' The game takes place during the Second Europan War . Gallian Army Squad 422 , also known as \" The Nameless \" , are a', ' As the Nameless officially do not exist , the upper echelons of the Gallian Army exploit the concept of plausible deniability', ' Partly due to these events , and partly due to the major losses in manpower Gallia suffers towards the end of', ' = = Development = = \\n', ' Concept work for Valkyria Chronicles III began after development finished on Valkyria Chronicles II in early 2010 , with full development', ' The majority of material created for previous games , such as the <unk> system and the design of maps , was carried', ' = = = Music = = = \\n', ' The music was composed by Hitoshi Sakimoto , who had also worked on the previous Valkyria Chronicles games . When he originally', ' = = = Release = = = \\n', ' In September 2010 , a teaser website was revealed by Sega , hinting at a new Valkyria Chronicles game . In its September', ' Unlike its two predecessors , Valkyria Chronicles III was not released in the west . According to Sega , this was due to', ' = = Reception = = \\n', ' On its day of release in Japan , Valkyria Chronicles III topped both platform @-@ exclusive and multi @-@ platform sales charts . By', ' Famitsu enjoyed the story , and were particularly pleased with the improvements to gameplay . Japanese gaming site Game Watch Impress , despite', \" PlayStation Official Magazine - UK praised the story 's blurring of Gallia 's moral standing , art style , and most points about\", ' In a preview of the TGS demo , Ryan Geddes of IGN was left excited as to where the game would', ' = = Legacy = = \\n', ' Kurt and Riela were featured in the Nintendo 3DS crossover Project X Zone , representing the Valkyria series . Media.Vision would', ' = = = Adaptations = = = \\n', ' Valkyria Chronicles 3 was adapted into a two @-@ episode original video animation series in the same year of its release', \" The anime 's title was inspired by the principle purpose of the Nameless : to suffer in battle for the goals\", \" Two manga adaptations were produced , following each of the game 's main female protagonists Imca and Riela . They were Senjō\"]\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "wikitext = load_dataset(\"wikitext\", \"wikitext-103-v1\")\n",
    "prompts = []\n",
    "\n",
    "for text in wikitext[\"train\"][\"text\"][:50]:\n",
    "    if len(text) > 0:\n",
    "        # Use regex to find word boundaries\n",
    "        matches = list(re.finditer(r'\\b\\w+\\b', text))\n",
    "        if len(matches) >= 20:\n",
    "            # Get the end position of the 20th word\n",
    "            end_pos = matches[19].end()\n",
    "            prompt = text[:end_pos]\n",
    "            prompts.append(prompt)\n",
    "        else:\n",
    "            prompts.append(text)\n",
    "print(prompts)\n",
    "print(len(prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "810ca8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(' park', ' park', -1.8385964632034302), (' park', ' woods', -2.2997779846191406), (' park', ' streets', -3.1739540100097656), (' park', ' dark', -3.468158721923828), (' park', ' door', -3.519336700439453)]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(model.device)\n",
    "\n",
    "prompts = [\"I enjoy walking in the\"]\n",
    "results = generate_token_and_probability(model, tokenizer, prompts, max_length=3, top_k=5)\n",
    "\n",
    "for res in results:\n",
    "    print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "ce3e18b0-d3ed-4c7d-9599-c002b1159881",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jivesh\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "  0%|          | 0/31 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loop\n",
      "('\\n', tensor(464), 'The', -7.198333740234375)\n",
      "initial loop\n",
      "('\\n', tensor(32), 'A', -8.230270385742188)\n",
      "initial loop\n",
      "('\\n', tensor(40), 'I', -8.355133056640625)\n",
      "uniqueTokenLength:  [3, 9, 23, 47, 84, 136, 192]\n",
      "best_path:  [0, 1, 3, 9, 20, 35, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/31 [01:00<30:06, 60.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loop\n",
      "(' (', tensor(357), ' (', -2.1600699424743652)\n",
      "initial loop\n",
      "(' (', tensor(362), ' 2', -2.6191840171813965)\n",
      "initial loop\n",
      "(' (', tensor(11), ',', -2.8448691368103027)\n",
      "uniqueTokenLength:  [3, 9, 18, 36, 67, 104, 134]\n",
      "best_path:  [1, 3, 6, 8, 20, 38, 36]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 2/31 [02:04<30:09, 62.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loop\n",
      "(' the', tensor(262), ' the', -1.3904844522476196)\n",
      "initial loop\n",
      "(' the', tensor(340), ' it', -2.6403088569641113)\n",
      "initial loop\n",
      "(' the', tensor(569), ' V', -2.793346881866455)\n",
      "uniqueTokenLength:  [3, 9, 21, 37, 72, 106, 137]\n",
      "best_path:  [2, 6, 14, 31, 69, 20, 37]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 3/31 [03:00<27:44, 59.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loop\n",
      "(' mixed', tensor(7668), ' mixed', -1.6538703441619873)\n",
      "initial loop\n",
      "(' mixed', tensor(257), ' a', -2.064377546310425)\n",
      "initial loop\n",
      "(' mixed', tensor(3967), ' positive', -3.191574811935425)\n",
      "uniqueTokenLength:  [3, 7, 12, 25, 43, 80, 119]\n",
      "best_path:  [0, 0, 3, 7, 18, 39, 72]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 4/31 [03:39<23:14, 51.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loop\n",
      "('\\n', tensor(464), 'The', -9.109089851379395)\n",
      "initial loop\n",
      "('\\n', tensor(1), '\"', -9.503743171691895)\n",
      "initial loop\n",
      "('\\n', tensor(40), 'I', -9.685185432434082)\n",
      "uniqueTokenLength:  [3, 9, 23, 57, 85, 110, 148]\n",
      "best_path:  [2, 6, 15, 36, 62, 60, 74]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 5/31 [04:34<22:55, 52.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loop\n",
      "(' a', tensor(257), ' a', -1.2596646547317505)\n",
      "initial loop\n",
      "(' a', tensor(262), ' the', -1.9537640810012817)\n",
      "initial loop\n",
      "(' a', tensor(281), ' an', -2.8123836517333984)\n",
      "uniqueTokenLength:  [3, 9, 12, 19, 43, 54, 93]\n",
      "best_path:  [0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 6/31 [05:10<19:32, 46.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loop\n",
      "(' of', tensor(286), ' of', -1.9789097309112549)\n",
      "initial loop\n",
      "(' of', tensor(2095), ' character', -2.5431034564971924)\n",
      "initial loop\n",
      "(' of', tensor(17710), ' faction', -2.8442890644073486)\n",
      "uniqueTokenLength:  [3, 6, 18, 37, 55, 83, 96]\n",
      "best_path:  [0, 0, 0, 1, 3, 9, 15]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 7/31 [05:58<18:54, 47.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loop\n",
      "(' class', tensor(1398), ' class', -2.7211570739746094)\n",
      "initial loop\n",
      "(' class', tensor(8328), ' armor', -2.831005096435547)\n",
      "initial loop\n",
      "(' class', tensor(1438), ' name', -3.1902198791503906)\n",
      "uniqueTokenLength:  [3, 6, 14, 30, 45, 73, 95]\n",
      "best_path:  [2, 4, 10, 5, 12, 27, 17]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 8/31 [06:42<17:43, 46.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loop\n",
      "('\\n', tensor(13), '.', -10.018171310424805)\n",
      "initial loop\n",
      "('\\n', tensor(1), '\"', -10.205854415893555)\n",
      "initial loop\n",
      "('\\n', tensor(59), '\\\\', -10.61622428894043)\n",
      "uniqueTokenLength:  [3, 9, 22, 46, 66, 94, 133]\n",
      "best_path:  [1, 3, 2, 5, 8, 20, 31]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 9/31 [07:27<16:50, 45.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loop\n",
      "(' unit', tensor(4326), ' unit', -1.9791643619537354)\n",
      "initial loop\n",
      "(' unit', tensor(1448), ' group', -2.1561357975006104)\n",
      "initial loop\n",
      "(' unit', tensor(1402), ' small', -2.925354242324829)\n",
      "uniqueTokenLength:  [3, 7, 17, 38, 66, 92, 128]\n",
      "best_path:  [1, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 10/31 [08:19<16:42, 47.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loop\n",
      "(' to', tensor(284), ' to', -1.3860104084014893)\n",
      "initial loop\n",
      "(' to', tensor(13), '.', -2.0191891193389893)\n",
      "initial loop\n",
      "(' to', tensor(290), ' and', -2.4166347980499268)\n",
      "uniqueTokenLength:  [3, 9, 20, 48, 68, 108, 132]\n",
      "best_path:  [0, 1, 3, 8, 15, 28, 48]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 11/31 [09:17<16:59, 50.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loop\n",
      "(' the', tensor(262), ' the', -0.381603866815567)\n",
      "initial loop\n",
      "(' the', tensor(663), ' its', -2.8261077404022217)\n",
      "initial loop\n",
      "(' the', tensor(607), ' her', -3.694096326828003)\n",
      "uniqueTokenLength:  [3, 8, 9, 17, 34, 63, 85]\n",
      "best_path:  [0, 1, 3, 9, 6, 15, 12]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▊      | 12/31 [09:49<14:21, 45.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loop\n",
      "('\\n', tensor(464), 'The', -8.422683715820312)\n",
      "initial loop\n",
      "('\\n', tensor(40), 'I', -9.2152099609375)\n",
      "initial loop\n",
      "('\\n', tensor(32), 'A', -9.574951171875)\n",
      "uniqueTokenLength:  [3, 9, 24, 53, 90, 121, 142]\n",
      "best_path:  [1, 3, 8, 18, 36, 26, 29]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 13/31 [10:45<14:29, 48.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loop\n",
      "(' on', tensor(319), ' on', -2.03762149810791)\n",
      "initial loop\n",
      "(' on', tensor(17715), ' underway', -2.863396644592285)\n",
      "initial loop\n",
      "(' on', tensor(5668), ' completed', -2.99141788482666)\n",
      "uniqueTokenLength:  [3, 7, 16, 31, 53, 82, 116]\n",
      "best_path:  [0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 14/31 [11:30<13:26, 47.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loop\n",
      "(' out', tensor(503), ' out', -0.7753186821937561)\n",
      "initial loop\n",
      "(' out', tensor(625), ' over', -0.9287458062171936)\n",
      "initial loop\n",
      "(' out', tensor(319), ' on', -3.63969087600708)\n",
      "uniqueTokenLength:  [3, 7, 7, 19, 24, 38, 69]\n",
      "best_path:  [1, 3, 4, 12, 15, 27, 48]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 15/31 [11:55<10:50, 40.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loop\n",
      "('\\n', tensor(464), 'The', -8.665026664733887)\n",
      "initial loop\n",
      "('\\n', tensor(40), 'I', -9.075716972351074)\n",
      "initial loop\n",
      "('\\n', tensor(32), 'A', -9.599215507507324)\n",
      "uniqueTokenLength:  [3, 9, 25, 58, 88, 116, 144]\n",
      "best_path:  [2, 6, 17, 40, 52, 75, 17]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 16/31 [12:54<11:30, 46.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loop\n",
      "(' started', tensor(2067), ' started', -2.344339609146118)\n",
      "initial loop\n",
      "(' started', tensor(1625), ' came', -2.4865972995758057)\n",
      "initial loop\n",
      "(' started', tensor(2630), ' wrote', -2.9103033542633057)\n",
      "uniqueTokenLength:  [3, 8, 18, 29, 54, 75, 112]\n",
      "best_path:  [0, 0, 0, 1, 3, 5, 14]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 17/31 [13:37<10:35, 45.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loop\n",
      "('\\n', tensor(464), 'The', -9.169486045837402)\n",
      "initial loop\n",
      "('\\n', tensor(40), 'I', -9.734274864196777)\n",
      "initial loop\n",
      "('\\n', tensor(32), 'A', -10.054144859313965)\n",
      "uniqueTokenLength:  [3, 9, 23, 54, 83, 102, 153]\n",
      "best_path:  [2, 6, 14, 37, 57, 56, 95]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 18/31 [14:33<10:30, 48.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loop\n",
      "(' 2010', tensor(3050), ' 2010', -0.7175477147102356)\n",
      "initial loop\n",
      "(' 2010', tensor(2813), ' 2011', -1.9864838123321533)\n",
      "initial loop\n",
      "(' 2010', tensor(3717), ' 2009', -2.9732391834259033)\n",
      "uniqueTokenLength:  [3, 4, 7, 12, 22, 36, 57]\n",
      "best_path:  [0, 1, 0, 0, 14, 26, 46]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████▏   | 19/31 [14:56<08:10, 40.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loop\n",
      "(' the', tensor(262), ' the', -1.3555811643600464)\n",
      "initial loop\n",
      "(' the', tensor(257), ' a', -1.9958933591842651)\n",
      "initial loop\n",
      "(' the', tensor(281), ' an', -3.4174938201904297)\n",
      "uniqueTokenLength:  [3, 7, 17, 38, 72, 91, 124]\n",
      "best_path:  [0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 20/31 [15:49<08:08, 44.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loop\n",
      "('\\n', tensor(13), '.', -10.133956909179688)\n",
      "initial loop\n",
      "('\\n', tensor(1), '\"', -10.362136840820312)\n",
      "initial loop\n",
      "('\\n', tensor(59), '\\\\', -10.687728881835938)\n",
      "uniqueTokenLength:  [3, 7, 18, 38, 60, 83, 109]\n",
      "best_path:  [2, 5, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 21/31 [16:31<07:16, 43.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loop\n",
      "(' comparison', tensor(7208), ' comparison', -1.3037508726119995)\n",
      "initial loop\n",
      "(' comparison', tensor(262), ' the', -1.5154742002487183)\n",
      "initial loop\n",
      "(' comparison', tensor(6273), ' contrast', -3.531106948852539)\n",
      "uniqueTokenLength:  [3, 7, 16, 31, 53, 74, 116]\n",
      "best_path:  [0, 0, 1, 3, 9, 17, 40]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 22/31 [17:16<06:36, 44.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loop\n",
      "(' its', tensor(663), ' its', -2.006054401397705)\n",
      "initial loop\n",
      "(' its', tensor(852), ' being', -2.0092358589172363)\n",
      "initial loop\n",
      "(' its', tensor(262), ' the', -2.0614514350891113)\n",
      "uniqueTokenLength:  [3, 8, 20, 39, 73, 88, 108]\n",
      "best_path:  [2, 6, 16, 13, 34, 3, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 23/31 [18:09<06:13, 46.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loop\n",
      "(' the', tensor(262), ' the', -1.5221054553985596)\n",
      "initial loop\n",
      "(' the', tensor(607), ' her', -1.6995575428009033)\n",
      "initial loop\n",
      "(' the', tensor(703), ' how', -3.268885850906372)\n",
      "uniqueTokenLength:  [3, 8, 13, 23, 49, 71, 103]\n",
      "best_path:  [1, 0, 0, 2, 5, 13, 24]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 24/31 [18:50<05:16, 45.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loop\n",
      "(' go', tensor(467), ' go', -0.847834050655365)\n",
      "initial loop\n",
      "(' go', tensor(886), ' end', -1.853884220123291)\n",
      "initial loop\n",
      "(' go', tensor(307), ' be', -2.5809578895568848)\n",
      "uniqueTokenLength:  [3, 7, 13, 29, 60, 91, 105]\n",
      "best_path:  [0, 0, 1, 4, 9, 23, 48]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 25/31 [19:37<04:34, 45.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loop\n",
      "('\\n', tensor(464), 'The', -9.245551109313965)\n",
      "initial loop\n",
      "('\\n', tensor(40), 'I', -9.873419761657715)\n",
      "initial loop\n",
      "('\\n', tensor(32), 'A', -10.01676082611084)\n",
      "uniqueTokenLength:  [3, 9, 25, 59, 101, 141, 173]\n",
      "best_path:  [2, 6, 18, 47, 83, 120, 16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 26/31 [20:40<04:13, 50.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loop\n",
      "(' later', tensor(1568), ' later', -0.7500079274177551)\n",
      "initial loop\n",
      "(' later', tensor(635), ' also', -3.022423028945923)\n",
      "initial loop\n",
      "(' later', tensor(4191), ' eventually', -3.124077081680298)\n",
      "uniqueTokenLength:  [3, 7, 9, 23, 42, 67, 89]\n",
      "best_path:  [0, 1, 3, 9, 18, 35, 59]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 27/31 [21:17<03:07, 46.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loop\n",
      "('\\n', tensor(464), 'The', -8.804549217224121)\n",
      "initial loop\n",
      "('\\n', tensor(40), 'I', -9.292571067810059)\n",
      "initial loop\n",
      "('\\n', tensor(32), 'A', -9.751875877380371)\n",
      "uniqueTokenLength:  [3, 9, 25, 58, 83, 108, 145]\n",
      "best_path:  [2, 6, 16, 40, 65, 90, 86]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 28/31 [22:16<02:30, 50.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loop\n",
      "('.', tensor(13), '.', -0.5161438584327698)\n",
      "initial loop\n",
      "('.', tensor(11), ',', -2.1396942138671875)\n",
      "initial loop\n",
      "('.', tensor(287), ' in', -3.19915771484375)\n",
      "uniqueTokenLength:  [3, 9, 23, 43, 73, 97, 126]\n",
      "best_path:  [0, 2, 6, 16, 30, 43, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▎| 29/31 [23:15<01:45, 52.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loop\n",
      "(' of', tensor(286), ' of', -0.5144959688186646)\n",
      "initial loop\n",
      "(' of', tensor(290), ' and', -2.7704010009765625)\n",
      "initial loop\n",
      "(' of', tensor(326), ' that', -2.822998046875)\n",
      "uniqueTokenLength:  [3, 9, 20, 46, 70, 89, 109]\n",
      "best_path:  [0, 0, 0, 0, 1, 3, 9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 30/31 [24:08<00:53, 53.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loop\n",
      "(' and', tensor(290), ' and', -1.8769232034683228)\n",
      "initial loop\n",
      "(' and', tensor(11), ',', -2.310577869415283)\n",
      "initial loop\n",
      "(' and', tensor(338), \"'s\", -2.874199390411377)\n",
      "uniqueTokenLength:  [3, 8, 20, 38, 47, 79, 119]\n",
      "best_path:  [0, 0, 0, 0, 1, 3, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [24:55<00:00, 48.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to C:/Users/jivesh/Desktop/SeniorThesis/decoder_comparison_20250401_133230.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "# model2 = LMHeadModel(model_name)\n",
    "model.eval()  # put model in inference mode\n",
    "\n",
    "# If using GPU (e.g., on Colab), you could also do:\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Function to generate text using different decoders\n",
    "# ---------------------------------------------------------\n",
    "def generate_texts(model, tokenizer, prompt, max_length=40):\n",
    "    \"\"\"Generate text from a prompt using different decoding strategies.\"\"\"\n",
    "    tokenized_result = tokenizer(prompt,return_tensors = \"pt\")\n",
    "    input_ids = tokenized_result[\"input_ids\"]\n",
    "    # If on GPU, uncomment next line:\n",
    "    # input_ids = input_ids.to(device)\n",
    "    attn_mask = tokenized_result['attention_mask']\n",
    "    \n",
    "\n",
    "    # -- Greedy decoding\n",
    "    greedy_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        do_sample=False,  # Greedy\n",
    "        attention_mask = attn_mask\n",
    "    )\n",
    "    greedy_text = tokenizer.decode(greedy_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # -- Beam search\n",
    "    beam_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        num_beams=3,    # for example\n",
    "        early_stopping=True,\n",
    "        attention_mask = attn_mask\n",
    "    )\n",
    "    beam_text = tokenizer.decode(beam_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # -- Top-k sampling\n",
    "    topk_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        do_sample=True,\n",
    "        top_k=50,  # for example\n",
    "        attention_mask = attn_mask\n",
    "    )\n",
    "    topk_text = tokenizer.decode(topk_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # -- Nucleus (top-p) sampling\n",
    "    topp_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,  # for example\n",
    "        attention_mask = attn_mask\n",
    "    )\n",
    "    topp_text = tokenizer.decode(topp_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    Mydecoder_text,decoder_prob = runViterbiTransformerPipeline(prompt, loop_runner = 7)\n",
    "    Mygreedy_text,Mygreedy_prob  =runTransformerPipeline(prompt,loop_runner = 7)\n",
    "\n",
    "    return {\n",
    "        \"greedy\": greedy_text,\n",
    "        \"beam\": beam_text,\n",
    "        \"topk\": topk_text,\n",
    "        \"topp\": topp_text,\n",
    "        \"ourDecoder\": Mydecoder_text,\n",
    "        \"ourGreedy\": Mygreedy_text\n",
    "\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Function to compute perplexity of a string\n",
    "# ---------------------------------------------------------\n",
    "def compute_perplexity(model, tokenizer, text):\n",
    "    \"\"\"Compute perplexity of `text` under `model`.\"\"\"\n",
    "    encodings = tokenizer(text, return_tensors=\"pt\")\n",
    "    input_ids = encodings.input_ids\n",
    "    # If on GPU, uncomment next line:\n",
    "    # input_ids = input_ids.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # The model returns a tuple of (loss, logits, ...)\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        # outputs.loss is the average cross-entropy across tokens\n",
    "        neg_log_likelihood = outputs.loss.item()\n",
    "\n",
    "    perplexity = math.exp(neg_log_likelihood)\n",
    "    return perplexity\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Main loop: generate text, then compute perplexities\n",
    "# ---------------------------------------------------------\n",
    "all_results = []\n",
    "\n",
    "for prompt in tqdm(prompts):\n",
    "\n",
    "    input_ids = tokenizer(prompt, return_tensors = \"pt\")[\"input_ids\"]\n",
    "    length = len(input_ids[0])\n",
    "\n",
    "    # Generate text via different decoding methods\n",
    "    gen_texts = generate_texts(model, tokenizer, prompt,max_length = length+7)\n",
    "\n",
    "    # Compute perplexities of the generated texts\n",
    "    results_for_prompt = {\"prompt\": prompt}\n",
    "    for method, text in gen_texts.items():\n",
    "        ppl = compute_perplexity(model, tokenizer, text)\n",
    "        results_for_prompt[f'{method}_text'] = text\n",
    "        results_for_prompt[f'{method}_ppl'] = ppl\n",
    "\n",
    "    # Store results\n",
    "    all_results.append(results_for_prompt)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Printing the Results\n",
    "# for res in all_results:\n",
    "#     print(f\"Prompt: {res['prompt']}\")\n",
    "#     print(f\"  Greedy PPL: {res['greedy_ppl']:.2f}\")\n",
    "#     print(f\"  Beam   PPL: {res['beam_ppl']:.2f}\")\n",
    "#     print(f\"  Top-k  PPL: {res['topk_ppl']:.2f}\")\n",
    "#     print(f\"  Top-p  PPL: {res['topp_ppl']:.2f}\")\n",
    "#     print(f\" Viterbi PPL: {res['ourDecoder_ppl']:.2f}\")\n",
    "#     print(f\"OurGreedy  PPL: {res[\"ourGreedy_ppl\"]:.2f}\")\n",
    "#     print(f\"Greedy answer: {res[\"greedy_text\"]}\")\n",
    "\n",
    "#     print(f\"ourGreedy answer: {res[\"ourGreedy_text\"]}\")\n",
    "#     print(f\"ourDecoder answer: {res[\"ourDecoder_text\"]}\")\n",
    "#     print(f\"Beam answer: {res['beam_text']}\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Save results to CSV\n",
    "# ---------------------------------------------------------\n",
    "results_df = pd.DataFrame(all_results)\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = f\"C:/Users/jivesh/Desktop/SeniorThesis/decoder_comparison_{timestamp}.csv\"\n",
    "results_df.to_csv(output_file, index=False)\n",
    "print(f\"Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "36bc6714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of times ourDecoder_ppl is greater than or equal to beam_ppl  22\n",
      "Number of times ourDecoder_ppl is greater than or equal to greedy_ppl  23\n",
      "Number of times ourDecoder_ppl is greater than to beam_ppl  17\n",
      "Number of times ourDecoder_ppl is greater than to greedy_ppl  20\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(output_file)  \n",
    "\n",
    "# Count occurrences where ourDecoder_ppl > beam_ppl and ourDecoder_ppl > greedy_ppl\n",
    "count_beam = ((df[\"ourDecoder_ppl\"] <= df[\"beam_ppl\"])).sum()  \n",
    "count_greedy = (df[\"ourDecoder_ppl\"] <= df[\"greedy_ppl\"]).sum()\n",
    "\n",
    "print(\"Number of times ourDecoder_ppl is greater than or equal to beam_ppl \", count_beam)\n",
    "print(\"Number of times ourDecoder_ppl is greater than or equal to greedy_ppl \", count_greedy)\n",
    "\n",
    "count_beam = ((df[\"ourDecoder_ppl\"] < df[\"beam_ppl\"])).sum()  \n",
    "count_greedy = (df[\"ourDecoder_ppl\"] < df[\"greedy_ppl\"]).sum()\n",
    "\n",
    "print(\"Number of times ourDecoder_ppl is greater than to beam_ppl \", count_beam)\n",
    "print(\"Number of times ourDecoder_ppl is greater than to greedy_ppl \", count_greedy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d7601372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.413704459110896\n"
     ]
    }
   ],
   "source": [
    "perplexity = compute_perplexity(model,tokenizer,beam_text)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "07c0f804-76b8-4d46-8468-b63c16518a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Valkyria', 'Chronicles', 'III']\n",
      "9\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "prompt = prompts[0]\n",
    "result = re.findall(r'\\w+|[.,!?;'']', prompt)\n",
    "print(result)\n",
    "input_ids = tokenizer(prompt, return_tensors = \"pt\")[\"input_ids\"]\n",
    "print(len(input_ids[0]))\n",
    "print(len(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8b7ff021-5f40-40c1-8ad3-a40bd7313f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import itertools\n",
    "\n",
    "class SearchTree:\n",
    "    def __init__(self, context, probability, parent=None, parent_index=None):\n",
    "        self.context = context\n",
    "        self.probability = probability\n",
    "        self.parent = parent\n",
    "        self.parent_index = parent_index\n",
    "        self.children = []\n",
    "    \n",
    "    def create_child(self):\n",
    "        return self\n",
    "    \n",
    "    def build_Context(self):\n",
    "        if self.parent is None:\n",
    "            return self.context\n",
    "        else:\n",
    "            return self.parent.build_Context() + self.context\n",
    "    \n",
    "    def calcProbTillNow(self):\n",
    "        if self.parent is None:\n",
    "            return self.probability\n",
    "        else:\n",
    "            return self.probability * self.parent.calcProbTillNow()\n",
    "    \n",
    "    def replace_parent(self, new_parent):\n",
    "        self.parent = new_parent\n",
    "        if new_parent is not None:\n",
    "            new_parent.children.append(self)\n",
    "    \n",
    "    def assign_parent_index(self, index):\n",
    "        self.parent_index = index\n",
    "\n",
    "def findProbability(prevToken, newTokens, model_tokenizer_tuple):\n",
    "    model, tokenizer = model_tokenizer_tuple\n",
    "    \n",
    "    probs = []\n",
    "    batch_sentences = [prevToken.build_Context() + newToken.context for newToken in newTokens]\n",
    "    \n",
    "    # Tokenize inputs\n",
    "    inputs = tokenizer(batch_sentences, return_tensors=\"pt\", padding=True)\n",
    "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(model.device)\n",
    "    \n",
    "    # Pass through the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    # Get logits for the last token in each sequence\n",
    "    last_token_indices = attention_mask.sum(dim=1) - 1\n",
    "    batch_size = input_ids.shape[0]\n",
    "    logits = outputs.logits[torch.arange(batch_size), last_token_indices]\n",
    "    \n",
    "    # Compute probabilities using softmax\n",
    "    probs_tensor = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    \n",
    "    # Get the probability of the specific token that was generated\n",
    "    for i, newToken in enumerate(newTokens):\n",
    "        token_id = tokenizer.encode(newToken.context, add_special_tokens=False)[-1]\n",
    "        if token_id < probs_tensor.shape[1]:\n",
    "            probs.append(probs_tensor[i, token_id].item())\n",
    "        else:\n",
    "            probs.append(0.0)  # Fallback if token ID is out of range\n",
    "    \n",
    "    return probs\n",
    "\n",
    "def generateIntermediates(root, numTokens=3, loop_runner=4):\n",
    "    # Initialize transformers model and tokenizer directly\n",
    "    model_name = \"gpt2\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    model.eval()\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Ensure padding token is set\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.padding_side = \"right\"\n",
    "    \n",
    "    # Model and tokenizer tuple for probability calculations\n",
    "    model_tokenizer = (model, tokenizer)\n",
    "    \n",
    "    sentence = SearchTree(root, 1)\n",
    "    context = []\n",
    "    prob_list = []\n",
    "    num_tokens = numTokens\n",
    "    content = []\n",
    "    probability = []\n",
    "    children = []\n",
    "    overlap = []\n",
    "    most_common = []\n",
    "    unique_tokens = set()\n",
    "    probabilityMatrix = []\n",
    "    uniqueTokensList = []\n",
    "    new_content = []\n",
    "    uniqueTokenLength = []\n",
    "    \n",
    "    flops_counter = {}\n",
    "    batch_size = 75\n",
    "    holdout_number = 15\n",
    "    \n",
    "    # Get initial predictions\n",
    "    input_ids = tokenizer(sentence.context, return_tensors=\"pt\").input_ids.to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "    \n",
    "    # Get logits for the last token\n",
    "    logits = outputs.logits[:, -1, :]\n",
    "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    top_probs, top_indices = torch.topk(probs, k=numTokens+3)\n",
    "    \n",
    "    # Process initial tokens\n",
    "    for i in range(num_tokens):\n",
    "        token_id = top_indices[0, i].item()\n",
    "        token_text = tokenizer.decode([token_id]).strip()\n",
    "        token_prob = top_probs[0, i].item()\n",
    "        \n",
    "        unique_tokens.add(token_text)\n",
    "        new_content.append(token_text)\n",
    "        probability.append(token_prob)\n",
    "        \n",
    "        context = SearchTree(token_text, token_prob, sentence, parent_index=0)\n",
    "        context.create_child()\n",
    "        uniqueTokensList.append(context)\n",
    "        children.append(context)\n",
    "    \n",
    "    content.append(new_content)\n",
    "    previousUniqueLength = num_tokens\n",
    "    initialStateProbability = probability\n",
    "    uniqueTokenLength.append(num_tokens)\n",
    "    \n",
    "    # Main loop for building the trellis\n",
    "    for i in range(2, loop_runner):\n",
    "        unique_tokens = set()\n",
    "        probability = []\n",
    "        new_content = []\n",
    "        previousSetLength = 0\n",
    "        \n",
    "        # Prepare batch sentences\n",
    "        batch_sentences = [child.build_Context() for child in uniqueTokensList]\n",
    "        total_predictions = []\n",
    "        \n",
    "        # Process in batches\n",
    "        if len(batch_sentences) > holdout_number:\n",
    "            # First batch\n",
    "            first_batch = batch_sentences[0:-holdout_number]\n",
    "            first_batch_predictions = get_top_k_predictions(first_batch, model, tokenizer, numTokens+2)\n",
    "            total_predictions.extend(first_batch_predictions)\n",
    "            \n",
    "            # Second batch\n",
    "            second_batch = batch_sentences[-holdout_number:]\n",
    "            second_batch_predictions = get_top_k_predictions(second_batch, model, tokenizer, numTokens+2)\n",
    "            total_predictions.extend(second_batch_predictions)\n",
    "        else:\n",
    "            total_predictions = get_top_k_predictions(batch_sentences, model, tokenizer, numTokens+2)\n",
    "        \n",
    "        # Process predictions\n",
    "        for j in range(len(uniqueTokensList)):\n",
    "            for s in range(num_tokens):\n",
    "                if s < len(total_predictions[j]):\n",
    "                    context_text = total_predictions[j][s][0]\n",
    "                    prob = total_predictions[j][s][1]\n",
    "                    \n",
    "                    unique_tokens.add(context_text)\n",
    "                    context = SearchTree(context_text, prob, uniqueTokensList[j])\n",
    "                    \n",
    "                    if len(unique_tokens) > previousSetLength:\n",
    "                        previousSetLength = len(unique_tokens)\n",
    "                        uniqueTokensList.append(context)\n",
    "                        new_content.append(context.context)\n",
    "        \n",
    "        # Store content\n",
    "        content.append(new_content)\n",
    "        \n",
    "        # Calculate combined probabilities\n",
    "        comb_prob = []\n",
    "        for prevToken in uniqueTokensList[:previousUniqueLength]:\n",
    "            comb_prob.append(findProbability(prevToken, uniqueTokensList[previousUniqueLength:], model_tokenizer))\n",
    "        comb_prob = list(itertools.chain(*comb_prob))  # flattening the list\n",
    "        \n",
    "        # Update parent relationships\n",
    "        for tokenumber, newToken in enumerate(uniqueTokensList[previousUniqueLength:]):\n",
    "            probs = [comb_prob[a*len(uniqueTokensList[previousUniqueLength:]) + tokenumber] for a in range(len(uniqueTokensList[:previousUniqueLength]))]\n",
    "            probs2 = [probs[i]*uniqueTokensList[:previousUniqueLength][i].calcProbTillNow() for i in range(len(probs))]\n",
    "            \n",
    "            if not probs2:\n",
    "                continue\n",
    "            else:\n",
    "                max_value = max(probs2)\n",
    "                max_index = probs2.index(max_value)\n",
    "                newToken.replace_parent(uniqueTokensList[:previousUniqueLength][max_index])\n",
    "                newToken.assign_parent_index(max_index)\n",
    "            \n",
    "            probability.append(probs)\n",
    "        \n",
    "        probabilityMatrix.append(probability)\n",
    "        flops_counter[i-1] = i  # Just a placeholder since we're not tracking actual FLOPS\n",
    "        \n",
    "        uniqueTokenLength.append(len(uniqueTokensList[previousUniqueLength:]))\n",
    "        previousUniqueLength = len(uniqueTokensList[previousUniqueLength:])\n",
    "        uniqueTokensList = uniqueTokensList[len(uniqueTokensList)-previousUniqueLength:]\n",
    "    \n",
    "    return probabilityMatrix, initialStateProbability, content, uniqueTokenLength, flops_counter\n",
    "\n",
    "def get_top_k_predictions(sentences, model, tokenizer, top_k=100):\n",
    "    \"\"\"\n",
    "    Get top-k token predictions for each sentence.\n",
    "    Returns a list of lists of (token, probability) tuples.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Tokenize\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "        input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "        \n",
    "        # Get model output\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "        \n",
    "        # Get predictions for the last token\n",
    "        logits = outputs.logits[:, -1, :]\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        top_probs, top_indices = torch.topk(probs, k=top_k)\n",
    "        \n",
    "        # Decode tokens and pair with probabilities\n",
    "        sentence_predictions = []\n",
    "        for i in range(min(top_k, top_indices.shape[1])):\n",
    "            token_id = top_indices[0, i].item()\n",
    "            token = tokenizer.decode([token_id]).strip()\n",
    "            if token and token != \"\\n\":\n",
    "                sentence_predictions.append((token, top_probs[0, i].item()))\n",
    "        \n",
    "        predictions.append(sentence_predictions)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63a5e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def generate_with_probabilities(text, max_new_tokens=50, top_k=10):\n",
    "    # Load model and tokenizer\n",
    "    model_name = \"gpt2\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    model.eval()\n",
    "    \n",
    "    # Make sure padding token is set\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Tokenize input text\n",
    "    input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "    \n",
    "    # Track generated tokens and their probabilities\n",
    "    generated_tokens = []\n",
    "    token_probabilities = []\n",
    "    \n",
    "    # Generate tokens one by one\n",
    "    for _ in range(max_new_tokens):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "            \n",
    "        # Get logits for the last token\n",
    "        next_token_logits = outputs.logits[:, -1, :]\n",
    "        \n",
    "        # Apply softmax to get probabilities\n",
    "        next_token_probs = torch.nn.functional.softmax(next_token_logits, dim=-1)\n",
    "        \n",
    "        # Get top k tokens and their probabilities\n",
    "        topk_probs, topk_indices = torch.topk(next_token_probs, top_k)\n",
    "        \n",
    "        # Select the token with highest probability\n",
    "        next_token = topk_indices[0, 0].unsqueeze(0).unsqueeze(0)\n",
    "        next_token_prob = topk_probs[0, 0].item()\n",
    "        \n",
    "        # Append to results\n",
    "        generated_tokens.append(tokenizer.decode(next_token[0]))\n",
    "        token_probabilities.append(next_token_prob)\n",
    "        \n",
    "        # Update input_ids for next iteration\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "    \n",
    "    return generated_tokens, token_probabilities\n",
    "\n",
    "# Example usage\n",
    "text = \"I enjoy walking in the park\"\n",
    "tokens, probs = generate_with_probabilities(text)\n",
    "\n",
    "# Print results\n",
    "for token, prob in zip(tokens, probs):\n",
    "    print(f\"Token: {token}, Probability: {prob:.4f}\")\n",
    "\n",
    "# Get the full generated text\n",
    "full_text = text + \"\".join(tokens)\n",
    "print(f\"\\nFull text: {full_text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
