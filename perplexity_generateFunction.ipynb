{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2307ad42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------------------------------------\n",
    "# Setup\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "import math\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import itertools\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "import math\n",
    "\n",
    "\n",
    "class LMHeadModel:\n",
    "    def __init__(self, model_name, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "        # Initialize the model and tokenizer\n",
    "        self.device = device\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float32).to(self.device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model.eval()\n",
    "\n",
    "        # Ensure the tokenizer has a padding token\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token  # Use EOS token as padding\n",
    "            self.tokenizer.padding_side = \"right\"\n",
    "\n",
    "        self.batch_prediction_count = 0\n",
    "\n",
    "\n",
    "    def batch_encode(self, sentences):\n",
    "        \"\"\"\n",
    "        Encodes a batch of sentences into input tensors.\n",
    "        Args:\n",
    "            sentences (list of str): The input sentences to encode.\n",
    "        Returns:\n",
    "            inputs (dict): A dictionary of tokenized inputs ready for the model.\n",
    "        \"\"\"\n",
    "        return self.tokenizer(\n",
    "            sentences,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,  # Pad to the longest sequence in the batch\n",
    "            truncation=True,  # Truncate sequences longer than the model's max length\n",
    "        ).to(self.device)\n",
    "\n",
    "    def batch_decode(self, token_ids):\n",
    "        \"\"\"\n",
    "        Decodes a batch of token IDs back to sentences.\n",
    "        Args:\n",
    "            token_ids (torch.Tensor): A tensor of token IDs to decode.\n",
    "        Returns:\n",
    "            decoded_sentences (list of str): The decoded sentences.\n",
    "        \"\"\"\n",
    "        return self.tokenizer.batch_decode(token_ids, skip_special_tokens=True)\n",
    "    def batch_decode_top_k(self, token_ids_batch, tokenizer):\n",
    "        \"\"\"\n",
    "        Decodes token IDs to meaningful text while merging subword tokens.\n",
    "        Args:\n",
    "            token_ids_batch (torch.Tensor): A batch of token IDs (e.g., from `topk`).\n",
    "            tokenizer: The tokenizer used for encoding/decoding.\n",
    "        Returns:\n",
    "            list of list of str: Decoded tokens (words/subwords) for each sequence in the batch.\n",
    "        \"\"\"\n",
    "        decoded_tokens = []\n",
    "        for token_ids in token_ids_batch:\n",
    "            # Decode each token ID in the batch, joining subwords correctly\n",
    "            tokens = [tokenizer.decode([token_id]).strip() for token_id in token_ids]\n",
    "            decoded_tokens.append(tokens)\n",
    "        return decoded_tokens\n",
    "\n",
    "    def get_batch_predictions(self, sentences, top_k=100):\n",
    "        \"\"\"\n",
    "        Predicts the next tokens for a batch of input sentences.\n",
    "        Args:\n",
    "            sentences (list of str): The input sentences.\n",
    "            top_k (int): Number of top tokens to return for each sentence.\n",
    "        Returns:\n",
    "            predictions (list of list of tuples): Top-k token predictions for each sentence.\n",
    "        \"\"\"\n",
    "        #Increment to see how many times this function is called after a given layer of trellis.\n",
    "        self.batch_prediction_count += 1\n",
    "\n",
    "\n",
    "        # Tokenize inputs\n",
    "        inputs = self.batch_encode(sentences)\n",
    "\n",
    "        # Pass through the model\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs,use_cache = False)\n",
    "\n",
    "        # Get logits for the last token in each sequence\n",
    "        logits = outputs.logits[:, -1, :]  # Shape: (batch_size, vocab_size)\n",
    "\n",
    "\n",
    "        # Compute probabilities using softmax\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        top_probs, top_token_ids = torch.topk(probs, k=top_k, dim=-1)\n",
    "        top_tokens = self.batch_decode_top_k(top_token_ids, self.tokenizer)\n",
    "\n",
    "\n",
    "        predictions = [\n",
    "            [(token, prob.item()) for token, prob in zip(top_tokens[i], top_probs[i]) if token and token != \"\\n\"]\n",
    "            for i in range(len(sentences))\n",
    "        ]\n",
    "        return predictions\n",
    "\n",
    "    def get_batch_prediction_count(self):\n",
    "        \"\"\"\n",
    "        Returns the number of times batch predictions have been made.\n",
    "        \"\"\"\n",
    "        return self.batch_prediction_count\n",
    "\n",
    "    def reset_batch_prediction_count(self):\n",
    "        \"\"\" Resets the count\n",
    "        \"\"\"\n",
    "\n",
    "        self.batch_prediction_count = 0\n",
    "class SearchTree:\n",
    "    def __init__(self,context,probability,token_id,model,tokenizer,parent = None,child = None,parent_index = None):\n",
    "        self.token_id = token_id\n",
    "        context = context.strip()\n",
    "        self.context = context\n",
    "        self.probability = probability\n",
    "        self.parent = parent\n",
    "        self.child = []\n",
    "        self.parent_index = parent_index  # newly created.\n",
    "        if child is not None:\n",
    "           self.child.append(child)\n",
    "        \n",
    "        # Cache cumulative probability at node creation\n",
    "        if parent:\n",
    "            self.cached_prob = parent.calcProbTillNow()+probability #parent.calcProbTillNow() * probability\n",
    "        else:\n",
    "            self.cached_prob = probability\n",
    "\n",
    "    def build_Context(self):\n",
    "        context_list = []\n",
    "        full_context = []\n",
    "        node = self\n",
    "        while node.parent is not None:\n",
    "            context_list.extend([node.token_id])\n",
    "            node = node.parent\n",
    "        context_list.reverse()\n",
    "        full_context.extend(node.token_id)\n",
    "        full_context.extend(context_list)\n",
    "        \n",
    "        # formatted_contextList = []\n",
    "        # for i in range(len(context_list)):\n",
    "        #     if context_list[i] in ['.',':',',','?','!',';'] or (\"'\" in context_list[i]):\n",
    "        #         if (i-1>= 0):\n",
    "        #             if context_list[i-1] not in  ['.',':',',','?','!',';'] and (\"'\" not in context_list[i-1]):#if two consecutive contexts are , ' etc.\n",
    "        #                 word = context_list[i-1]+context_list[i]\n",
    "\n",
    "        #                 formatted_contextList.remove(context_list[i-1])\n",
    "        #                 formatted_contextList.append(word)\n",
    "        #             else:\n",
    "        #                 formatted_contextList.append(context_list[i])\n",
    "        #     else:\n",
    "\n",
    "        #         formatted_contextList.append(context_list[i])\n",
    "        # return ' '.join(formatted_contextList)\n",
    "        generated_sentence = tokenizer.decode(full_context, skip_special_tokens=True)\n",
    "        return generated_sentence\n",
    "\n",
    "\n",
    "    def create_child(self):\n",
    "        if self.parent is not None:\n",
    "           self.parent.child.append(self)\n",
    "\n",
    "    def replace_parent(self, new_parent):\n",
    "        \"\"\"Assign a new parent and update cached probability.\"\"\"\n",
    "        self.parent = new_parent\n",
    "        self.cached_prob = new_parent.calcProbTillNow() * self.probability\n",
    "    \n",
    "\n",
    "    def calcProbTillNow(self):\n",
    "        \"\"\"Return cached cumulative probability to avoid redundant calculations.\"\"\"\n",
    "        return self.cached_prob\n",
    "    \n",
    "    def change_probability(self,new_probability):\n",
    "        self.cached_prob = self.cached_prob - self.probability\n",
    "        self.probability = new_probability\n",
    "        self.cached_prob += self.probability\n",
    "\n",
    "    # def calcProbTillNow(self):\n",
    "    #   prob = self.probability\n",
    "    #   node = self\n",
    "    #   while node.parent is not None:\n",
    "    #     prob = prob*node.parent.probability\n",
    "    #     node = node.parent\n",
    "    #   return prob    #can make this negative log probability.\n",
    "\n",
    "    def assign_parent_index(self,parent_index):\n",
    "      self.parent_index = parent_index\n",
    "\n",
    "\n",
    "def findProbability(InitialToken, FinalTokens, model,tokenizer):\n",
    "    context = InitialToken.build_Context()\n",
    "    # tokens_50K = model.get_batch_predictions([context], 500)\n",
    "    tokens_50K = generate_token_and_probability(model, tokenizer, [context], top_k=500)\n",
    "    token_dict = {}  # Dictionary to store only the first occurrence of each token\n",
    "\n",
    "    for _,token_id,token, prob in tokens_50K[0]:\n",
    "        # token = token.strip()\n",
    "        if token_id.item() not in token_dict or prob>token_dict[token_id.item()]:  # Store only the first occurrence\n",
    "            token_dict[token_id.item()] = prob\n",
    "    return [token_dict.get(FinalToken.token_id, -math.inf) for FinalToken in FinalTokens]  # Return probability if found, else 0\n",
    "\n",
    "\n",
    "def VITERBI_Lists(state_transition_probmat, initial_state_prob):\n",
    "\n",
    "    viterbi_mat = []\n",
    "    backpointer = []\n",
    "    viterbi_1stLayer = []\n",
    "    for i in range(len(initial_state_prob)):\n",
    "        viterbi_1stLayer.append(float(initial_state_prob[i]))\n",
    "    viterbi_mat.append(viterbi_1stLayer)\n",
    "\n",
    "    for time_step in range(len(state_transition_probmat)):\n",
    "        viterbi_layer = []\n",
    "        backpointer_layer = []\n",
    "        for state in range(len(state_transition_probmat[time_step])):\n",
    "            iteration_vec = [viterbi_mat[time_step][i]+state_transition_probmat[time_step][state][i] for i in range(len(viterbi_mat[time_step]))]\n",
    "\n",
    "            maxval = max(iteration_vec)\n",
    "            maxind = iteration_vec.index(maxval)\n",
    "            viterbi_layer.append(maxval)\n",
    "            backpointer_layer.append(maxind)\n",
    "\n",
    "        viterbi_mat.append(viterbi_layer)\n",
    "        backpointer.append(backpointer_layer)\n",
    "\n",
    "    best_path_prob = max(viterbi_mat[-1])\n",
    "    # max_index = max(range(len(viterbi_mat[-1])), key = lambda i: viterbi_mat[-1][i])\n",
    "    max_index = viterbi_mat[-1].index(best_path_prob)\n",
    "    best_backpointer = max_index\n",
    "    best_path = [best_backpointer]\n",
    "    j = 0\n",
    "    for i in reversed(range(len(state_transition_probmat))):\n",
    "        best_path.append(backpointer[i][best_path[j]])\n",
    "        j += 1\n",
    "    best_path = best_path[::-1]\n",
    "    return best_path, viterbi_mat,best_path_prob\n",
    "def decodePath(best_path,unique_tokens_list,root_string,tokenizer):\n",
    "    resultant_token_ids = []\n",
    "\n",
    "    for i in range(len(best_path)):\n",
    "      #   print(\"decode path: \")\n",
    "      #   print(\"i: \", i)\n",
    "      #   print(unique_tokens_list[i][best_path[i]])\n",
    "      #   if unique_tokens_list[i][best_path[i]] in ['.',':',',','?','!',';']:\n",
    "      #         if (i-1>= 0):\n",
    "      #             resultant_string = resultant_string+unique_tokens_list[i][best_path[i]]\n",
    "      #   elif \"'\" in unique_tokens_list[i][best_path[i]]:\n",
    "      #           resultant_string = resultant_string + unique_tokens_list[i][best_path[i]]\n",
    "      #   else:\n",
    "      #         resultant_string = resultant_string + ' '+ unique_tokens_list[i][best_path[i]]\n",
    "      token_id_to_add=(unique_tokens_list[i][best_path[i]]).token_id\n",
    "      resultant_token_ids.append(token_id_to_add)\n",
    "    print(\"Generated Token Ids: \", resultant_token_ids)\n",
    "    generated_sentence = tokenizer.decode(resultant_token_ids, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "    return root_string + generated_sentence\n",
    "\n",
    "def generate_token_and_probability(model, tokenizer, batch_prompts, probabilityMatrix,uniqueTokensList,max_length=1, top_k=3):\n",
    "    tokenizer.pad_token= tokenizer.eos_token\n",
    "    tokenized_result = tokenizer(batch_prompts, return_tensors=\"pt\",padding = True,truncation = True)\n",
    "    input_ids = tokenized_result[\"input_ids\"].to(model.device)\n",
    "    for i in range(len(input_ids)):\n",
    "        if input_ids[i][-1] == 50256: #end of text token or the pad token\n",
    "\n",
    "    attn_mask = tokenized_result[\"attention_mask\"].to(model.device)\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attn_mask,\n",
    "        max_length=input_ids.size(-1) + max_length,\n",
    "        do_sample=False,  # Greedy decoding\n",
    "        output_scores=True, \n",
    "        return_dict_in_generate=True\n",
    "    )\n",
    "\n",
    "    sequences, scores = outputs.sequences, outputs.scores  # scores will have only one element per batch\n",
    "    predictions = []\n",
    "    # print(\"generated_token_id\",generated_token_id)\n",
    "    for i in range(len(batch_prompts)):\n",
    "        generated_token_id = sequences[i][input_ids.size(-1):].tolist()[0]  # Extract generated token ID\n",
    "        generated_token = tokenizer.decode(generated_token_id, skip_special_tokens=True)\n",
    "       \n",
    "\n",
    "        # Log probabilities of all possible tokens at the generated step\n",
    "        log_probs = torch.nn.functional.log_softmax(scores[0][i], dim=-1)  # scores[0] corresponds to the single generation step\n",
    "        topk_logprobs, topk_ids = log_probs.topk(top_k)  # Get top-k log probabilities\n",
    "        topk_tokens = tokenizer.batch_decode(topk_ids, skip_special_tokens=True)\n",
    "\n",
    "        predictions.append([\n",
    "            (generated_token,token_id,tok,lp.item()) for token_id,tok, lp in zip(topk_ids,topk_tokens, topk_logprobs)\n",
    "        ])\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def check_bad_predictions(text):\n",
    "    bad_patterns = [r'={2,}', r'!{2,}', r'\\?{2,}', r',{2,}', r';{2,}', r'\\|{2,}', r'~{2,}', r'&{2,}', r'-{2,}']\n",
    "    \n",
    "    # Check for unwanted punctuation patterns (two or more consecutive occurrences)\n",
    "    for pattern in bad_patterns:\n",
    "        if re.search(pattern, text):\n",
    "            return True\n",
    "    \n",
    "    # Check for non-ASCII characters\n",
    "    if any(ord(char) > 127 for char in text):  # ASCII characters are in the range 0-127\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def generateIntermediates(root,model,tokenizer,numTokens = 3, loop_runner = 4): \n",
    "  root_token_id = tokenizer.encode(root)\n",
    "  print(\"root_token_id: \", root_token_id)\n",
    "  sentence = SearchTree(root,0,token_id =root_token_id,model = model, tokenizer = tokenizer)\n",
    "  context = []\n",
    "  prob_list = []\n",
    "  num_tokens = numTokens\n",
    "  content = []\n",
    "  probability = []\n",
    "\n",
    "  tokens_50K = generate_token_and_probability(model, tokenizer, [root], top_k=numTokens) #arbitrarly setting +1\n",
    "  children = []\n",
    "  overlap = []\n",
    "  most_common = []\n",
    "  #unique_elements = []   # to store unique elements at each iteration\n",
    "  unique_tokens = set()\n",
    "  probabilityMatrix = []\n",
    "  uniqueTokensList = []\n",
    "  new_content = []\n",
    "  uniqueTokenLength = []\n",
    "\n",
    "  flops_counter = {}\n",
    "  cached_probs = {}\n",
    "  batch_size = 75\n",
    "  holdout_number = 15\n",
    "  for i in range(num_tokens):\n",
    "    _,token_id,context,prob = tokens_50K[0][i]  # Assuming it's structured as a tuple (best_token, token, probability)\n",
    "    context = context.strip()  #This is not the correct solution. I am doing this rather than only leaving one strip command in search tree because I am appending to unique tokens before I am assigning this to search tree. \n",
    "    # context2 = context.strip()\n",
    "    # bad_prediction_checker = check_bad_predictions(context2)\n",
    "    print(\"initial loop\")\n",
    "    print(tokens_50K[0][i])\n",
    "    # if context2:\n",
    "    unique_tokens.add(context)\n",
    "    probability.append(prob)  \n",
    "    context = SearchTree(context,prob,token_id = token_id.item(),model = model,tokenizer = tokenizer,parent =sentence,parent_index = 0)\n",
    "    new_content.append(context)\n",
    "    context.create_child()\n",
    "    uniqueTokensList.append(context)\n",
    "    children.append(context)\n",
    "\n",
    "  content.append(new_content)\n",
    "  previousUniqueLength = num_tokens\n",
    "  #unique_elements.append(unique_tokens)\n",
    "  initialStateProbability = probability \n",
    "  uniqueTokenLength.append(num_tokens)\n",
    "  for i in range(2,loop_runner):\n",
    "    unique_tokens = set()\n",
    "    probability = []\n",
    "    new_content = []\n",
    "    total_predictions = []\n",
    "    previousSetLength = 0\n",
    "    batch_sentences = [child.build_Context() for child in uniqueTokensList]\n",
    "    print(\"batch_sentences: \", batch_sentences)\n",
    "    if len(batch_sentences)>batch_size:\n",
    "        batch_sentences2 = batch_sentences[0:-holdout_number]\n",
    "        batch_predictions =  generate_token_and_probability(model, tokenizer, batch_sentences2, top_k=numTokens)\n",
    "        total_predictions = []\n",
    "        total_predictions.extend(batch_predictions)\n",
    "        batch_predictions1 = generate_token_and_probability(model, tokenizer, batch_sentences[-holdout_number:], top_k=numTokens)\n",
    "        total_predictions.extend(batch_predictions1)\n",
    "    else:\n",
    "        total_predictions = generate_token_and_probability(model, tokenizer, batch_sentences, top_k=numTokens)\n",
    "\n",
    "    for j in range(len(uniqueTokensList)):\n",
    "      print(\"j: \", j)\n",
    "      for s in range(num_tokens):\n",
    "        print(\"s: \", s)\n",
    "        print(\"total_predictions[j][s]: \", total_predictions[j][s])\n",
    "        _,token_id,context,prob = total_predictions[j][s]\n",
    "        context2 = context.strip()\n",
    "        #bad_predictions_checker = check_bad_predictions(context2)\n",
    "        # if context2:\n",
    "        unique_tokens.add(context)   # also this if condition is not the correct solution\n",
    "        context = SearchTree(context,prob,token_id = token_id.item(),model = model,tokenizer = tokenizer,parent = uniqueTokensList[j])   #probably redundant: Because I should only create SearchTree of unique tokens\n",
    "        # context.create_child() Removed this 2/19/2025\n",
    "        if (len(unique_tokens)>previousSetLength):\n",
    "          previousSetLength = len(unique_tokens)\n",
    "          uniqueTokensList.append(context)\n",
    "          new_content.append(context)\n",
    "\n",
    "\n",
    "    #unique_elements.append(unique_tokens) # append the unique tokens list at each iteration to unique_elements list\n",
    "    content.append(new_content) # for storing tokens which will pass to the decode_path function.\n",
    "\n",
    "   \n",
    "    comb_prob = []\n",
    "    for prevToken in uniqueTokensList[:previousUniqueLength]:\n",
    "      comb_prob.append(findProbability(prevToken,uniqueTokensList[previousUniqueLength:], model,tokenizer))\n",
    "    comb_prob = list(itertools.chain(*comb_prob)) # flattening the list\n",
    "\n",
    "    for tokenumber,newToken in enumerate(uniqueTokensList[previousUniqueLength:]):\n",
    "      probs = [comb_prob[a*len(uniqueTokensList[previousUniqueLength:]) + tokenumber] for a in range(len(uniqueTokensList[:previousUniqueLength]))]\n",
    "      probs2 = [probs[i] + uniqueTokensList[:previousUniqueLength][i].calcProbTillNow() for i in range(len(probs))]\n",
    "      #   print(\"parent_prob Up Till now: \",[uniqueTokensList[:previousUniqueLength][i].calcProbTillNow() for i in range(len(probs))])\n",
    "      #   print(\"combined probs: \", probs2)\n",
    "      #   print(\"actual_probs: \", [math.exp(probs2[i]) for i in range(len(probs2))])\n",
    "      if not probs2:\n",
    "        continue\n",
    "      else:\n",
    "        max_value = max(probs2)\n",
    "        max_index = probs2.index(max_value)\n",
    "        newToken.replace_parent(uniqueTokensList[:previousUniqueLength][max_index])\n",
    "        newToken.change_probability(max_value) # just added this 3/27/2025\n",
    "        newToken.assign_parent_index(max_index)\n",
    "      probability.append(probs)\n",
    "    probabilityMatrix.append(probability)\n",
    "    # flops_counter[i-1] = model.get_batch_prediction_count()\n",
    "    #model.reset_batch_prediction_count()\n",
    "\n",
    "\n",
    "    uniqueTokenLength.append(len(uniqueTokensList[previousUniqueLength:]))\n",
    "\n",
    "    previousUniqueLength = len(uniqueTokensList[previousUniqueLength:])\n",
    "    uniqueTokensList = uniqueTokensList[len(uniqueTokensList)-previousUniqueLength:]\n",
    "\n",
    "  return probabilityMatrix, initialStateProbability, content,uniqueTokenLength #, flops_counter\n",
    "def runViterbiTransformerPipeline(rootSentence, numTokens = 3, loop_runner=3):\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "    probabilityMatrix,initialStateProbability,content,uniqueTokenLength = generateIntermediates(rootSentence,model,tokenizer,numTokens = numTokens,loop_runner =loop_runner+1)\n",
    "    best_path,viterbi_mat,best_path_prob = VITERBI_Lists(probabilityMatrix, initialStateProbability)\n",
    "    # print('content: ',content)\n",
    "    decodedString = decodePath(best_path,content,rootSentence,tokenizer)\n",
    "    return decodedString,best_path_prob\n",
    "def runTransformerPipeline(rootSentence,loop_runner = 3):\n",
    "  model = LMHeadModel(\"gpt2\")\n",
    "  prob = 1\n",
    "  finalSentence = rootSentence\n",
    "  for i in range(loop_runner):\n",
    "    tokens_50K = model.get_batch_predictions([finalSentence])\n",
    "\n",
    "    context = tokens_50K[0][0][0]\n",
    "    prob =  prob*tokens_50K[0][0][1]\n",
    "    if context in ['.',':',',','?','!',';'] or \"'\" in context:\n",
    "      finalSentence += context\n",
    "\n",
    "    else:\n",
    "      finalSentence = finalSentence + ' ' + context\n",
    "  return finalSentence,prob\n",
    "\n",
    "def gather_log_probabilities(logits: torch.Tensor, labels: torch.LongTensor) -> torch.Tensor:\n",
    "    \"\"\"Gather log probabilities of the given labels from the logits.\"\"\"\n",
    "    log_probs = torch.nn.functional.log_softmax(logits.float(), dim=-1)\n",
    "    log_probs_labels = log_probs.gather(dim=-1, index=labels.unsqueeze(dim=-1))\n",
    "    return log_probs_labels.squeeze(dim=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d7ff7ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root_token_id:  [40, 2883, 6155, 287, 262, 3952, 13, 628]\n",
      "initial loop\n",
      "('\\n', tensor(198), '\\n', -0.0003575639275368303)\n",
      "initial loop\n",
      "('\\n', tensor(13), '.', -9.920859336853027)\n",
      "initial loop\n",
      "('\\n', tensor(357), ' (', -10.000357627868652)\n",
      "Generated Token Ids:  [198]\n",
      "Mydecoder_text:  I enjoy walking in the park.\n",
      "\n",
      "\n",
      "\n",
      "0.9996424999908258\n",
      "input ids:  tensor([[  40, 2883, 6155,  287,  262, 3952,   13,  628]])\n",
      "greedy_ids:  tensor([[  40, 2883, 6155,  287,  262, 3952,   13,  628,  198,   40]])\n",
      "greedy_text:  I enjoy walking in the park.\n",
      "\n",
      "\n",
      "I\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "Mydecoder_text,decoder_prob = runViterbiTransformerPipeline(\"I enjoy walking in the park.\\n\\n\", loop_runner = 1)\n",
    "print(\"Mydecoder_text: \",Mydecoder_text)\n",
    "print(math.exp(decoder_prob))\n",
    "# -- Greedy decoding\n",
    "tokenized_result = tokenizer(\"I enjoy walking in the park.\\n\\n\",return_tensors = \"pt\")\n",
    "input_ids = tokenized_result[\"input_ids\"]\n",
    "#= Valkyria Chronicles III = \\n \\n The\n",
    "print(\"input ids: \", input_ids)\n",
    "attn_mask = tokenized_result['attention_mask']\n",
    "greedy_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_length= len(input_ids[0])+2,\n",
    "    do_sample=False,  # Greedy\n",
    "    attention_mask = attn_mask\n",
    ")\n",
    "print(\"greedy_ids: \",greedy_ids)\n",
    "greedy_text = tokenizer.decode(greedy_ids[0], skip_special_tokens=True)\n",
    "print(\"greedy_text: \", greedy_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "bb794146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(' I', tensor(314), ' I', -1.1460292339324951), (' I', tensor(340), ' it', -2.455843687057495), (' I', tensor(618), ' when', -3.269465208053589)]\n",
      "[(' I', tensor(314), ' I', -1.2076544761657715), (' I', tensor(340), ' it', -2.942868709564209), (' I', tensor(262), ' the', -3.3185324668884277)]\n",
      "[(' love', tensor(1842), ' love', -2.1047072410583496), (' love', tensor(588), ' like', -2.1849684715270996), (' love', tensor(1101), \"'m\", -2.600236415863037)]\n",
      "[(' the', tensor(262), ' the', -1.2142137289047241), (' the', tensor(661), ' people', -2.660304546356201), (' the', tensor(477), ' all', -2.892085552215576)]\n",
      "[(\"'s\", tensor(338), \"'s\", -0.7176357507705688), (\"'s\", tensor(318), ' is', -2.272681713104248), (\"'s\", tensor(1838), ' makes', -2.7028956413269043)]\n",
      "[(\"'s\", tensor(338), \"'s\", -0.49828243255615234), (\"'s\", tensor(318), ' is', -2.229475975036621), (\"'s\", tensor(1838), ' makes', -3.3147878646850586)]\n",
      "[('\\n', tensor(198), '\\n', -0.008988158777356148), ('\\n', tensor(40), 'I', -6.90969181060791), ('\\n', tensor(464), 'The', -7.71303653717041)]\n",
      "[(' York', tensor(1971), ' York', -0.14046283066272736), (' York', tensor(12255), ' Orleans', -2.8610286712646484), (' York', tensor(8221), ' Jersey', -3.695688247680664)]\n",
      "[(' city', tensor(1748), ' city', -1.8384276628494263), (' city', tensor(1578), ' United', -3.0676679611206055), (' city', tensor(2688), ' West', -3.417391777038574)]\n",
      "[(',', tensor(11), ',', -1.4301437139511108), (',', tensor(13), '.', -1.7115386724472046), (',', tensor(290), ' and', -1.8055938482284546)]\n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "batch_sentences = ['I enjoy walking in the park, but','I enjoy walking in the park, and', 'I enjoy walking in the park. I', 'I enjoy walking in the park and seeing', 'I enjoy walking in the park and it', 'I enjoy walking in the park. It', 'I enjoy walking in the park.\\n', 'I enjoy walking in the streets of New', 'I enjoy walking in the streets of the', 'I enjoy walking in the streets of London']\n",
    "batch_predictions =  generate_token_and_probability(model, tokenizer, batch_sentences, top_k=3)\n",
    "\n",
    "for k in range(len(batch_sentences)):\n",
    "    print(batch_predictions[k])\n",
    "\n",
    "batch_sentences2 = [\"Hi How are you\", \"I am good\"]\n",
    "tokenizer.pad_token= tokenizer.eos_token\n",
    "\n",
    "tokenized_result = tokenizer(batch_sentences2, return_tensors=\"pt\",padding = True,truncation = True)\n",
    "s = tokenized_result[\"input_ids\"][1][-1] == tokenizer.decode(1)\n",
    "if s:\n",
    "    print(\"YES\")\n",
    "\n",
    "print(tokenizer.decode(50256))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05391b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(\"'s\", 0.6075733304023743), ('is', 0.10758479684591293), ('makes', 0.03634175658226013), ('feels', 0.02430432289838791), ('gives', 0.020894328132271767), ('has', 0.01991804502904415), ('was', 0.014500156976282597), ('reminds', 0.010581542737782001), ('really', 0.010477031581103802), ('helps', 0.007916023954749107), ('can', 0.006842233706265688), ('doesn', 0.0053792246617376804), ('keeps', 0.004913671407848597), ('takes', 0.004836901556700468), ('gets', 0.004592712037265301), ('seems', 0.004339112900197506), ('allows', 0.004170210566371679), ('just', 0.003746775444597006), ('would', 0.0037268472369760275), ('means', 0.0032763087656348944), ('does', 0.003250489942729473), ('brings', 0.003102917456999421), ('will', 0.0028180910740047693), ('looks', 0.0025168289430439472), ('provides', 0.002448026556521654), ('always', 0.002390843816101551), ('also', 0.002261810004711151), ('isn', 0.0019119289936497808), ('adds', 0.0015579728642478585), ('definitely', 0.0012738914228975773), ('offers', 0.0012293400941416621), ('creates', 0.0011253380216658115), ('may', 0.0010886697564274073), ('smells', 0.0010847481898963451), ('lets', 0.0010590058518573642), ('fills', 0.001058149733580649), ('might', 0.0009557544253766537), ('sounds', 0.0009538749582134187), ('puts', 0.0009370610932819545), ('leaves', 0.0009292865870520473), (\"'ll\", 0.0009257908095605671), ('shows', 0.000852559634950012), ('made', 0.0008206558413803577), ('works', 0.0008180179283954203), ('could', 0.0008001849637366831), ('attracts', 0.0007901086355559528), ('all', 0.0007152537582442164), ('actually', 0.0007099096546880901), ('felt', 0.0006674634059891105), ('comes', 0.0006615975289605558), ('only', 0.0006517877918668091), ('goes', 0.0006455964175984263), ('opens', 0.0006218928028829396), ('should', 0.0006133030983619392), ('saves', 0.0005950508639216423), ('inspires', 0.0005667281802743673), ('never', 0.0005623479373753071), ('changes', 0.000550193537492305), ('reminded', 0.0005365019896999002), ('wasn', 0.0005121759604662657), ('kind', 0.0004964293329976499), ('becomes', 0.0004934236640110612), ('encourages', 0.0004915824974887073), ('teaches', 0.00048435275675728917), ('certainly', 0.0004841015615966171), ('truly', 0.0004838393651880324), ('gave', 0.0004677989927586168), ('turns', 0.00044122361578047276), ('took', 0.00042649186798371375), ('draws', 0.00040402551530860364), ('must', 0.000379822013201192), ('almost', 0.0003776203084271401), ('usually', 0.00037397051346488297), ('used', 0.00037058215821161866), ('serves', 0.00036877987440675497), ('fits', 0.0003650651196949184), ('requires', 0.0003587286628317088), ('fre', 0.00032395083690062165), ('suits', 0.0003007735940627754), ('still', 0.0002894520293921232), ('places', 0.00028739238041453063), (',', 0.0002871184260584414), (\"'d\", 0.00028489058604463935), ('had', 0.0002821435045916587), ('starts', 0.0002636134158819914), ('sucks', 0.00025729864137247205), ('pays', 0.0002507235622033477), ('tends', 0.00024368726008106023), ('seemed', 0.00024024075537454337), ('got', 0.00023333659919444472), ('drives', 0.0002323081425856799), ('and', 0.00022983124654274434), ('tells', 0.00022868384257890284), ('relax', 0.0002260902983834967), ('ruins', 0.00021759819355793297), ('often', 0.0002167300262954086), ('lights', 0.0002114512026309967), ('lends', 0.00021063486929051578), ('tastes', 0.00020850833971053362), ('even', 0.00020463147666305304)]]\n"
     ]
    }
   ],
   "source": [
    "model2 = LMHeadModel(\"gpt2\")\n",
    "tokens = model2.get_batch_predictions([\"I enjoy walking in the park.\\n\"])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "757b1065",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(' the', ' the', -1.3555811643600464), (' the', ' a', -1.9958933591842651), (' the', ' an', -3.4174938201904297)]]\n",
      "(' the', ' a', -1.9958933591842651)\n",
      "(' the', ' an', -3.4174938201904297)\n"
     ]
    }
   ],
   "source": [
    "answer=generate_token_and_probability(model,tokenizer,[\" Unlike its two predecessors , Valkyria Chronicles III was not released in the west . According to Sega , this was due to\"])\n",
    "print(answer)\n",
    "print(answer[0][1])\n",
    "print(answer[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "894a36e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' = Valkyria Chronicles III = \\n', ' Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles', ' The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While', ' It met with positive sales in Japan , and was praised by both Japanese and western critics . After release , it received', ' = = Gameplay = = \\n', ' As with previous <unk> Chronicles games , Valkyria Chronicles III is a tactical role @-@ playing game where players take control of', \" The game 's battle system , the <unk> system , is carried over directly from <unk> Chronicles . During missions , players select each\", ' Troops are divided into five classes : Scouts , <unk> , Engineers , Lancers and Armored Soldier . Troopers can switch classes by changing their', ' = = Plot = = \\n', ' The game takes place during the Second Europan War . Gallian Army Squad 422 , also known as \" The Nameless \" , are a', ' As the Nameless officially do not exist , the upper echelons of the Gallian Army exploit the concept of plausible deniability', ' Partly due to these events , and partly due to the major losses in manpower Gallia suffers towards the end of', ' = = Development = = \\n', ' Concept work for Valkyria Chronicles III began after development finished on Valkyria Chronicles II in early 2010 , with full development', ' The majority of material created for previous games , such as the <unk> system and the design of maps , was carried', ' = = = Music = = = \\n', ' The music was composed by Hitoshi Sakimoto , who had also worked on the previous Valkyria Chronicles games . When he originally', ' = = = Release = = = \\n', ' In September 2010 , a teaser website was revealed by Sega , hinting at a new Valkyria Chronicles game . In its September', ' Unlike its two predecessors , Valkyria Chronicles III was not released in the west . According to Sega , this was due to', ' = = Reception = = \\n', ' On its day of release in Japan , Valkyria Chronicles III topped both platform @-@ exclusive and multi @-@ platform sales charts . By', ' Famitsu enjoyed the story , and were particularly pleased with the improvements to gameplay . Japanese gaming site Game Watch Impress , despite', \" PlayStation Official Magazine - UK praised the story 's blurring of Gallia 's moral standing , art style , and most points about\", ' In a preview of the TGS demo , Ryan Geddes of IGN was left excited as to where the game would', ' = = Legacy = = \\n', ' Kurt and Riela were featured in the Nintendo 3DS crossover Project X Zone , representing the Valkyria series . Media.Vision would', ' = = = Adaptations = = = \\n', ' Valkyria Chronicles 3 was adapted into a two @-@ episode original video animation series in the same year of its release', \" The anime 's title was inspired by the principle purpose of the Nameless : to suffer in battle for the goals\", \" Two manga adaptations were produced , following each of the game 's main female protagonists Imca and Riela . They were Senjō\", ' = Tower Building of the Little Rock Arsenal = \\n', ' The Tower Building of the Little Rock Arsenal , also known as U.S. Arsenal Building , is a building located in', ' The building receives its name from its distinct octagonal tower . Besides being the last remaining structure of the original Little', ' = = Construction = = \\n', ' The arsenal was constructed at the request of Governor James Sevier Conway in response to the perceived dangers of frontier', ' = = Civil War = = \\n', ' For several years the arsenal , which was owned by the federal government , served as a simple arms depot and was', ' The United States troops at the outposts of the western frontier of the state and in the Indian nation have', ' <unk> M Harrel Telegram , January 31 , 1861 \\n', ' The item was intended simply as a piece of news , but telegraph lines quickly spread the news throughout the state', ' This movement is prompted by the feeling that pervades the citizens of this State that in the present emergency the', ' Perhaps because Abraham Lincoln had not yet been inaugurated as President , Captain Totten received no instructions from his superiors and', ' The governor would take possession of the arsenal in the name of the United States . \\n', ' The soldiers would be allowed safe passage in any direction carrying any personal and public property besides munitions of war', ' The soldiers would be allowed to march away as men leaving under orders , not as conquered and surrendering soldiers . \\n', ' On the morning of February 8 , 1861 , Rector and Totten signed an agreement placing the arsenal in the hands of', ' The Little Rock Arsenal was classified in 1860 as an \" arsenal of deposit , \" meaning that it was simply a warehouse', ' Inside the Little Rock Arsenal after its seizure in February , 1861 , the Confederates inventoried some 10 @,@ 247 weapons , 250 @,@ 000', ' M1822 .69 cal ( flintlock ) 5 @,@ 625 \\n', ' M1822 .69 cal ( percussion @-@ converted ) 53 \\n', ' <unk> .69 cal smoothbore ( percussion ) 357 \\n', ' <unk> .58 cal rifle @-@ muskets 900 \\n', ' <unk> common rifles 125 \\n', ' <unk> rifle ( \" Mississippi Rifle \" ) 54 \\n', ' <unk> <unk> 2 \\n', \" Hall 's carbines 267 \\n\", \" Hall 's rifles ( flintlock ) 2 @,@ 864 \\n\", ' Total 10 @,@ 247 \\n', ' Of this number , approximately 9600 weapons were serviceable , or ready @-@ for @-@ issue . Note there were only 1 @,@ 364 percussion weapons', ' Most of the equipment , arms , and machinery at the Little Rock Arsenal was removed to east of the Mississippi River', ' Major General Thomas C. Hindman , sent to command the district of Arkansas in May , 1862 , found the state nearly destitute', ' \" Machinery was made for manufacturing percussion caps and small arms , and both were turned out in small quantity , but of', ' This ammunition , and that which I brought with me , was rapidly prepared for use at the Laboratory established at the', ' The Confederate ordnance establishment at Little Rock was reactivated in August , 1862 . Looking around for a suitable person to head', ' Lt. Col. Dunnington \\'s \" Returns for the month of August , 1862 , at Little Rock Arsenal , C.S.A. , \" are found', \" The military force at Little Rock under Dunnington 's command consisted of four officers : himself , Major John B. Lockman , Captain\", ' During the month of August , 1862 , the following work was performed : \" <unk> : one pair of musket bullet moulds ; 10 @,@ 000', ' Lt. Col. Dunnington continued to build up his works at Little Rock until November 1862 , when Captain Sanford C. Faulkner', ' A \" Summary of the Work Done for November , 1862 , Little Rock Arsenal \" shows : Fabrication : \\n', ' 75 @,@ 000 buck & ball cartridges - percussion \\n', ' 14 @,@ 000 buck & ball cartridges - flint \\n', ' 275 paper fuzes \\n', ' 117 rounds , 6 @-@ pounder canister shot \\n', ' 130 rounds , 6 @-@ pounder ball shot \\n', ' 96 ammunition packing boxes \\n', ' Repaired : \\n', ' 2 @,@ 236 shotguns and rifles ( repaired mostly for troops in service ) \\n', ' 23 pistols ( repaired mostly for troops in service ) \\n', ' Received & Issued : \\n', ' 752 packages of ordnance and ordnance stores received and mostly issued to troops in service . \\n', ' Repaired and painted : \\n', ' 4 gun carriages \\n', ' Performed : \\n', ' Guard , office , and police duties . \\n', ' Perhaps the most illuminating points of the above \" Summary of Work \" and those for following months are that the standard', ' The \" Summaries of Work done at Little Rock Arsenal , C.S.A. \" continue at about the same pace and scale', ' In 1864 , after Little Rock fell to the Union Army and the arsenal had been recaptured , General Fredrick Steele marched', ' The arsenal was briefly seized once more by Joseph Brooks loyalists during the Brooks @-@ Baxter War of 1874 . \\n', ' = = Decommissioning = = \\n', ' In 1873 , the building was renamed Little Rock Barracks and used as a barracks for married officers and their families', ' In the 1880s , the federal government began closing many small arsenals around the country in favor of smaller ones built', ' = = Æsthetic Club = = \\n', \" In 1894 the Little Rock Æsthetic Club , one of the oldest women 's societies west of the Mississippi River , moved\", ' Except for Æsthetic Club meetings , the Tower Building remained largely unoccupied for almost fifty years and suffered significant deterioration . The', ' = = Public use = = \\n', ' The building and the surrounding park were used for many public purposes throughout the early 20th century . The Tower Building', ' The arsenal was listed in the National Register of Historic Places in 1970 . Due to its association with the Camden', ' In 1942 , the Tower Building was renovated due to the efforts of the Æsthetic Club , Little Rock philanthropist Frederick W', \" In 1997 , the Museum of Science and Natural History merged with the Little Rock Children 's Museum , which had been\", ' = Cicely Mary Barker = \\n', ' Cicely Mary Barker ( 28 June 1895 – 16 February 1973 ) was an English illustrator best known for a series of fantasy', ' Barker was a devout Anglican , and donated her artworks to Christian fundraisers and missionary organizations . She produced a few Christian', ' Barker was equally proficient in watercolour , pen and ink , oils , and pastels . Kate Greenaway and the Pre @-@ Raphaelites were the', ' = = Biography = = \\n', ' = = = Early life = = = \\n', ' Barker was born the second daughter and youngest child of Walter Barker , a partner in a seed supply company and', ' The family of four was moderately well off , and belonged to the lower end of the upper middle class . A', ' = = = Art education and first professional work = = = \\n', ' Barker took correspondence courses in art , probably until about 1919 . In 1908 at 13 years , she entered an evening class', ' In 1911 , Raphael Tuck & Sons bought four of Barker \\'s \" little drawings \" for half a sovereign , and published them as', ' Following her father ’ s death in June 1912 , the seventeen @-@ year @-@ old Barker submitted art and poetry to My Magazine', ' = = = Flower Fairies of the Spring , 1923 = = = \\n', ' Fairies became a popular theme in art and literature in the early 20th century following the releases of The Coming', ' In 1923 , Barker sent her flower fairy paintings to various publishers . Blackie paid £ 25 for 24 paintings with accompanying verses', ' = = = The Waldrons = = = \\n', ' In 1924 , the family moved into a four @-@ level , semi @-@ detached Victorian house at 23 The Waldrons . Barker had a', ' The children in the kindergarten modelled for the Flower Fairies until the kindergarten closed in 1940 . In an interview in', ' = = = Middle years = = = \\n', ' In the late 1920s , Barker began to doubt she was doing enough for the church and considered focusing solely on', ' Barker continued to attend evening classes at the Croydon Art School between the 1920s and the 1940s , eventually receiving a', \" In 1940 , the Barker 's live @-@ in maid retired , and Dorothy Barker closed her school at the back of the\", ' = = = Later life and death = = = \\n', \" Barker 's mother died in 1960 , and , in 1961 , Barker moved from 23 The Waldrons to 6 <unk> Avenue in\", ' Barker died at Worthing Hospital on 16 February 1973 , aged 77 years . Two funeral services were held – one in Storrington', ' = = Art = = \\n', ' Barker worked principally in watercolor with pen @-@ and @-@ ink , but she was equally competent in black @-@ and @-@ white , in oils', \" Kate Greenaway was a childhood favorite and an influence on her art . Barker 's child subjects wear nostalgic clothing as\", ' The Pre @-@ Raphaelites were a strong , lifelong influence on Barker . She once indicated , \" I am to some extent influenced by', ' = = = Depictions of children = = = \\n', \" Barker 's sketches , drawings , and paintings of children were given to friends or to the parents of the subjects , donated\", ' = = = Christian @-@ themed works = = = \\n', ' Barker was a devout Christian , and produced religious @-@ themed works throughout her life . She published eight postcards and five guardian', \" Religious @-@ themed books include The Children 's Book of Hymns ( 1929 ) and He Leadeth Me ( 1933 ) , the latter written in\", ' = = Works = = \\n', ' = = = Cards = = = \\n', ' Picturesque Children of the Allies ; J. Salmon , 1916 \\n', ' National Mission ; Society for the Preservation of Christian Knowledge , 1916 \\n', \" Shakespeare 's Boy Characters ; C. W. Faulkner , 1917 \\n\", \" Shakespeare 's Girl Characters ; C. W. Faulkner , 1920 \\n\", ' Seaside Holiday ; J. Salmon , 1918 , 1921 \\n', ' Elves and Fairies ; S. Harvey , 1918 \\n', ' Guardian Angel ; Society for the Preservation of Christian Knowledge , 1923 \\n', \" Christmas cards ; Girls ' Friendly Society , 1920s , 1930s \\n\", ' Christmas cards ( US ) ; Barton @-@ Colton , 1920s , 1930s \\n', ' Beautiful Bible Pictures ; Blackie , 1932 \\n', ' = = = Books = = = \\n', ' Flower Fairies of the Spring ; Blackie , 1923 \\n', ' Spring Songs with Music ; Blackie , 1923 \\n', ' Flower Fairies of the Summer ; Blackie , 1925 \\n', ' Child Thoughts in Picture and Verse ( by M. K. Westcott ) ; Blackie , 1925 \\n', ' Flower Fairies of the Autumn ; Blackie , 1926 \\n', ' Summer Songs with Music ; Blackie , 1926 \\n', ' The Book of the Flower Fairies ; Blackie , 1927 \\n', ' Autumn Songs with Music ; Blackie , 1927 \\n', ' Old Rhymes for All Times ; Blackie , 1928 \\n', ' The Children ’ s Book of Hymns ; Blackie , 1929 ; rep . 1933 \\n', ' Our Darling ’ s First Book ( written in collaboration with Dorothy Barker ) ; Blackie , 1929 \\n', ' The Little Picture Hymn Book ; Blackie , 1933 \\n', ' Rhymes New and Old ; Blackie , 1933 \\n', ' A Flower Fairy Alphabet ; Blackie , 1934 \\n', ' A Little Book of Old Rhymes ; Blackie , 1936 \\n', ' He Leadeth Me ( written in collaboration with Dorothy Barker ) ; Blackie , 1936 \\n', ' A Little Book of Rhymes New and Old ; Blackie , 1937 \\n', ' The Lord of the Rushie River ; Blackie , 1938 \\n', ' Flower Fairies of the Trees ; Blackie , 1940 \\n', ' When Spring Came In at the Window ; Blackie , 1942 \\n', ' A Child ’ s Garden of Verses ( Robert Louis Stevenson ) ; Blackie , 1944 \\n', ' Flower Fairies of the Garden ; Blackie , 1944 \\n', ' Groundsel and Necklaces ; Blackie , 1946 ; reprinted as Fairy Necklaces \\n', ' Flower Fairies of the Wayside ; Blackie , 1948 \\n', ' Flower Fairies of the Flowers and Trees ; Blackie , 1950 \\n', ' Lively Stories ; Macmillan , 1954 \\n', ' The Flower Fairy Picture Book ; Blackie , 1955 \\n', ' Lively Numbers ; Macmillan , 1957 \\n', ' Lively Words ; Macmillan , 1961 . \\n', ' The Sand , the Sea and the Sun ; Gibson , 1970 \\n', ' = = = = Posthumously published = = = = \\n', ' Flower Fairies of the Winter ; Blackie , 1985 \\n', ' Simon the Swan ; Blackie , 1988 \\n', ' Flower Fairies of the Seasons ; <unk> / Blackie , 1988 \\n', ' A Little Book of Prayers and Hymns ; Frederick Warne , 1994 \\n', ' A Flower Fairies Treasury ; Frederick Warne , 1997 \\n', ' <unk> ; Frederick Warne , 2005 \\n', ' Wild Cherry Makes A Wish ; ( collaboration with Pippa Le Quesne ) Frederick Warne , 2006 \\n', ' How to find Flower Fairies ; Frederick Warne , 2007 \\n', ' Return to <unk> ; Frederick Warne , 2008 \\n', ' = = = Book covers = = = \\n', ' A New Epiphany ; Society for the Preservation of Christian Knowledge , 1919 \\n', ' 43 Annuals ; Blackie , 1920s , 1930s \\n', ' = = = Religious works = = = \\n', \" St. Cecily 's Garden ; 1920 \\n\", \" Cradle roll design ; St. Edmund 's , Pitlake , 1922 \\n\", \" Banner design ; St. Mary 's , <unk> , 1923 \\n\", ' The Feeding of the Five Thousand ; reredos triptych , chapel at Penarth , Wales ; 1929 \\n', \" The Parable of the Great Supper ; triptych , St. George 's chapel , Waddon \\n\", \" The Seven Sacraments ; baptismal font panels , St. Andrew 's , Croydon \\n\", ' St. John the Baptist ; central banner panel , <unk> church , 1943 \\n', ' Lettering , sword , and shield ; mount for a list of men and woman serving in the Forces , St. Andrews , Croydon , 1943', ' <unk> rolls ; St. Andrews , Croydon , 1948 , 1962 \\n', \" The font in St Andrew 's Church , South Croydon \\n\", ' Out of Great Tribulation ; memorial chapel , Norbury <unk> church , 1948 \\n', \" I Am Among You As He That <unk> ; stained glass window design , St. Edmund 's , Pitlake , 1962 \\n\", \" = Gambia women 's national football team = \\n\", \" The Gambia women 's national football team represents the Gambia in international football competition . The team , however , has not competed\", ' = = The team = = \\n', \" In 1985 , few countries had women 's national football teams . While the sport gained popularity worldwide in later decades , the\", \" The country did not have a FIFA @-@ recognised youth national team until 2012 , when the Gambia under @-@ 17 women 's\", \" The Gambia also has an under @-@ 19 team that was to play in the African Women 's U @-@ 19 Championship\", ' = = Background and development = = \\n', \" The development of women 's football in Africa faces several challenges , including limited access to education , poverty amongst women , inequalities\", \" Gambia 's national football association was founded in 1952 , and became affiliated with FIFA in 1968 . Football is the most\", ' = Plain maskray = \\n', ' The plain maskray or brown stingray ( Neotrygon annotata ) is a species of stingray in the family Dasyatidae . It is found', ' Benthic in nature , the plain maskray feeds mainly on caridean shrimp and polychaete worms , and to a lesser extent on', ' = = Taxonomy and phylogeny = = \\n', ' The first scientific description of the plain maskray was authored by Commonwealth Scientific and Industrial Research Organisation ( CSIRO ) researcher Peter', ' In a 2012 phylogenetic analysis based on mitochondrial and nuclear DNA , the plain maskray and the Ningaloo maskray ( N. <unk', ' = = Description = = \\n', ' The pectoral fin disc of the plain maskray is thin and diamond @-@ shaped with narrowly rounded outer corners , measuring 1', ' The tail is short , barely exceeding the length of the disc when intact , and has a broad and flattened base', ' = = Distribution and habitat = = \\n', ' The plain maskray inhabits the continental shelf of northern Australia from the Wellesley Islands in Queensland to the Bonaparte Archipelago', ' = = Biology and ecology = = \\n', ' The plain maskray generally hunts at the surface of the bottom substrate , rather than digging for prey . Its diet consists', ' Like other stingrays , the plain maskray is viviparous with the developing embryos sustained to term by histotroph ( \" uterine milk \" ) produced', ' = = Human interactions = = \\n', ' The main conservation threat to the plain maskray is incidental capture by commercial bottom trawl fisheries . In the present day', ' = 2011 – 12 Columbus Blue Jackets season = \\n', \" The 2011 – 12 Columbus Blue Jackets season was the team 's 12th season in the National Hockey League ( NHL ) . The\", ' The Blue Jackets began the year with the worst start in franchise history and the worst by any team in', ' The team was involved in a controversial loss to the Los Angeles Kings , when the Staples Center clock appeared to', ' = = Off @-@ season = = \\n', \" In the off @-@ season the Blue Jackets ' approach to building their team changed , moving from a team of young developing\", ' Columbus also traded former first round draft pick Nikita Filatov to the Ottawa Senators for a third @-@ round pick in', ' = = Regular season = = \\n', ' = = = October – December = = = \\n', ' After the first five games , all losses , Jeff Carter suffered a broken foot that kept him out of the line', ' During the same time frame as the Hitchcock rumors , goaltender Curtis Sanford returned from his groin injury on November 13', ' = = = January – February = = = \\n', ' With the losing continuing , more rumors began to surface . Unlike before , the rumors were about player moves rather than coaching', ' At the halfway point of the season , with the Blue Jackets barely into double digit wins with an 11 – 25', ' Following the break , the Blue Jackets were on the road playing the Los Angeles Kings , and with the score tied', ' Two weeks prior to the NHL trade deadline , Columbus announced that unlike earlier in the season , they would listen to', ' = = = March – April = = = \\n', ' Columbus started March with a 2 – 0 shutout against the Colorado Avalanche . They proceeded to win their next game against', ' Three days later , on March 28 , goaltender Steve Mason was injured in the morning skate when a shot from Colton', ' The Blue Jackets struggled in shorthanded situations , allowing the most power @-@ play goals in the League , with 64 , and having', ' = = Post @-@ season = = \\n', ' Finishing with the worst record in the NHL , Columbus had the best chance of receiving the first overall pick in', ' A month later , on May 14 , the Blue Jackets announced that Richards would remain as head coach and signed him', ' = = Standings = = \\n', ' Since being founded as an expansion team , the Blue Jackets have played in the Central Division of the Western Conference', ' Divisions : CE – Central , NW – Northwest , PA – Pacific \\n', \" bold - qualified for playoffs , y – Won division , p – Won Presidents ' Trophy ( best record in NHL ) \\n\", ' = = Schedule and results = = \\n', ' = = = Pre @-@ season = = = \\n', ' = = = Regular season = = = \\n', ' Green background indicates win ( 2 points ) . \\n', ' Red background indicates regulation loss ( 0 points ) . \\n', ' Silver background indicates overtime / shootout loss ( 1 point ) . \\n', ' = = Player statistics = = \\n', \" In ice hockey , a combination of a player 's goals and assists are collectively called points . Penalty minutes are the\", ' = = = Skaters = = = \\n', ' Note : Pos \\n', ' = Position ; GP = \\n', ' Games played in ; G \\n', ' = Goals ; A = \\n', ' Assists ; Pts \\n', ' = Points ; PIM = \\n', ' Penalty minutes ; + / - = Plus / minus \\n', ' = = = Goaltenders = = = \\n', ' Note : GP \\n', ' = Games Played ; TOI = \\n', ' Time On Ice ( minutes ) ; W \\n', ' = Wins ; L = \\n', ' Losses ; OT \\n', ' = Overtime Losses ; GA = \\n', ' Goals Against ; GAA = Goals Against Average ; SA = Shots Against ; SV \\n', ' = Saves ; Sv % = \\n', ' Save Percentage ; SO = Shutouts \\n', ' † Denotes player spent time with another team before joining Blue Jackets . Stats reflect time with the Blue Jackets only . ‡ Traded', ' = = Milestones = = \\n', ' When Mason was injured in warm @-@ ups late in the year , Columbus was without an active goaltender on their roster', ' = = Transactions = = \\n', ' During the off @-@ season the Blue Jackets parted ways with defensemen Jan Hejda , Anton Stralman , Sami <unk> and Mike Commodore', ' = Gregorian Tower = \\n', ' The Gregorian Tower ( Italian : Torre <unk> ) or Tower of the Winds ( Italian : Torre dei Venti ) is a round tower located', ' = = Early history = = \\n', ' The first stage of building of the tower , as recorded by Leo XIII in his motu proprio Ut <unk> of', ' = = Second stage = = \\n', ' The second stage of construction in the 17th and 18th centuries , when the tower was under the charge of the', ' = = Third stage = = \\n', ' The revival of the observatory on the Gregorian Tower was initiated by the <unk> Francesco Denza with the approval of', ' = = Fourth stage = = \\n', ' The fourth stage involved remedying the problem of communicating between the two towers during the time of Pope Pius X', ' = = Features = = \\n', ' The tower had two floors and a mezzanine . On the first floor was the famous Sundial Room or Meridian Room', ' The Sundial Room , also called the Meridian Hall , was once the residence of Queen Christina of Sweden , then newly converted', ' The interior walls and ceiling of the hall were richly decorated , in some cases with gaudy frescoes of the hills', \" = There 's Got to Be a Way = \\n\", ' \" There \\'s Got to Be a Way \" is a song by American singer and songwriter Mariah Carey from her self', ' = = Background and release = = \\n', ' \" There \\'s Got to Be a Way \" was written by Mariah Carey and Ric Wake for Carey \\'s self @-@ titled', ' = = Composition = = \\n', ' \" There \\'s Got to Be a Way \" is an R & B @-@ pop music song with elements of gospel . The theme', ' = = Critical reception = = \\n', ' Music critic Robert Christgau felt that Carey was being too political in her \" brave , young , idealistic attack \" on war and', ' = = Music video = = \\n', ' The accompanying music video begins with a shot of an empty street , followed by clips of disadvantaged and poorer members', ' = = Track listings = = \\n', ' \" There \\'s Got to Be a Way \" ( Original album version ) – 4 : 52 \\n', ' \" There \\'s Got to Be a Way \" ( 7 \" remix ) \\n', ' \" There \\'s Got to Be a Way \" ( 12 \" remix ) \\n', ' \" There \\'s Got to Be a Way \" ( Alternative Vocal Dub Mix ) \\n', ' = = Charts = = \\n', ' = Nebraska Highway 88 = \\n', ' Nebraska Highway 88 ( N @-@ 88 ) is a highway in northwestern Nebraska . It has a western terminus at Wyoming Highway 151', ' = = Route description = = \\n', ' N @-@ 88 starts at the Nebraska – Wyoming state line in Banner County , where WYO 151 ends , and travels northeast . The', ' = = History = = \\n', ' N @-@ 88 was unofficially designated around 1937 , connecting from N @-@ 29 , to N @-@ 86 and N @-@ 19 in Bridgeport . The', ' = = Major intersections = = \\n', ' = USS Atlanta ( 1861 ) = \\n', ' Atlanta was a casemate ironclad that served in the Confederate and Union Navies during the American Civil War . She was', ' = = Description and career as Fingal = = \\n', \" Fingal was designed and built as a merchantman by J & G Thomson 's Clyde Bank Iron Shipyard at Govan in\", \" The ship briefly operated between Glasgow and other ports in Scotland for Hutcheson 's West Highland Service before she was\", ' While Fingal was discharging her cargo , Bulloch went to Richmond to confer with Stephen Mallory , Secretary of the Navy . Mallory', ' = = As Atlanta = = \\n', ' The brothers Asa and Nelson Tift received the contract to convert the blockade runner into an ironclad in early 1862', ' The armor of the casemate was angled at 30 ° from the horizontal and made from two layers of railroad rails', ' The rectangular casemate was pierced with eight narrow gun ports , one each at the bow and stern and three along', ' On 31 July 1862 , under the command of Lieutenant Charles H. McBlair , Atlanta conducted her sea trials down the Savannah', ' Attempts were made to fix the problems and were at least partially successful in stopping many of the leaks . The', ' Webb demonstrated his aggressiveness when he attempted to sortie on the first spring tide ( 30 May ) after taking command , but', ' In the early evening of 15 June , Webb began his next attempt by passing over the lower obstructions in the', ' A lookout aboard Weehawken spotted Atlanta at 04 : 10 on the morning of 17 June . When the latter ship closed', ' = = In the Union Navy = = \\n', ' Atlanta was easily pulled free by the Union ships and she reached Port Royal under her own power . Not badly', ' After the end of the war in April , Atlanta was decommissioned in Philadelphia on 21 June 1865 and placed in', ' = Jacqueline Fernandez = \\n', ' Jacqueline Fernandez ( born 11 August 1985 ) is a Sri Lankan actress , former model , and the winner of the 2006 Miss', \" While on a modelling assignment in India in 2009 , Fernandez successfully auditioned for Sujoy Ghosh 's fantasy drama Aladin , which\", ' One of the most popular actresses in India , she was the recipient of the IIFA Award for Star Debut of', ' = = Early life and modeling career = = \\n', ' Fernandez was born on 11 August 1985 , in Manama , Bahrain , and was raised in a multi @-@ ethnic family . Her father', ' According to Fernandez , she had aspired to become an actress at a young age and fantasized about becoming a Hollywood', ' = = Acting career = = \\n', ' = = = 2009 – 2013 = = = \\n', ' In 2009 , Fernandez traveled to India for a modeling assignment . She studied acting under the mentorship of theatre director Barry', ' In 2010 , Fernandez appeared opposite Deshmukh in the science fiction romantic comedy Jaane Kahan Se Aayi Hai . She was cast', \" Mahesh Bhatt 's thriller Murder 2 was Fernandez 's first commercial success and marker a turning point in her career\", \" Fernandez 's first release of 2013 was Race 2 , an ensemble action thriller ( alongside Saif Ali Khan , John Abraham , Deepika\", ' = = = 2014 – present = = = \\n', \" In 2014 , Fernandez appeared in Sajid Nadiadwala 's directorial debut — the action film Kick , a remake of a 2009 Telugu\", \" In 2015 , Fernandez featured in Vicky Singh 's Roy , a romantic thriller , which critic Sarita A. Tanwar described as a\", \" Karan Malhotra 's action drama Brothers was Fernandez 's next release . Co @-@ starring alongside Akshay Kumar and Sidharth Malhotra , Fernandez\", ' As of September 2015 , Fernandez has several projects in various stages of production . She has completed shooting for Chandran <unk', ' = = Personal life and other work = = \\n', ' Fernandez shares a close bond with her family , and admits to missing being around them . She says : \" I miss them', \" In 2008 , Fernandez started dating Bahraini prince Hassan bin Rashid Al Khalifa , whom she met at a mutual friend 's\", ' In addition to acting in films , Fernandez has supported charitable organisations and a number of causes . In 2011 , on the', ' Fernandez has participated in several concert tours and televised award ceremonies . In 2013 , she performed at the Temptations Reloaded in', ' = = In the media = = \\n', ' In the early 2013 , Fernandez became the ambassador for HTC One , which she endorses in India . She was the face', ' In 2008 and 2011 , Fernandez featured in the UK magazine Eastern Eye \\'s \" World \\'s Sexiest Asian Women \" list , ranking', ' = = Filmography = = \\n', ' = = TV Appearances = = \\n', ' = = Awards = = \\n', ' = John Cullen = \\n', ' Barry John Cullen ( born August 2 , 1964 ) is a Canadian former professional ice hockey centre who played in the National', ' His career was halted in 1997 when he was diagnosed with Non @-@ Hodgkin lymphoma . He attempted a brief comeback in', ' = = Early life = = \\n', ' Cullen was born in <unk> @-@ Ontario on August 2 , 1964 . He is one of six children of Barry and Loretta', \" He idolized his elder brother Terry , who was considered a top NHL prospect until Terry 's career was ended when\", ' At the same time , his mother Loretta was diagnosed with skin cancer . Following her death early in his freshman year', ' = = Playing career = = \\n', ' Cullen was a standout with BU ; he was named the East Coast Athletic Conference Rookie of the Year in 1983', ' Passed over in the Entry Draft , Cullen was finally selected by the Buffalo Sabres in the 1986 NHL Supplemental Draft', ' = = = National Hockey League = = = \\n', ' Cullen made his NHL debut in 1988 – 89 , appearing in 79 games with the Penguins and scoring 49 points . He', \" The Penguins ' needs led them to complete a blockbuster trade on March 1 , 1991 . Cullen was sent to the Hartford\", \" In Hartford , Cullen worked to overcome the team 's fans ' disappointment at losing Francis . The Hartford fans initially booed him\", \" Midway through the 1992 – 93 NHL season , the Whalers sent Cullen to the Toronto Maple Leafs for Toronto 's second\", ' Cullen enjoyed immediate success with linemates Shawn Burr and Alexander <unk> as the trio combined to score 130 points and', ' = = = Cancer and comeback = = = \\n', \" After two months of quietly dealing with his symptoms , Cullen 's wife finally called team trainers and asked them to\", ' On one day during his treatments , as his wife was wheeling him down a hospital corridor , Cullen went into cardiac', ' The Lightning signed Cullen to a one @-@ year , $ 500 @,@ 000 contract for the 1998 – 99 season . He played his first', \" Cullen appeared in four of the Lightning 's first eight games , but it was evident that he had lost much\", ' However , a bout of bronchitis led Cullen to fear that his cancer had returned . Tests came back negative , but after', ' Former Lightning head coach Terry Crisp has stated publicly that Cullen was a player that stood out as something special', ' = = Off the ice = = \\n', ' Cullen and his wife Valerie have three daughters , Kennedy and twins <unk> and <unk> . Unwilling to spend so much time', \" Cullen 's battle with cancer inspired Timm Harmon of the Moffitt Cancer Centre to partner with the Lightning to raise\", ' Prior to marrying his wife Valerie , John dated Carolyn Bessette the future wife of John F. Kennedy , Jr . The two', ' = = Career statistics = = \\n', ' = = = Regular season and playoffs = = = \\n', ' = = = International = = = \\n', ' = = Awards = = \\n', ' Cullen is the namesake of the John Cullen Award , previously given to key IHL players . \\n', ' = SMS Erzherzog Ferdinand Max = \\n', ' For the ironclad present at the Battle of Lissa of the same name , see SMS Erzherzog Ferdinand Max ( 1865 ) . \\n', ' SMS Erzherzog Ferdinand Max ( German : \" His Majesty \\'s ship Archduke Ferdinand Max \" ) was a pre @-@ dreadnought battleship built by the', ' For most of World War I , Erzherzog Ferdinand Max remained in her home port of Pola , in present @-@ day Croatia', ' = = Design = = \\n', ' Erzherzog Ferdinand Max displaced 10 @,@ 472 long tons ( 10 @,@ 640 t ) . She was 414 feet 2 inches ( 126 @.@ 2 m', ' Erzherzog Ferdinand Max carried a primary armament of four 24 @-@ centimeter ( 9 @.@ 4 in ) / 40 caliber guns in two twin', ' = = Service history = = \\n', ' At the outbreak of World War I , Erzherzog Ferdinand Max was in the III division of the Austrian @-@ Hungarian battle', ' A major mutiny among crews of the armored cruisers stationed in Cattaro , including Sankt Georg and Kaiser Karl VI , began', ' Near the end of World War I , the Erzherzog Karl @-@ class battleships were handed over to the newly formed State', ' = Ancient Egyptian deities = \\n', ' Ancient Egyptian deities are the gods and goddesses worshipped in ancient Egypt . The beliefs and rituals surrounding these gods formed', \" The gods ' complex characteristics were expressed in myths and in intricate relationships between deities : family ties , loose groups and hierarchies\", ' In different eras , various gods were said to hold the highest position in divine society , including the solar deity Ra', ' Gods were assumed to be present throughout the world , capable of influencing natural events and the course of human lives', ' = = Definition = = \\n', ' The beings in ancient Egyptian tradition who might be labeled as deities are difficult to count . Egyptian texts list the', ' The Egyptian language \\'s terms for these beings were nṯr , \" god \" , and its feminine form <unk> , \" goddess \" . Scholars have tried', ' The Egyptians distinguished <unk> , \" gods \" , from <unk> , \" people \" , but the meanings of the Egyptian and the English terms do not', ' Confronting these blurred distinctions between gods and other beings , scholars have proposed various definitions of a \" deity \" . One widely accepted', ' = = Origins = = \\n', ' The first written evidence of deities in Egypt comes from the Early Dynastic Period ( c . 3100 – 2686 BC ) . Deities must', ' Many Egyptologists and anthropologists have suggested theories about how the gods developed in these early times . Gustave <unk> , for instance', ' Predynastic Egypt originally consisted of small , independent villages . Because many deities in later times were strongly tied to particular towns', ' The final step in the formation of Egyptian religion was the unification of Egypt , in which rulers from Upper Egypt', ' New gods continued to emerge after this transformation . Some important deities like Isis and Amun are not known to have', ' Through contact with neighboring civilizations , the Egyptians also adopted foreign deities . <unk> , who is first mentioned in the Old Kingdom', ' = = Characteristics = = \\n', \" Modern knowledge of Egyptian beliefs about the gods is mostly drawn from religious writings produced by the nation 's scribes\", ' = = = Roles = = = \\n', ' Most Egyptian deities represent natural or social phenomena . The gods were generally said to be immanent in these phenomena — to', ' Not all aspects of existence were seen as deities . Although many deities were connected with the Nile , no god personified', ' The roles of each deity were fluid , and each god could expand its nature to take on new characteristics . As', ' The deities with the most limited and specialized domains are often called \" minor divinities \" or \" demons \" in modern writing , although', ' = = = Behavior = = = \\n', ' Divine behavior was believed to govern all of nature . Except for the few deities who disrupted the divine order , the', \" The gods ' actions in the present are described and praised in hymns and funerary texts . In contrast , mythology mainly concerns\", ' In myth , the gods behave much like humans . They feel emotion ; they can eat , drink , fight , weep , sicken , and die', ' The first divine act is the creation of the cosmos , described in several creation myths . They focus on different gods', ' A recurring theme in these myths is the effort of the gods to maintain maat against the forces of disorder', ' = = = Locations = = = \\n', ' Gods were linked with specific regions of the universe . In Egyptian tradition , the world includes the earth , the sky , and', ' In the time after myth , most gods were said to be either in the sky or invisibly present within the', ' = = = Names and epithets = = = \\n', ' In Egyptian belief , names express the fundamental nature of the things to which they refer . In keeping with this belief', ' The Egyptians also devised false etymologies giving more meanings to divine names . A passage in the Coffin Texts renders the', ' The gods were believed to have many names . Among them were secret names that conveyed their true natures more profoundly', ' In addition to their names , gods were given epithets , like \" possessor of splendor \" , \" ruler of Abydos \" , or \" lord of the', ' = = = Relationships = = = \\n', \" Egyptian deities are connected in a complex and shifting array of relationships . A god 's connections and interactions with other\", ' Family relationships are a common type of connection between gods . Deities often form male and female pairs , reflecting the importance', ' Other divine groups were composed of deities with interrelated roles , or who together represented a region of the Egyptian mythological', ' Nine , the product of three and three , represents a multitude , so the Egyptians called several large groups \" <unk> \" , or sets', ' This divine assemblage had a vague and changeable hierarchy . Gods with broad influence in the cosmos or who were mythologically', ' = = = Manifestations and combinations = = = \\n', ' The gods were believed to manifest in many forms . The Egyptians had complex conception of the human soul , consisting of', ' Nationally important deities gave rise to local manifestations , which sometimes absorbed the characteristics of older regional gods . Horus had many', ' Gods were combined with each other as easily as they were divided . A god could be called the ba of', ' = = = The Aten and possible monotheism = = = \\n', ' In the reign of Akhenaten ( c . 1353 – 1336 BC ) in the mid @-@ New Kingdom , a single solar deity , the Aten', ' = = = Unity of the divine in traditional religion = = = \\n', ' Scholars have long debated whether traditional Egyptian religion ever asserted that the multiple gods were , on a deeper level , unified', ' In 1971 , Erik Hornung published a study rebutting these views . He points out that in any given period many deities', \" Hornung 's arguments have greatly influenced other scholars of Egyptian religion , but some still believe that at times the gods\", ' = = Descriptions and depictions = = \\n', \" Egyptian writings describe the gods ' bodies in detail . They are made of precious materials ; their flesh is gold , their bones\", ' Most gods were depicted in several ways . Hathor could be a cow , cobra , lioness , or a woman with bovine horns', \" Certain features of divine images are more useful than others in determining a god 's identity . The head of a\", ' The forms in which the gods are shown , although diverse , are limited in many ways . Many creatures that are widespread', ' The basic anthropomorphic form varies . Child gods are depicted nude , as are some adult gods when their procreative powers are', ' = = Interactions with humans = = \\n', ' = = = Relationship with the pharaoh = = = \\n', ' In official writings , pharaohs are said to be divine , and they are constantly depicted in the company of the deities', \" However much it was believed , the king 's divine status was the rationale for his role as Egypt 's representative\", ' = = = Presence in the human world = = = \\n', ' Although the Egyptians believed their gods to be present in the world around them , contact between the human and divine', ' The ba of a god was said to periodically leave the divine realm to dwell in the images of that', ' Temples , where the state rituals were carried out , were filled with images of the gods . The most important temple image', ' To insulate the sacred power in the sanctuary from the impurities of the outside world , the Egyptians enclosed temple sanctuaries', ' = = = Intervention in human lives = = = \\n', ' Egyptian gods were involved in human lives as well as in the overarching order of nature . This divine influence applied', ' Thoth , as the overseer of time , was said to allot fixed lifespans to both humans and gods . Other gods were', ' Humans had free will to ignore divine guidance and the behavior required by maat , but by doing so they could', ' Egyptian texts take different views on whether the gods are responsible when humans suffer unjustly . Misfortune was often seen as', ' = = = Worship = = = \\n', ' Official religious practices , which maintained maat for the benefit of all Egypt , were related to , but distinct from , the religious', ' Official religion involved a variety of rituals , based in temples . Some rites were performed every day , whereas others were festivals', ' Festivals often involved a ceremonial procession in which a cult image was carried out of the temple in a barque', ' Personal interaction with the gods took many forms . People who wanted information or advice consulted oracles , run by temples , that', ' Prayer and private offerings are generally called \" personal piety \" : acts that reflect a close relationship between an individual and a', ' The worship of some Egyptian gods spread to neighboring lands , especially to Canaan and Nubia during the New Kingdom , when', ' Under the Greek Ptolemaic Dynasty and then Roman rule , Greeks and Romans introduced their own deities to Egypt . These newcomers', ' Temples and cults in Egypt itself declined as the Roman economy deteriorated in the third century AD , and beginning in', ' = South of Heaven = \\n', ' South of Heaven is the fourth studio album by American thrash metal band Slayer . Released on July 5 , 1988 , the', \" South of Heaven was Slayer 's second album to enter the Billboard 200 , and its last to be released by\", \" In order to offset the pace of the group 's previous album , Slayer deliberately slowed down the album 's tempo\", ' = = Background = = \\n', ' South of Heaven was recorded in Los Angeles , California with Reign in Blood producer Rick Rubin . PopMatters reviewer Adrien Begrand', ' King has since been critical of his performance , which he describes as his \" most lackluster . \" King attributes this to the', ' Judas Priest \\'s \" Dissident Aggressor \" is the only cover version to appear on a Slayer studio album . The song was', ' = = Photography and illustration = = \\n', ' Artist Larry Carroll and Illustrator Howard Schwartzberg designed the cover artwork for South of Heaven , having designed the artwork for', ' = = Critical reception = = \\n', ' South of Heaven was released on July 5 , 1988 , and was the final Slayer album distributed via Def Jam Records', ' Reviewing the 2003 Slayer box set Soundtrack to the Apocalypse , Adrien Begrand of PopMatters described the album as \" their most', ' Kim Neely of Rolling Stone dismissed the album as \" genuinely offensive satanic drivel . \" Slayer \\'s official biography states : \" The new', ' = = Cover versions = = \\n', ' The title track and the song \" Mandatory Suicide \" have received various cover interpretations , particularly on Slayer tribute albums . Toni Ferguson', ' 1995 Slayer tribute album Slatanic Slaughter featured three tracks which originally appeared on South of Heaven , with the title track', ' The title track itself has also been covered by Integrity 2000 , Modest Mouse and <unk> , Pro @-@ Pain , and Universe Eye', ' = = Live performances = = \\n', ' Two songs taken from the album ( \" Mandatory Suicide \" and \" South of Heaven \" ) have become near constant fixtures in the band', ' \" Behind the Crooked Cross \" is rarely played live as Hanneman hates the track , though King has always wanted to play', \" Slayer has toyed with the idea of creating a live set mixed with selections from the album and 1990 's\", ' = = Track listing = = \\n', ' = = Personnel = = \\n', ' = = = Slayer = = = \\n', ' Tom Araya – bass , lead vocals \\n', ' Jeff Hanneman – lead and rhythm guitar \\n', ' Kerry King – lead and rhythm guitar , backing vocals \\n', ' Dave Lombardo – drums \\n', ' = = Charts and certifications = = \\n', ' = General aviation in the United Kingdom = \\n', ' General aviation in the United Kingdom has been defined as a civil aircraft operation other than a commercial air transport', ' Of the 21 @,@ 000 civil aircraft registered in the UK , 96 per cent are engaged in GA operations , and annually', ' GA is regulated by the Civil Aviation Authority ( CAA ) , although regulatory powers are being increasingly transferred to the European Aviation', ' = = Definitions = = \\n', ' The International Civil Aviation Organization ( ICAO ) defines general aviation ( GA ) as \" an aircraft operation other than a commercial air transport', ' Organisations in the United Kingdom ( UK ) describe GA in less restrictive terms that include elements of commercial aviation . The British', ' = = History = = \\n', ' The first aerodrome in the UK was established by the Aero Club at Muswell Manor on the Isle of Sheppey', ' During World War II civil aerodromes were taken over for military use , existing military airfields were expanded , and new ones', ' With an expanded infrastructure in place , GA became established after the war when manufacturers such as Cessna and Piper introduced', ' = = Activities = = \\n', ' The GA sector operates a range of aircraft , including balloons and airships , gliders , hang gliders , paragliders , microlights , <unk> , helicopters , amateur', ' = = = Commercial operations = = = \\n', ' Commercial operations are remunerated activities which fall within the ICAO definition of CAT . Some are , however , closely aligned to , and', ' = = = Flying schools = = = \\n', ' Flying schools are commercial businesses engaged in the training of pilots , both for recreational purposes and for those intending to', ' = = = Private flying = = = \\n', ' Private flying can be for both recreational purposes and personal transport , using aircraft that are owned individually , collectively as part', ' Private flying is most associated with the traditional form of factory @-@ produced two and four @-@ seater , single piston @-@ engine training', ' There is a strong vintage aircraft movement in the UK , with two @-@ thirds of the 500 registered historic aircraft active', ' = = = Sports = = = \\n', ' Competitive gliding in the UK takes place between May and September . Regionals are local competitions , organised and run by one', ' Handicapped air racing is open to any propeller @-@ driven aircraft capable of maintaining a minimum speed of 100 miles ( 160', ' Aerobatic competitions take place for both powered aircraft and gliders , with up to 30 events each year in the UK', ' Parachute competitions are held at club , regional , national and international levels , and include the disciplines of accuracy landings , freefall gymnastics', ' = = Aerodromes = = \\n', ' Aerodrome is a collective term for any location from which flying operations take place , although more specific terminology can be', ' = = = GASAR aerodrome classification = = = \\n', ' The factors used in determining how an individual aerodrome is categorised by the GASAR study are based broadly on size', ' Airports generally have long , fully lit , hard @-@ surfaced runways , full air traffic control , and navigation and landing aids . They are', ' = = = Aerodrome licensing = = = \\n', ' Most aerodromes used for public transport operations are required to be licensed by the CAA . To be granted a licence', ' = = Scale of the sector = = \\n', ' There are an estimated 27 @,@ 000 civil aircraft registered in the UK , 96 per cent of which are engaged in', ' The number of pilots licensed by the CAA to fly powered aircraft in 2005 was 47 @,@ 000 , of whom 28', ' The number of aerodromes that support GA in the UK is difficult to establish with certainty . <unk> 2008 United Kingdom', ' The sector was estimated to employ nearly 12 @,@ 000 people and directly contribute £ 1 @.@ 4 billion to the UK economy', ' = = Trends = = \\n', ' Most sectors of GA for which data are available have experienced growth in aircraft numbers and hours flown over the', ' Business aviation has shown strong growth , although the numbers of aircraft on the UK register have declined . This reflects a', ' Since 1990 the total number of hours flown annually by the GA sector has remained in the range 1 @.@ 25', ' = = Regulation = = \\n', ' The objective of regulation is to \" promote high standards of safety in all aspects of aviation \" , and this is the', ' = = = Devolved and self @-@ regulation = = = \\n', ' Within this framework certain sectors of GA are governed on a devolved basis . In all cases the CAA / EASA retains', ' = = = Airworthiness = = = \\n', ' Under CAA and EASA rules , all aircraft are required to meet certain standards of airworthiness to fly safely and legally', ' = = = Pilot licensing = = = \\n', ' The pilot qualification most relevant to GA is the Private Pilot Licence ( PPL ) , which permits the holder to fly for', ' = = Safety = = \\n', ' Between 1995 and 2004 there were 2 @,@ 630 accidents involving GA aircraft , of which 139 were fatal , resulting in the', ' There were 27 fatal accidents involving GA aircraft in 2007 , resulting in the loss of 48 lives . These compare with', ' = = Issues = = \\n', ' The growth in Commercial Air Transport ( CAT ) has eroded the operational freedom of GA , both in the air and on', ' = = = Airspace access = = = \\n', ' Airspace is shared by CAT , military and GA users . It is divided into controlled airspace , in which aircraft must always', ' Controlled airspace is essential for the provision of a known air traffic environment necessary for the safe operation of CAT', ' Increases in the number of CAT operations , and in the number of airports they operate from , has resulted in a', ' = = = Aerodrome access = = = \\n', ' Regional airports , such as Edinburgh Airport , have experienced strong growth in CAT operations in recent years . These operations are commercially', ' In addition to this de facto loss of facilities , the number of aerodromes in the UK has been in decline', ' = = = Planning system = = = \\n', ' The planning system is critical to the viability and operation of GA aerodromes . With many cities lacking scheduled air transport', ' = = Criticism = = \\n', ' Public opinion towards aviation generally is worsening , based on increasing environmental concerns relating to emissions and noise , and private flying', ' Planning guidance on aircraft noise advises that \" in some circumstances the public perceive general aircraft noise levels as more disturbing', ' = SMS Zrínyi = \\n', ' SMS Zrínyi ( \" His Majesty \\'s ship Zrínyi \" ) was a Radetzky @-@ class pre @-@ dreadnought battleship ( Schlachtschiff ) of the Austro @-@ Hungarian Navy', ' During World War I , Zrínyi saw action in the Adriatic Sea . She served with the Second Division of the Austro', ' With the war going against the Austrians by the end of 1918 , Zrínyi was prepared to be transferred to the', ' = = Design and construction = = \\n', ' Zrínyi was built at the Stabilimento Tecnico Triestino dockyard in Trieste , the same place where her sister ships were built', ' Zrínyi was 138 @.@ 8 m ( 455 ft 4 in ) long , and had a beam of 24 @.@ 6 m ( 80 ft', \" The ship 's primary armament consisted of four 30 @.@ 5 cm ( 12 in ) 45 @-@ caliber guns in two twin gun\", ' = = Service history = = \\n', \" The ship was assigned to the Austro @-@ Hungarian Fleet 's 1st Battle Squadron after her 1911 commissioning . In 1912 , Zrínyi\", ' In 1913 , Zrínyi participated in an international naval demonstration in the Ionian Sea to protest the Balkan Wars . Ships from', ' During that year , the first of four new dreadnoughts , SMS Viribus Unitis , that made up the Tegetthoff class — the only', ' = = = World War I = = = \\n', ' At that time of the assassination of Archduke Franz Ferdinand of Austria on 28 June 1914 , the battleships in the', ' On 23 May 1915 , between two and four hours after news of the Italian declaration of war reached the main', ' The objective of the bombardment of Ancona was to delay the Italian Army from deploying its forces along the border', ' Aside from the attack on Ancona , the Austro @-@ Hungarian battleships were largely confined to Pola for the duration of the', ' = = = Post @-@ war fate = = = \\n', ' After the Austro @-@ Hungarian Empire collapsed in 1918 , the Austrians wanted to turn the fleet over to the newly created', ' On the morning of 7 November 1920 , Zrínyi was decommissioned . USS Chattanooga took her in tow and , assisted by Brooks', ' = Geopyxis carbonaria = \\n', ' Geopyxis carbonaria is a species of fungus in the genus Geopyxis , family Pyronemataceae . First described to science in 1805 , and', ' = = Taxonomy = = \\n', ' The fungus was first described scientifically in 1805 by Johannes Baptista von Albertini and Lewis David de Schweinitz as Peziza', ' The specific epithet carbonaria derives from the Latin word for \" charcoal \" . Common names given to the fungus include \" charcoal loving', ' = = Description = = \\n', ' The fruitbodies ( <unk> ) of Geopyxis <unk> are cup shaped , 1 – 2 cm wide , and have fringed whitish margins . The inner', ' = = = Microscopic characteristics = = = \\n', ' In mass , the spores are whitish . The spores are elliptical , smooth , hyaline , devoid of oil droplets ( <unk> ) , and have dimensions', ' = = = Similar species = = = \\n', ' The closely related <unk> elf cup ( Geopyxis <unk> ) has a pale orange to yellowish fruitbody that is deeply cup shaped', ' = = Habitat and distribution = = \\n', ' Geopyxis carbonaria is widespread on burned soil or charcoal in the spring and throughout the growing season . It is one', ' The fungus is found in Europe ( from where it was originally described ) , and is widespread throughout North America . The North', ' = = Ecology = = \\n', ' Although primarily a saprotrophic fungus involved in the post @-@ fire breakdown of duff and coniferous roots , Geopyxis carbonaria has been', ' Large fruitings of the fungus are often associated with damage to the host tree , such as that which occurs with', ' = Gold dollar = \\n', ' The gold dollar or gold one @-@ dollar piece was a coin struck as a regular issue by the United States', ' A gold dollar had been proposed several times in the 1830s and 1840s , but was not initially adopted . Congress was', ' Gold did not again circulate in most of the nation until 1879 ; once it did , the gold dollar did not', ' = = Background = = \\n', ' In proposing his plan for a mint and a coinage system , Secretary of the Treasury Alexander Hamilton in 1791 proposed', ' In 1831 , the first gold dollar was minted , at the private mint of Christopher Bechtler in North Carolina . Much of', ' Soon after the <unk> began to strike their private issues , Secretary of the Treasury Levi Woodbury became an advocate of', ' Consideration was given to including the gold dollar as an authorized denomination in the revisionary legislation that became the Mint', ' = = Inception = = \\n', ' In January 1844 , North Carolina Representative James Iver McKay , the chairman of the Committee on Ways and Means , solicited the', ' Even before 1848 , record amounts of gold were flowing to American mints to be struck into coin , but the California', ' McKay got his fellow Democrat , New Hampshire Senator Charles Atherton , to introduce the bill to authorize the gold dollar and', ' = = Preparation = = \\n', ' The officers at the Philadelphia Mint , including Chief Coiner Franklin Peale , were mostly the friends and relations of Director Patterson', ' When Longacre began work on the two new coins in early 1849 , he had no one to assist him . Longacre', ' The engraving was unusually minute and required very close and incessant labor for several weeks . I made the original dies']\n",
      "647\n"
     ]
    }
   ],
   "source": [
    "wikitext = load_dataset(\"wikitext\", \"wikitext-103-v1\")\n",
    "prompts = []\n",
    "\n",
    "for text in wikitext[\"train\"][\"text\"][:1000]:\n",
    "    if len(text) > 0:\n",
    "        # Use regex to find word boundaries\n",
    "        matches = list(re.finditer(r'\\b\\w+\\b', text))\n",
    "        if len(matches) >= 20:\n",
    "            # Get the end position of the 20th word\n",
    "            end_pos = matches[19].end()\n",
    "            prompt = text[:end_pos]\n",
    "            prompts.append(prompt)\n",
    "        else:\n",
    "            prompts.append(text)\n",
    "print(prompts)\n",
    "print(len(prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "810ca8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(' park', ' park', -1.8385964632034302), (' park', ' woods', -2.2997779846191406), (' park', ' streets', -3.1739540100097656), (' park', ' dark', -3.468158721923828), (' park', ' door', -3.519336700439453)]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(model.device)\n",
    "\n",
    "prompts = [\"I enjoy walking in the\"]\n",
    "results = generate_token_and_probability(model, tokenizer, prompts, max_length=3, top_k=5)\n",
    "\n",
    "for res in results:\n",
    "    print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3e18b0-d3ed-4c7d-9599-c002b1159881",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "# model2 = LMHeadModel(model_name)\n",
    "model.eval()  # put model in inference mode\n",
    "\n",
    "# If using GPU (e.g., on Colab), you could also do:\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Function to generate text using different decoders\n",
    "# ---------------------------------------------------------\n",
    "def generate_texts(model, tokenizer, prompt, max_length=40):\n",
    "    \"\"\"Generate text from a prompt using different decoding strategies.\"\"\"\n",
    "    tokenized_result = tokenizer(prompt,return_tensors = \"pt\")\n",
    "    input_ids = tokenized_result[\"input_ids\"]\n",
    "    # If on GPU, uncomment next line:\n",
    "    # input_ids = input_ids.to(device)\n",
    "    attn_mask = tokenized_result['attention_mask']\n",
    "    \n",
    "\n",
    "    # -- Greedy decoding\n",
    "    greedy_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        do_sample=False,  # Greedy\n",
    "        attention_mask = attn_mask\n",
    "    )\n",
    "    greedy_text = tokenizer.decode(greedy_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # -- Beam search\n",
    "    beam_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        num_beams=3,    # for example\n",
    "        early_stopping=True,\n",
    "        attention_mask = attn_mask\n",
    "    )\n",
    "    beam_text = tokenizer.decode(beam_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # -- Top-k sampling\n",
    "    topk_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        do_sample=True,\n",
    "        top_k=50,  # for example\n",
    "        attention_mask = attn_mask\n",
    "    )\n",
    "    topk_text = tokenizer.decode(topk_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # -- Nucleus (top-p) sampling\n",
    "    topp_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,  # for example\n",
    "        attention_mask = attn_mask\n",
    "    )\n",
    "    topp_text = tokenizer.decode(topp_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    Mydecoder_text,decoder_prob = runViterbiTransformerPipeline(prompt, loop_runner = 5)\n",
    "    Mygreedy_text,Mygreedy_prob  =runTransformerPipeline(prompt,loop_runner = 5)\n",
    "\n",
    "    return {\n",
    "        \"greedy\": greedy_text,\n",
    "        \"beam\": beam_text,\n",
    "        \"topk\": topk_text,\n",
    "        \"topp\": topp_text,\n",
    "        \"ourDecoder\": Mydecoder_text,\n",
    "        \"ourGreedy\": Mygreedy_text\n",
    "\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Function to compute perplexity of a string\n",
    "# ---------------------------------------------------------\n",
    "def compute_perplexity(model, tokenizer, text):\n",
    "    \"\"\"Compute perplexity of `text` under `model`.\"\"\"\n",
    "    encodings = tokenizer(text, return_tensors=\"pt\")\n",
    "    input_ids = encodings.input_ids\n",
    "    # If on GPU, uncomment next line:\n",
    "    # input_ids = input_ids.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # The model returns a tuple of (loss, logits, ...)\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        # outputs.loss is the average cross-entropy across tokens\n",
    "        neg_log_likelihood = outputs.loss.item()\n",
    "\n",
    "    perplexity = math.exp(neg_log_likelihood)\n",
    "    return perplexity\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Main loop: generate text, then compute perplexities\n",
    "# ---------------------------------------------------------\n",
    "all_results = []\n",
    "\n",
    "#prompts = [\"I enjoy walking in the\"]\n",
    "for prompt in prompts[0]:\n",
    "\n",
    "    input_ids = tokenizer(prompt, return_tensors = \"pt\")[\"input_ids\"]\n",
    "    length = len(input_ids[0])\n",
    "\n",
    "    # Generate text via different decoding methods\n",
    "    gen_texts = generate_texts(model, tokenizer, prompt,max_length = length+5)\n",
    "\n",
    "    # Compute perplexities of the generated texts\n",
    "    results_for_prompt = {\"prompt\": prompt}\n",
    "    for method, text in gen_texts.items():\n",
    "        ppl = compute_perplexity(model, tokenizer, text)\n",
    "        results_for_prompt[f'{method}_text'] = text\n",
    "        results_for_prompt[f'{method}_ppl'] = ppl\n",
    "\n",
    "    # Store results\n",
    "    all_results.append(results_for_prompt)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Printing the Results\n",
    "for res in all_results:\n",
    "    print(f\"Prompt: {res['prompt']}\")\n",
    "    print(f\"  Greedy PPL: {res['greedy_ppl']:.2f}\")\n",
    "    print(f\"  Beam   PPL: {res['beam_ppl']:.2f}\")\n",
    "    print(f\"  Top-k  PPL: {res['topk_ppl']:.2f}\")\n",
    "    print(f\"  Top-p  PPL: {res['topp_ppl']:.2f}\")\n",
    "    print(f\" Viterbi PPL: {res['ourDecoder_ppl']:.2f}\")\n",
    "    print(f\"OurGreedy  PPL: {res[\"ourGreedy_ppl\"]:.2f}\")\n",
    "    print(f\"Greedy answer: {res[\"greedy_text\"]}\")\n",
    "\n",
    "    print(f\"ourGreedy answer: {res[\"ourGreedy_text\"]}\")\n",
    "    print(f\"ourDecoder answer: {res[\"ourDecoder_text\"]}\")\n",
    "    print(f\"Beam answer: {res['beam_text']}\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Save results to CSV\n",
    "# ---------------------------------------------------------\n",
    "# results_df = pd.DataFrame(all_results)\n",
    "# timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "# output_file = f\"C:/Users/jivesh/Desktop/SeniorThesis/decoder_comparison_{timestamp}.csv\"\n",
    "# results_df.to_csv(output_file, index=False)\n",
    "# print(f\"Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "36bc6714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of times ourDecoder_ppl is greater than beam_ppl  2\n",
      "Number of times ourDecoder_ppl is greater than greedy_ppl  2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(output_file)  \n",
    "\n",
    "# Count occurrences where ourDecoder_ppl > beam_ppl and ourDecoder_ppl > greedy_ppl\n",
    "count_beam = ((df[\"ourDecoder_ppl\"] < df[\"beam_ppl\"])).sum()  \n",
    "count_greedy = (df[\"ourDecoder_ppl\"] < df[\"greedy_ppl\"]).sum()\n",
    "\n",
    "print(\"Number of times ourDecoder_ppl is greater than beam_ppl \", count_beam)\n",
    "print(\"Number of times ourDecoder_ppl is greater than greedy_ppl \", count_greedy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7601372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58.50249398366958\n"
     ]
    }
   ],
   "source": [
    "perplexity = compute_perplexity(model,tokenizer,greedy_text)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "07c0f804-76b8-4d46-8468-b63c16518a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Valkyria', 'Chronicles', 'III']\n",
      "9\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "prompt = prompts[0]\n",
    "result = re.findall(r'\\w+|[.,!?;'']', prompt)\n",
    "print(result)\n",
    "input_ids = tokenizer(prompt, return_tensors = \"pt\")[\"input_ids\"]\n",
    "print(len(input_ids[0]))\n",
    "print(len(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8b7ff021-5f40-40c1-8ad3-a40bd7313f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import itertools\n",
    "\n",
    "class SearchTree:\n",
    "    def __init__(self, context, probability, parent=None, parent_index=None):\n",
    "        self.context = context\n",
    "        self.probability = probability\n",
    "        self.parent = parent\n",
    "        self.parent_index = parent_index\n",
    "        self.children = []\n",
    "    \n",
    "    def create_child(self):\n",
    "        return self\n",
    "    \n",
    "    def build_Context(self):\n",
    "        if self.parent is None:\n",
    "            return self.context\n",
    "        else:\n",
    "            return self.parent.build_Context() + self.context\n",
    "    \n",
    "    def calcProbTillNow(self):\n",
    "        if self.parent is None:\n",
    "            return self.probability\n",
    "        else:\n",
    "            return self.probability * self.parent.calcProbTillNow()\n",
    "    \n",
    "    def replace_parent(self, new_parent):\n",
    "        self.parent = new_parent\n",
    "        if new_parent is not None:\n",
    "            new_parent.children.append(self)\n",
    "    \n",
    "    def assign_parent_index(self, index):\n",
    "        self.parent_index = index\n",
    "\n",
    "def findProbability(prevToken, newTokens, model_tokenizer_tuple):\n",
    "    model, tokenizer = model_tokenizer_tuple\n",
    "    \n",
    "    probs = []\n",
    "    batch_sentences = [prevToken.build_Context() + newToken.context for newToken in newTokens]\n",
    "    \n",
    "    # Tokenize inputs\n",
    "    inputs = tokenizer(batch_sentences, return_tensors=\"pt\", padding=True)\n",
    "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(model.device)\n",
    "    \n",
    "    # Pass through the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    # Get logits for the last token in each sequence\n",
    "    last_token_indices = attention_mask.sum(dim=1) - 1\n",
    "    batch_size = input_ids.shape[0]\n",
    "    logits = outputs.logits[torch.arange(batch_size), last_token_indices]\n",
    "    \n",
    "    # Compute probabilities using softmax\n",
    "    probs_tensor = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    \n",
    "    # Get the probability of the specific token that was generated\n",
    "    for i, newToken in enumerate(newTokens):\n",
    "        token_id = tokenizer.encode(newToken.context, add_special_tokens=False)[-1]\n",
    "        if token_id < probs_tensor.shape[1]:\n",
    "            probs.append(probs_tensor[i, token_id].item())\n",
    "        else:\n",
    "            probs.append(0.0)  # Fallback if token ID is out of range\n",
    "    \n",
    "    return probs\n",
    "\n",
    "def generateIntermediates(root, numTokens=3, loop_runner=4):\n",
    "    # Initialize transformers model and tokenizer directly\n",
    "    model_name = \"gpt2\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    model.eval()\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Ensure padding token is set\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.padding_side = \"right\"\n",
    "    \n",
    "    # Model and tokenizer tuple for probability calculations\n",
    "    model_tokenizer = (model, tokenizer)\n",
    "    \n",
    "    sentence = SearchTree(root, 1)\n",
    "    context = []\n",
    "    prob_list = []\n",
    "    num_tokens = numTokens\n",
    "    content = []\n",
    "    probability = []\n",
    "    children = []\n",
    "    overlap = []\n",
    "    most_common = []\n",
    "    unique_tokens = set()\n",
    "    probabilityMatrix = []\n",
    "    uniqueTokensList = []\n",
    "    new_content = []\n",
    "    uniqueTokenLength = []\n",
    "    \n",
    "    flops_counter = {}\n",
    "    batch_size = 75\n",
    "    holdout_number = 15\n",
    "    \n",
    "    # Get initial predictions\n",
    "    input_ids = tokenizer(sentence.context, return_tensors=\"pt\").input_ids.to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "    \n",
    "    # Get logits for the last token\n",
    "    logits = outputs.logits[:, -1, :]\n",
    "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    top_probs, top_indices = torch.topk(probs, k=numTokens+3)\n",
    "    \n",
    "    # Process initial tokens\n",
    "    for i in range(num_tokens):\n",
    "        token_id = top_indices[0, i].item()\n",
    "        token_text = tokenizer.decode([token_id]).strip()\n",
    "        token_prob = top_probs[0, i].item()\n",
    "        \n",
    "        unique_tokens.add(token_text)\n",
    "        new_content.append(token_text)\n",
    "        probability.append(token_prob)\n",
    "        \n",
    "        context = SearchTree(token_text, token_prob, sentence, parent_index=0)\n",
    "        context.create_child()\n",
    "        uniqueTokensList.append(context)\n",
    "        children.append(context)\n",
    "    \n",
    "    content.append(new_content)\n",
    "    previousUniqueLength = num_tokens\n",
    "    initialStateProbability = probability\n",
    "    uniqueTokenLength.append(num_tokens)\n",
    "    \n",
    "    # Main loop for building the trellis\n",
    "    for i in range(2, loop_runner):\n",
    "        unique_tokens = set()\n",
    "        probability = []\n",
    "        new_content = []\n",
    "        previousSetLength = 0\n",
    "        \n",
    "        # Prepare batch sentences\n",
    "        batch_sentences = [child.build_Context() for child in uniqueTokensList]\n",
    "        total_predictions = []\n",
    "        \n",
    "        # Process in batches\n",
    "        if len(batch_sentences) > holdout_number:\n",
    "            # First batch\n",
    "            first_batch = batch_sentences[0:-holdout_number]\n",
    "            first_batch_predictions = get_top_k_predictions(first_batch, model, tokenizer, numTokens+2)\n",
    "            total_predictions.extend(first_batch_predictions)\n",
    "            \n",
    "            # Second batch\n",
    "            second_batch = batch_sentences[-holdout_number:]\n",
    "            second_batch_predictions = get_top_k_predictions(second_batch, model, tokenizer, numTokens+2)\n",
    "            total_predictions.extend(second_batch_predictions)\n",
    "        else:\n",
    "            total_predictions = get_top_k_predictions(batch_sentences, model, tokenizer, numTokens+2)\n",
    "        \n",
    "        # Process predictions\n",
    "        for j in range(len(uniqueTokensList)):\n",
    "            for s in range(num_tokens):\n",
    "                if s < len(total_predictions[j]):\n",
    "                    context_text = total_predictions[j][s][0]\n",
    "                    prob = total_predictions[j][s][1]\n",
    "                    \n",
    "                    unique_tokens.add(context_text)\n",
    "                    context = SearchTree(context_text, prob, uniqueTokensList[j])\n",
    "                    \n",
    "                    if len(unique_tokens) > previousSetLength:\n",
    "                        previousSetLength = len(unique_tokens)\n",
    "                        uniqueTokensList.append(context)\n",
    "                        new_content.append(context.context)\n",
    "        \n",
    "        # Store content\n",
    "        content.append(new_content)\n",
    "        \n",
    "        # Calculate combined probabilities\n",
    "        comb_prob = []\n",
    "        for prevToken in uniqueTokensList[:previousUniqueLength]:\n",
    "            comb_prob.append(findProbability(prevToken, uniqueTokensList[previousUniqueLength:], model_tokenizer))\n",
    "        comb_prob = list(itertools.chain(*comb_prob))  # flattening the list\n",
    "        \n",
    "        # Update parent relationships\n",
    "        for tokenumber, newToken in enumerate(uniqueTokensList[previousUniqueLength:]):\n",
    "            probs = [comb_prob[a*len(uniqueTokensList[previousUniqueLength:]) + tokenumber] for a in range(len(uniqueTokensList[:previousUniqueLength]))]\n",
    "            probs2 = [probs[i]*uniqueTokensList[:previousUniqueLength][i].calcProbTillNow() for i in range(len(probs))]\n",
    "            \n",
    "            if not probs2:\n",
    "                continue\n",
    "            else:\n",
    "                max_value = max(probs2)\n",
    "                max_index = probs2.index(max_value)\n",
    "                newToken.replace_parent(uniqueTokensList[:previousUniqueLength][max_index])\n",
    "                newToken.assign_parent_index(max_index)\n",
    "            \n",
    "            probability.append(probs)\n",
    "        \n",
    "        probabilityMatrix.append(probability)\n",
    "        flops_counter[i-1] = i  # Just a placeholder since we're not tracking actual FLOPS\n",
    "        \n",
    "        uniqueTokenLength.append(len(uniqueTokensList[previousUniqueLength:]))\n",
    "        previousUniqueLength = len(uniqueTokensList[previousUniqueLength:])\n",
    "        uniqueTokensList = uniqueTokensList[len(uniqueTokensList)-previousUniqueLength:]\n",
    "    \n",
    "    return probabilityMatrix, initialStateProbability, content, uniqueTokenLength, flops_counter\n",
    "\n",
    "def get_top_k_predictions(sentences, model, tokenizer, top_k=100):\n",
    "    \"\"\"\n",
    "    Get top-k token predictions for each sentence.\n",
    "    Returns a list of lists of (token, probability) tuples.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Tokenize\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "        input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "        \n",
    "        # Get model output\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "        \n",
    "        # Get predictions for the last token\n",
    "        logits = outputs.logits[:, -1, :]\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        top_probs, top_indices = torch.topk(probs, k=top_k)\n",
    "        \n",
    "        # Decode tokens and pair with probabilities\n",
    "        sentence_predictions = []\n",
    "        for i in range(min(top_k, top_indices.shape[1])):\n",
    "            token_id = top_indices[0, i].item()\n",
    "            token = tokenizer.decode([token_id]).strip()\n",
    "            if token and token != \"\\n\":\n",
    "                sentence_predictions.append((token, top_probs[0, i].item()))\n",
    "        \n",
    "        predictions.append(sentence_predictions)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63a5e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def generate_with_probabilities(text, max_new_tokens=50, top_k=10):\n",
    "    # Load model and tokenizer\n",
    "    model_name = \"gpt2\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    model.eval()\n",
    "    \n",
    "    # Make sure padding token is set\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Tokenize input text\n",
    "    input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "    \n",
    "    # Track generated tokens and their probabilities\n",
    "    generated_tokens = []\n",
    "    token_probabilities = []\n",
    "    \n",
    "    # Generate tokens one by one\n",
    "    for _ in range(max_new_tokens):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "            \n",
    "        # Get logits for the last token\n",
    "        next_token_logits = outputs.logits[:, -1, :]\n",
    "        \n",
    "        # Apply softmax to get probabilities\n",
    "        next_token_probs = torch.nn.functional.softmax(next_token_logits, dim=-1)\n",
    "        \n",
    "        # Get top k tokens and their probabilities\n",
    "        topk_probs, topk_indices = torch.topk(next_token_probs, top_k)\n",
    "        \n",
    "        # Select the token with highest probability\n",
    "        next_token = topk_indices[0, 0].unsqueeze(0).unsqueeze(0)\n",
    "        next_token_prob = topk_probs[0, 0].item()\n",
    "        \n",
    "        # Append to results\n",
    "        generated_tokens.append(tokenizer.decode(next_token[0]))\n",
    "        token_probabilities.append(next_token_prob)\n",
    "        \n",
    "        # Update input_ids for next iteration\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "    \n",
    "    return generated_tokens, token_probabilities\n",
    "\n",
    "# Example usage\n",
    "text = \"I enjoy walking in the park\"\n",
    "tokens, probs = generate_with_probabilities(text)\n",
    "\n",
    "# Print results\n",
    "for token, prob in zip(tokens, probs):\n",
    "    print(f\"Token: {token}, Probability: {prob:.4f}\")\n",
    "\n",
    "# Get the full generated text\n",
    "full_text = text + \"\".join(tokens)\n",
    "print(f\"\\nFull text: {full_text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
